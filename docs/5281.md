# 生产中的测试，安全的方式|作者 Cindy Sridharan | Medium

> 原文：<https://medium.com/@copyconstruct/testing-in-production-the-safe-way-18ca102d0ef1?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

# 生产测试，安全的方式

> ***作者注*** *:* 非常感谢[马克·麦克布莱德](https://twitter.com/mccv)阅读了这篇文章的草稿并提供了一些很好的建议。这是我的分布式系统测试系列的第二部分。本系列中的帖子如下 *:*
> 
> [测试微服务，相同的方式](/testing-microservices-the-sane-way-9bb31d158c16)(2017 年 12 月发布)
> 
> [生产测试，安全方式](https://medium.com/@copyconstruct/testing-in-production-the-safe-way-18ca102d0ef1)(2018 年 3 月发布)
> 
> [生产测试:难点](/testing-in-production-the-hard-parts-3f06cefaf592)(2019 年 9 月出版)
> 
> 生产测试:国家的命运(2022 年出版)

在这篇文章中，我希望探索不同形式的“生产中的测试”，当每种形式的测试是最有益的，以及如何以安全的方式测试生产中的服务。

然而，在我继续之前，我需要明确地澄清一下，这篇文章的内容只适用于测试那些部署在工程师控制下的**服务。通过*绝对没有任何方式*我建议这种形式的测试是适用的，更不用说*可取的，*用于测试其他类型的软件，例如移动应用或安全关键系统或嵌入式软件。**

**同样重要的是预先声明，这里描述的测试形式没有一种是简单的，通常需要系统设计、开发和测试方式的根本改变。尽管标题如此，我不相信任何形式的生产测试是完全没有风险的(T2 );只是它极大地帮助最小化服务的风险，使投资变得合理。**

## **既然可以在试运行中测试，为什么还要在生产中测试呢？**



**对于不同的人来说，临时集群或临时环境可能有不同的含义。在许多公司,“部署”到试运行和试运行中的测试是最终部署的必要前提。**

**我所知道的大多数组织都将暂存视为生产环境的微型副本。在这种情况下，保持暂存环境尽可能与生产“同步”就成了一项要求。这通常包括运行数据库等有状态系统的“不同”实例，并定期同步生产数据，以对可能包含任何个人身份信息(PII)的用户数据的某些敏感位进行分级，以符合 [GDPR](https://www.eugdpr.org) 、 [PCI](https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard) 、 [HIPAA](https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html) 等。**

**这种方法的问题是——至少在我的经验中——这种“区别”不仅仅局限于使用填充了最近生产数据的数据库的不同实例。这种差异通常还延伸到:**

**—暂存集群的大小(如果它甚至可以被称为“集群”——有时它是一个伪装成集群的单个机器)
—暂存通常是一个小得多的集群的事实也意味着几乎每个服务的配置选项都将是不同的。这适用于负载平衡器、数据库和队列的配置(比如打开的文件描述符的数量、打开的数据库连接的数量、线程池的大小、Kafka 分区的数量等等)。如果配置碰巧存储在数据库或键值存储中，如 Zookeeper 或 Consul，这些辅助系统也需要在登台环境中支持。
—无状态服务正在处理的正在进行的连接的数量，或者代理重用 TCP 连接的方式(如果它正在重用的话)
—缺乏对暂存环境的监控。即使存在监控，如果监控的是与生产环境“不同”的环境，那么几个分级监控信号最终可能会完全不准确。例如，即使监控了 MySQL 查询延迟或端点的响应时间，也很难检测新代码是否包含可能触发 MySQL 全表扫描的查询，因为对测试数据库中使用的小表进行全表扫描比对生产数据库进行全表扫描更快(有时甚至更好),因为生产数据库中的查询可能具有非常不同的性能特征。**

**虽然有理由认为前面提到的所有差异与其说是反对登台环境本身的理由，不如说是应该避免的反模式，但是“正确地做”通常需要投入大量的工程努力来实现环境之间的任何相似性。随着生产的不断变化和对各种力量的可塑性，试图达到这种平等可能类似于无所遁形。**

**此外，即使可以将试运行环境模拟得尽可能接近生产环境，也有一些其他形式的测试最好使用真实流量来执行。一个很好的例子是浸泡测试，这是一种在真实的并发和负载水平下长时间验证服务可靠性和稳定性的形式，用于检测一段时间内的内存泄漏、GC 暂停时间、CPU 利用率等。**

**这并不意味着维护一个登台环境*完全*无用(这在阴影部分会变得很明显)。只是，通常情况下，对它的依赖程度远远超过了必要的程度，以至于在许多组织中，它仍然是在全面生产部署之前进行的*唯一*形式的测试。**

## **生产中的测试艺术**

**历史上，生产中的测试带有某种耻辱和负面含义，与牛仔编程、不充分或缺乏单元和集成测试，以及某种对最终用户体验的鲁莽或缺乏关心有关。**

**事实上，如果做得很差或者很随意，“生产中的测试”*确实很配得上这个名声。生产中的测试绝不是生产前测试的*替代品*，无论怎么说，*也不容易。*事实上，我认为能够在生产中成功地*和安全地*测试需要大量的自动化，对最佳实践的深刻理解，以及从头开始设计系统以很好地适应这种形式的测试。*****

**为了能够精心设计一个整体的、安全的过程来有效地测试生产中的服务，不要把“生产中的测试”当作一个宽泛的总括术语来指代各种工具和技术就变得很突出了。在我的前一篇文章中，我自己也犯了这样的错误，在那里我介绍了一个不太科学的测试分类，并把各种各样的方法和工具归在“生产测试”下。**



**From my post “Testing Microservices, the sane way”**



**自从 2017 年 12 月底发布那篇帖子以来，我已经与几个人就帖子的内容以及更广泛的话题“生产中的测试”进行了交谈。**

**根据这些讨论以及某些正交的讨论，对我来说很明显的是“生产中的测试”这个主题不是一个插图上的几个要点就能概括的。**

**相反，我现在认为“生产中的测试”包含了跨越三个不同阶段的全部技术。**



**这个列表并不全面，界限也不是一成不变的。然而，我觉得它确实提供了一个很好的起点来开始讨论生产中的测试这个话题。**

# **“生产”的三个阶段**

**在很大程度上，关于“生产”的讨论只限于“部署”代码到生产环境中，或者在出现问题时监控或解决生产问题。**

**我自己也曾交替使用过诸如“部署”、“发布”、“发布”等术语，直到现在也没怎么想过。事实上，直到几个月前，任何区分这些术语的尝试都会被我视为无关紧要而不予考虑。**

**经过进一步思考，我得出了这样一个观点:区分生产的不同阶段是合理的需要。**

## **阶段 1 —部署**

**通过测试(甚至在生产中！)测试(或任何形式的验证)的*准确性*仅仅是一种*尽力而为*的验证，取决于测试的运行方式，这种方式类似于服务在生产中可能以最接近的方式实施的方式。**

**换句话说，测试需要在尽最大努力模拟生产环境的环境中运行。**

**而在我看来，生产环境的*尽力模拟*就是……生产环境本身。**

**为了在生产环境中运行尽可能多的测试，要求任何此类测试的失败都不要影响最终用户。**

**反过来，这只有在**在生产环境中部署服务不会立即向用户公开该服务的情况下才是可行的**。**

**在这篇文章中，我决定使用博文“ [Deploy！=由](https://blog.turbinelabs.io/deploy-not-equal-release-part-one-4724bc1e726b)[涡轮实验室](https://www.turbinelabs.io)的人发布(无关联)。这篇文章继续将“部署”描述为:**

> **部署是您的团队在生产基础设施上安装新版本服务代码的过程。当我们说一个新版本的软件被**部署**时，我们的意思是它正在你的生产基础设施中的某个地方运行。这可能是 AWS 上新启动的 EC2 实例，或者是在数据中心的 Kubernetes 集群中运行的 Docker 容器。你的软件已经成功启动，通过健康检查，准备就绪(你希望！)来处理生产流量，但实际上可能没有接收到任何流量。这是很重要的一点，所以我再重复一遍……**部署不需要向客户展示新版本的服务。**根据这个定义*，* **部署可以是一个几乎零风险的活动。****

**部署是一项零风险的活动，这对于许多因糟糕的部署而留下战斗伤疤的人来说可能是天籁之音(当然也包括你自己)。在测试时，能够在*实际生产环境*中安全地安装软件，而不会让用户接触到新安装的软件有很多好处。**

**首先，它最大限度地减少了(如果不是完全消除的话)维护单独的“开发”、“测试”和“试运行”环境的需要，这些环境总是*然后*成为与生产保持同步的依赖项。**

**它还对工程师施加了一定的设计压力，以某种方式分离他们的服务，使得针对服务的给定实例的生产中的测试运行的失败不会*而不是*导致其他服务的级联或影响用户的失败。一个这样的设计决策将是数据模型和数据库模式的设计，以便非等幂的请求(主要是*写*):**

**—可以通过任何服务在生产中的测试运行针对生产数据库执行(我的首选方法)
—或者在到达持久层之前在应用层安全地丢弃
— *或**通过某种机制(例如存储附加元数据)在持久层进行区分或隔离***

## ***第 2 阶段—发布***

***博文[部署！= Release](https://blog.turbinelabs.io/deploy-not-equal-release-part-two-acbfe402a91c) 继续将“发布”定义为:***

> ***当我们说一个服务的版本被**发布**时，我们的意思是它负责为生产流量提供服务。在动词形式中，**发布**是将生产流量转移到新版本的过程。根据这个定义，我们与发布新的二进制程序相关的所有风险——停机、愤怒的客户、在[注册](https://www.theregister.co.uk/2017/02/28/aws_is_awol_as_s3_goes_haywire)的尖刻评论——都与新软件的**发布**有关，而不是部署。(在一些公司，我听说这个发货阶段被称为**展示**。对于这篇文章，我们将坚持使用**版本**。)***

***谷歌 SRE 的书在关于发布工程的章节中使用了术语“展示”来描述发布过程。***

> ***部署是一个逻辑工作单元，由一个或多个单独的任务组成。我们的目标是使部署流程符合给定服务的风险概况。在开发或预生产环境中，我们可以每小时构建一次，并在所有测试通过后自动发布。对于面向用户的大型服务，我们可以从一个集群开始，然后以指数方式扩展，直到所有集群都被更新。**对于基础设施的敏感部分，我们可能会将首次展示延长几天，在不同地理区域的实例间交错进行。*****

***本术语中的“发布”或“首次展示”指的是“部署”一词在普通用语中的通常含义，以及经常用于描述不同的*部署*策略的术语，如“蓝绿色部署”或“金丝雀部署”，再次提及新软件的*发布*。***

**此外，一个坏的*软件版本*会成为部分或完全中断的原因。如果新发布的*版本的*服务被证明是不稳定的，这也是执行*回滚或*T4】向前的阶段。**

***发布*过程通常在自动化和*增量*时工作得最好。同样，通过将错误率或请求率与基线相关联，服务的*回滚*或*前滚*在自动化时工作得最好。**

## **第 3 阶段—发布后**

**如果服务的*发布*进行得很顺利，并且新发布的服务正在为生产流量服务，没有任何直接问题，我们可以认为*发布*是成功的。一个成功的发布之后是我所说的“发布后”阶段。**



**任何足够复杂的系统都会*总是*存在于某种程度的退化中。这些病症不是保证*回退*或*前滚*的理由；相反，需要对它们进行观察(出于各种业务和操作目的)并在需要时进行调试。因此，发布后阶段的“测试”与其说类似于传统测试，不如说类似于*调试*或收集分析。**

**事实上，我认为整个系统的每一个组件的构建都应该符合这样一个现实，即没有一个大型系统是完全健康的，在设计、编写、测试、部署和监控软件的时候应该承认并接受失败。**







**定义了“生产”的三个阶段之后，让我们来探索在每个不同阶段可用的不同测试机制。不是每个人都享受从事绿地项目或从头重写一切的奢侈。在这篇文章中，我很小心地明确说明了什么时候某项技术最适合绿地开发环境，以及在没有这项技术的情况下，团队可以做些什么来获得所提出的技术的大部分好处，而不需要对已经在工作的技术做出重大改变。**

# **在部署阶段进行生产测试**

**假设我们可以将“部署”阶段从上面概述的“发布”阶段中分离出来，那么让我们探索一下一旦代码被“部署”到生产环境中就可以执行的一些测试形式。**

## **集成测试**

**传统上，集成测试是由 CI 服务器在每个 git 分支上的隔离“测试”环境中执行的。整个服务拓扑(包括数据库、队列、缓存、代理等等)的一个副本被旋转起来，供*所有*服务的测试套件相互运行。**

**我认为这不是非常有效，原因有几个。首先，与阶段化一样，测试环境不可能与真实的生产环境*完全相同*，*甚至*，如果测试在将被部署到生产环境的同一个 Docker 容器中运行。尤其是当测试环境中运行的*唯一的*东西就是正在讨论的测试时。**

**此外，不管正在运行的测试是作为 Docker 容器还是 POSIX 进程产生的，测试最有可能产生*一个*(或者测试工具中内置的任何级别的并行)到上游服务或数据库或缓存的连接，这在服务处于生产状态时很少出现，因为它可能同时处理多个并发连接，经常重用不活动的 TCP 连接(称为 HTTP 连接重用)。**

**另一个有点问题的事实是，大多数测试产生一个新的数据库表或一个缓存键空间，通常是在每个测试运行期间测试运行的同一主机上。这种形式的测试最多只能证明系统对于一个非常具体的请求工作正常(在以前的工作中，我们称之为*测试雪花*)，并且很少，如果有的话，在模拟困难的、很好理解的故障模式方面是有效的，更不用说部分故障的多种模式了。有[足够的](http://www.hpl.hp.com/techreports/2006/HPL-2006-2.pdf) [研究](https://www.gribble.org/papers/robust.pdf)来证实分布式系统经常表现出*涌现行为*，这不能通过比系统整体更简单的任何层次的分析来预测。**

**这并不是说对整个的集成测试是无用的；只是在*完全隔离的人工环境*中进行集成测试，总的来说是没有意义的。为了验证服务的新版本不会:**

**—破坏其暴露给上游或下游的合同
—以不利的方式影响任何上游或下游的 SLO**

**第一个可以通过契约测试在一定程度上实现。凭借仅仅强制遵守服务之间的*接口*，c [契约测试](https://docs.pact.io/best_practices/contract_tests_not_functional_tests.html)被证明是在*预生产阶段*开发和测试单个服务的有效技术，而不需要启动整个服务拓扑。像 [Pact](https://docs.pact.io) 这样的消费者驱动的契约测试框架目前只支持服务之间的 RESTful JSON RPC 通信，尽管[我相信对通过 web 套接字、无服务器应用和消息队列的异步通信的支持正在](https://twitter.com/matthewfellows/status/970215548293332992)中进行。虽然将来可能会支持 gRPC 和 GraphQL 之类的协议，但目前看来还不是这样。**

**然而，在*发布*新版本之前，人们可能不仅仅想验证*接口*是否得到遵守。例如，当两个服务之间的接口发生变化时，可能需要验证两个服务之间的 RPC 调用的持续时间是否在可接受的阈值范围内。此外，我们可能还希望确保当一个额外的查询参数被添加到一个传入的请求时，缓存命中率保持一致。**

**事实证明，集成测试不是*可选的，*并且在我看来，集成测试的*目标*是确保被测试的变更在测试时不会触发系统的任何*难以理解的*故障模式(通常是那些有警报的模式)。**

**这就引出了一个问题——如何在生产中安全地执行集成测试？**

**为此，我们来看下面的例子。这是我几年前工作过的一个架构，我们有多个 web 和移动客户端连接到一个 web 服务器(服务 C ),由 MySQL(服务 D)支持，由 memcache 集群(服务 B)作为前端。**

**虽然这是一个相当传统的堆栈(并不是特别的微服务)，但我觉得这个架构中有状态和无状态服务的混合使它成为本文中用作示例的一个很好的候选。**



**从*部署*中分离*发布*意味着我们可以安全地*部署*一个服务的新实例到生产环境中。**

**现代服务发现工具允许一个同名的服务有多个*标签*(或标签)，这些标签可用于识别同名服务的*发布的*版本和*部署的*版本，以便下游可以尝试只连接到服务的*发布的*版本。**

**让我们假设在这种情况下，我们正在*将*服务 C 的新版本部署到生产中。**



**为了测试*部署的*版本是否正常工作，我们需要能够测试它并确认它所公开的契约没有被破坏。松散耦合服务的主要动力是允许团队独立地开发、部署和扩展，本着同样的精神，独立地*测试*也应该是可能的，我觉得(矛盾地)这也适用于集成测试。**

**谷歌有一篇关于测试的博客文章，名为 [**拒绝更多的端到端测试**](https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html) ，他们将集成测试描述为:**

> **集成测试采用一小组单元，通常是两个单元，并作为一个整体测试它们的行为，验证它们是否能协调地一起工作。**
> 
> **如果两个单元不能正确集成，为什么要编写一个端到端的测试，而你可以编写一个更小的、更集中的集成测试来检测相同的 bug？虽然你确实需要想得更远，但你只需要想得更远一点，就能验证各个单元是否能协同工作。**

**随之而来的是，生产中的集成测试应该遵循相同的原则，在这种情况下，只一起测试一小组单元就足够了——也许*有益—* 。如果设计得当，所有的上游依赖项应该与测试中的服务充分分离，这样来自服务 A 的格式不良的请求就不会导致架构中的级联故障。**

**在我们正在处理的例子中，这可能意味着测试服务 C 的*部署*版本及其与 MySQL 的交互，如下图所示。**



**虽然测试*读取*应该是非常简单的(除非被测服务的读取流量最终填充了一个缓存，导致由*发布的*服务使用的数据的缓存中毒)，但是测试*部署的*代码与 MySQL 的交互变得更加棘手，当它涉及可能导致数据修改的非等幂请求时。**

**我个人的偏好是对生产数据库执行集成测试。过去，我维护了一个允许向被测服务发出请求的客户白名单。某些团队为针对生产系统运行的测试维护一组专用的帐户或用户，以便将任何伴随的数据更改隔离到一个小的样本集。**

**但是，如果绝对需要在试运行期间不惜任何代价修改生产数据*而不是*，则写入/更新需要:**

**—在应用层 C 丢弃或写入数据库中的不同表/集合
—数据库中的新条目，标记为由测试运行创建的条目**

**在第二种情况下，如果*测试写*需要在数据库层被区分，那么它要求数据库的模式被预先设计以支持这种形式的测试(例如，通过有一个额外的字段)。**

**在第一种情况下，如果应用程序能够检测到请求是针对*而不是*执行的，则可以在应用层丢弃写入，这可以通过检查发出测试请求的客户端的 IP 地址或传入请求的用户 ID，或者通过检查客户端在测试模式下预期设置的请求中是否存在报头来完成。**

**如果我所建议的听起来像是嘲笑或固执，但在服务水平上，那么这不是太离谱了。这种方法也存在一些问题。正如脸书关于北海巨妖的白皮书所指出的:**

> **另一种设计选择是使用 shadow 流量，在测试环境中记录并重放传入的请求。对于 web 服务器用例，大多数操作都有副作用，这些副作用会深入到系统中。影子测试不能触发这些副作用，因为这样做会改变用户状态。**消除影子测试的副作用不仅因为服务器逻辑的频繁变化而不切实际，而且因为没有强调否则会受到影响的依赖性而降低了测试的保真度。****

**虽然绿地项目的设计可以最大限度地减少或避免这种副作用，甚至可能完全消除这种副作用，但将这种存根改造成现有的基础设施可能会比其价值更麻烦。**

**服务网格架构可以在一定程度上帮助这种形式的存根。在服务网状架构中，服务不知道网络拓扑，而是在本地主机上监听连接。服务之间的所有通信都通过 sidecar 代理进行。当每一跳都添加了一个代理时，上面描述的相同体系结构将如下所示。**



**如果我们测试服务 B，服务 B 的出口代理可以被配置为为每个测试请求注入一个定制的头`X-ServiceB-Test`,上游 C 的入口代理可以:**

**—检测报头并向服务 B 发送回一个固定响应
—向服务 C 发信号通知该请求是一个*测试*请求**



**Integration test for a deployed version of service B run against a released version of service C where the writes never make it to the database**



**以这种方式执行集成测试还可以确保服务 B 的测试是针对其上游*运行的，同时上游服务于正常的生产流量*，这很可能是对当*将*发布到生产时服务 B 将如何表现的更接近的模拟。**

**如果架构中的每个服务都支持进行真正的 API 调用，但以测试或虚拟模式进行，这也很好，这为根据服务公开的最新契约测试下游铺平了道路，但不会导致对实际数据的任何修改。这相当于合同测试，但是是在网络级别。**

## **阴影(也称为暗流量测试或镜像)**

**我发现在许多情况下，跟踪(根据谷歌的一篇博客文章，也称为 [**黑暗启动**](https://cloudplatform.googleblog.com/2017/08/CRE-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me.html) ，或者按照 [istio](https://istio.io) 的说法，称为 [*镜像*](https://istio.io/docs/tasks/traffic-management/mirroring.html) )比集成测试更有益。**

**正如混沌工程的[原理所说:](http://principlesofchaos.org)**

> **系统根据环境和流量模式表现不同。由于利用率的行为可能随时改变，因此对实际流量进行采样是可靠地捕获请求路径的唯一方法。**

**影子技术是一种技术，通过这种技术，任何给定服务的生产流量都可以被捕获，并根据新部署的服务版本进行重放。这可以实时发生，其中输入的生产流量被分为两部分并路由到*发布的*和*部署的*版本，也可以异步发生，当先前捕获的生产流量的副本针对*部署的*服务重放时。**

**当我在 [imgix](https://www.imgix.com) (一家有 7 名工程师的初创公司，其中只有 4 名是系统工程师)工作时，黑暗交通测试被广泛用于测试我们图像渲染基础设施的变化。我们捕获了一定比例的所有传入请求，并通过将 HAProxy 访问日志发送到 [heka](https://hekad.readthedocs.io/en/v0.10.0/) 管道，然后该管道将解析后的请求流送入 Kafka，从而将请求发送到 Kafka 集群。在*发布我们任何新版本的图像处理软件*之前，它已经针对捕获的黑暗交通进行了测试，以确保它能够正确处理请求。然而，我们的图像渲染栈在很大程度上也是一个无状态的服务，非常适合这种形式的测试。**

**某些公司不喜欢获取一定比例的流量，而是让他们的新软件接受输入生产流量的整个拷贝。[脸书的 McRouter](https://code.facebook.com/posts/296442737213493/introducing-mcrouter-a-memcached-protocol-router-for-scaling-memcached-deployments/) (一个 memcached 代理)支持这种形式的 memcache 流量屏蔽。**

> **在测试新的缓存硬件时，我们发现能够路由来自客户端的生产流量的完整副本非常有用。McRouter 支持灵活的阴影配置。可以对不同的池大小进行阴影测试(重新散列密钥空间)，只对一小部分密钥空间进行阴影测试，或者在运行时动态地改变阴影设置。**

**在生产环境中针对*部署的*服务隐藏整个流量的缺点是，如果在流量高峰时进行这种隐藏，可能最终需要两倍的容量来执行这种测试。**

**像 Envoy 这样的代理支持以一劳永逸的方式将流量实时隐藏到不同的集群。根据[文件](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route.proto.html?highlight=shadowing#route-routeaction-requestmirrorpolicy):**

> **路由器能够将流量从一个集群隐藏到另一个集群。当前的实现是“一劳永逸”，这意味着 Envoy 在从主集群返回响应之前不会等待影子集群的响应。为影子集群收集所有正常的统计数据，这使得这个特性对于测试非常有用。在隐藏期间，主机/授权头被改变，从而附加了`*-shadow*`。这对日志记录很有用。比如`*cluster1*`变成了`*cluster1-shadow*` *。***

**然而，通常维护一个与生产保持“同步”的副本测试集群是不可能的，也是不切实际的(原因与试图维护一个同步的“登台”集群是有问题的相同)。当用于测试一个新部署的服务时，隐藏会触发测试服务上游的非预期状态变化。根据写入生产数据库的*部署的*版本的服务跟踪一天的用户注册会导致错误率高达 100%，因为影子流量被视为重复注册并被拒绝。**

**根据我的经验，隐藏最适合测试幂等请求或测试任何有状态后端都被剔除的无状态服务。这种形式的测试更多的是测试负载、影响和配置。在这种情况下，测试服务与非幂等请求的有状态后端的交互可以在集成测试的帮助下或在一个阶段环境中完成。**

## **点击比较**

**我第一次听到“tap-compare”这个词是在今年一月的[生产测试](https://www.meetup.com/Test-in-Production/events/245714099/)会议上与[马特·诺克斯](https://twitter.com/mattknox)的一次讨论中。我能找到的唯一提到这个术语的地方是在 Twitter 的[博客帖子](https://blog.twitter.com/engineering/en_us/a/2016/superroot-launching-a-high-sla-production-service-at-twitter.html)中，主题是生产高 SLA 服务。**

> **为了验证现有系统的新实现的正确性，我们使用了一种我们称之为“tap-compare”的技术我们的 tap-compare 工具针对新系统重放生产流量样本，并将响应与旧系统进行比较。使用 tap-compare 工具的输出，我们发现并修复了实现中的错误，而没有将错误暴露给最终客户。**

**来自 Twitter 的另一篇博文将 tap compare 描述为:**

> **向生产和试运行环境中的服务实例发送生产请求**，比较结果**的正确性，并评估性能特征。**

**tap compare 和 shadowing 之间的区别似乎是，在前一种情况下，由*发布的*版本返回的响应与由*部署的*版本返回的响应进行比较，而在后一种情况下，请求以一劳永逸的方式被镜像到*部署的*版本。**

**这个领域的另一个工具是 GitHub 的[科学家](https://github.com/github/scientist)。Scientist 是为了测试 Ruby 代码而构建的，但后来被移植到了许多不同的语言上。虽然对某些类型的验证有用，但科学家也有未解决的问题。据我所在的 Slack 上的一位 GitHub 工程师说:**

> **它只是运行两个代码路径并比较结果。你必须对这两个路径小心一点，例如，不要重复发送数据库查询，如果那样会有损害的话。我认为这适用于任何你做了两次并进行比较的事情，而不是科学家。Scientist 的构建是为了确保新的权限系统与旧的相匹配，并不时地对每个 Rails 请求中发生的事情进行比较。我猜事情会花更长的时间，因为它是串行进行的，但这是 Ruby 的问题，它不是线程化的。**
> 
> **我见过的大多数用例都是读而不是写，例如 scientist 用于回答新的和改进的权限查询和模式是否与旧的获得相同的答案。所以两者都不利于生产(副本)。如果你想测试的东西有副作用，我认为你必须在应用层面上做。**

**Diffy 是 Twitter 在 2015 年开源的基于 Scala 的工具。标题为**不写测试**的[博客文章](https://blog.twitter.com/engineering/en_us/a/2015/diffy-testing-services-without-writing-tests.html)可能是理解 tap compare 在实践中如何工作的最佳资源。**

> **Diffy 通过并行运行新旧代码的实例来发现服务中潜在的 bug。它充当代理，将收到的任何请求多播给每个正在运行的实例。然后，它比较响应，并报告从这些比较中出现的任何回归。Diffy 的前提是**如果服务的两个实现对于足够大且多样化的请求集返回“相似”的响应，那么这两个实现可以被视为等价的，并且较新的实现是无回归的**。Diffy 的新颖噪声消除技术使其区别于其他基于比较的回归分析工具。**

**当我们测试两个版本是否返回相似的结果时，点击比较是一个很好的方法。根据[马克·麦克布莱德](https://twitter.com/mccv):**

> **Diffy 的一个很大的用例是系统重写。在我们的例子中，我们将一个 Rails 代码库分割成多个 Scala 服务，大量 API 消费者以我们没有预料到的方式使用这些东西。日期格式之类的东西尤其有害。**

**Tap compare 对于测试用户参与度或测试两个版本的服务在峰值负载下的行为是否相似不太有用。与隐藏一样，副作用仍然是一个未解决的问题，特别是如果部署版本和生产版本都写入同一个数据库，并且与集成测试一样，将 tap 比较测试限制为使用帐户的子集是解决这个问题的一种方法。**

## **负载测试**

**对于那些不熟悉负载测试的人来说，这篇文章应该是一个很好的起点。开源负载测试工具和框架并不少见，其中最流行的是 [Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html) 、 [Gatling](https://gatling.io) 、 [wrk2](https://github.com/giltene/wrk2) 、基于 Erlang 的 [Tsung](http://tsung.erlang-projects.org) 、 [Siege](https://www.joedog.org/siege-home/) ，基于 Scala 的 Twitter 的 [Iago](https://github.com/twitter/iago) (它清理 HTTP 服务器或代理或网络嗅探器的日志，并针对测试实例进行回放)。我还从一个熟人那里听说 [mzbench](https://github.com/satori-com/mzbench) 确实是生成负载的最佳产品，支持多种协议，如 MySQL、Postgres、Cassandra、MongoDB、TCP 等。网飞的 [NDBench](https://github.com/Netflix/ndbench) 是另一个用于负载测试数据存储的开源工具，支持大多数常见的问题。**

**Twitter 关于 Iago 的官方博客进一步揭示了优秀负载生成器的标准:**

> **非阻塞请求以指定的速率生成，使用底层的、可配置的统计分布(默认为模拟[泊松过程](http://en.wikipedia.org/wiki/Poisson_Process))。请求速率可以适当变化，例如在处理全部生产负载之前预热缓存。**
> 
> **一般来说，关注点是[利特尔法则](http://en.wikipedia.org/wiki/Little%27s_Law)的到达率方面，而不是并发用户，在给定服务延迟的情况下，允许并发用户适当浮动。这极大地增强了比较多个测试运行的能力，并防止服务退化导致负载生成器变慢。**
> 
> **简而言之，Iago 致力于建立一个系统模型，在这个系统中，请求的到达与服务处理请求的能力无关。这与模拟封闭系统的负载生成器相反，在封闭系统中，用户会耐心地处理您给他们的任何延迟。这种区别使我们能够近似模拟我们在生产中会遇到的故障模式。**

**负载测试的另一种形式是通过流量转移进行压力测试。一位优步·SRE 首先向我提到了这种测试形式，在这种测试中，所有生产流量都被定向到一个小于为服务配置的集群，如果情况不佳，流量就会被转移回一个更大的集群。脸书也使用这种方法，根据他们的官方博客文章之一:**

> **我们有意将更多流量重定向到单个集群或主机，测量这些主机上的资源使用情况，并找到服务的边界。这种测试方法对于确定支持并发脸书实况广播的峰值数量所需的 CPU 特别有用。**

**据我所在的 Slack 的一名前 LinkedIn 工程师说:**

> **LinkedIn 也曾通过从负载平衡器中移除后端来进行生产“红线”测试，直到负载达到特定阈值或出现错误。**

**事实上，谷歌搜索了一下就发现了一份完整的白皮书和来自 LinkedIn 的关于这个话题的博客文章:**

> **它被称为“Redliner”，使用生产环境中的实时流量来驱动测量，从而避免了许多妨碍容量测量在合成实验室环境中获得准确值的缺陷。**
> 
> **Redliner 的工作原理是智能地将一部分生产流量重定向到 SUT(测试中的服务),并实时分析性能。它已经被 LinkedIn 内部的数百个服务所采用，并且每天都被用于各种类型的容量分析。**
> 
> **Redliner 支持并行运行 canary 和生产实例的测试。这允许工程师在两个不同的服务实例上运行相同级别的流量:1)包含新变更(即配置/属性或新代码)的服务实例，以及 2)具有当前生产版本的服务实例。**
> 
> **负载测试的结果被用作部署决策的一部分，并且成功地防止了具有潜在性能退化的代码的部署。**

**脸书通过一个名为北海巨妖的系统将使用实时流量的负载测试带到了一个完全不同的水平，北海巨妖的白皮书非常值得一读。这主要是在流量转移的帮助下实现的，其中 [Proxygen 的](https://github.com/facebook/proxygen)(脸书的负载平衡器)配置被重新加载了边缘和集群权重的新值(从分布式配置存储中读取),这决定了有多少实时流量将被分别路由到给定 POP(存在点)内的每个集群和区域。**



**From the whitepaper on Kraken**



**监控系统( [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf) )报告各种服务的指标(详见上表)，并根据报告的监控数据和这些值的阈值，决定是否继续按权重路由流量，或者是否减少或完全中止将流量路由到特定集群。**

## **配置测试**



**开源基础设施工具的新浪潮使得将所有基础设施的变化捕获为代码不仅成为可能，而且变得相对容易。也有可能在不同程度上对这些变化进行*测试*，即使大多数基础设施预生产测试只是断言规范和语法是否正确。**

**然而，在代码的*发布*之前没有测试新的配置已经成为[大量停机](https://github.com/danluu/post-mortems#config-errors)的原因。**



**为了全面地测试配置变更，区分不同形式的配置变得很重要。[弗雷德·赫伯特](https://twitter.com/mononcqc)在与我的一次谈话中，提出了以下象限:**



**同样，这绝不是全面的，但是有了这个界限，就可以决定如何最好地测试每个配置以及在哪个阶段测试。如果真正可重现的构建是可能的，那么构建时配置是有意义的。并非所有的配置都是静态的，在现代平台上，动态配置变化是不可避免的(即使是在处理“不可变的基础设施”时)。**



**我很少看到有人像测试代码变更一样严格地测试配置变更。集成测试、跟踪峰值生产流量和针对配置更改的蓝绿色部署等技术可以极大地降低推出新配置的风险。正如谷歌[的 SRE 杰米·威尔金森](https://twitter.com/jaqx0r/status/1006170578183585792)所说:**

> **配置推送至少需要像处理程序二进制文件一样小心对待，我认为更多是因为没有编译器，单元测试配置是很困难的。你需要集成测试。你需要对配置变更进行预先和分阶段的部署，因为测试你的配置变更的唯一好地方是在生产现场，面对实际的用户使用你的配置启用的代码路径。 **全局同步配置改变=停机。****
> 
> **因此，在单独的时间表中禁用部署功能并释放配置以启用它们。像打包二进制文件一样，将 config 打包成一个漂亮的密封传递闭包。**

**几年前，脸书的一篇文章对配置变更的风险管理有很好的见解:**

> **配置系统倾向于被设计成在全球范围内快速复制变化。快速配置更改是一个强大的工具，可以让工程师快速管理新产品的推出或调整设置。然而，快速的配置更改意味着当部署了糟糕的配置时会很快失败。我们使用许多实践来防止配置更改导致故障。**
> 
> **让每个人都使用一个通用的配置系统。**
> 
> **使用通用配置系统可确保程序和工具适用于所有类型的配置。在脸书，我们发现团队有时倾向于以一次性的方式处理配置。避免这些诱惑并以一种统一的方式管理配置，使得配置系统成为一种使站点更加可靠的杠杆方式。**
> 
> ****静态验证配置变更。**很多配置系统都允许松散类型的配置，比如 JSON 结构。这些类型的配置使工程师很容易打错字段名，在需要整数的地方使用字符串，或者犯其他简单的错误。使用静态验证可以最好地捕捉这类简单的错误。一个结构化的格式(例如，在脸书我们使用节俭)可以提供最基本的验证。然而，编写程序验证来验证更详细的需求并不是不合理的。**
> 
> **跑金丝雀。首先将您的配置部署到小范围的服务中，可以避免灾难性的变化。金丝雀可以有多种形态。最明显的是 A/B 测试，比如只向 1%的用户发布新的配置。多个 A/B 测试可以同时运行，您可以使用一段时间内的数据来跟踪指标。**
> 
> **然而，出于可靠性的目的，A/B 测试并不能满足我们所有的需求。部署给少量用户，但导致相关服务器崩溃或内存不足的更改，显然会产生超出测试中有限用户的影响。A/B 测试也很耗时。工程师们经常希望在不使用 A/B 测试的情况下推出微小的变化。因此，脸书基础设施会在一小组服务器上自动测试新配置。例如，如果我们希望为 1%的用户部署一个新的 A/B 测试，我们将首先为访问少量服务器的 1%的用户部署测试(一种称为 sticky canary 的技术)。我们对这些服务器进行短时间的监控，以确保它们不会崩溃或出现其他明显的问题。这种机制为所有的变更提供了一个基本的“健全检查”,以确保它们不会导致大范围的失败。**
> 
> **保持良好的配置。脸书的配置系统旨在更新配置时，在出现故障时保留良好的配置。开发人员很自然地倾向于创建配置系统，当他们接收到无效的更新配置时，配置系统会崩溃。我们更喜欢在这些情况下保留旧配置的系统，并向系统操作员发出配置更新失败的警报。使用陈旧的配置运行通常比向用户返回错误更可取。**
> 
> ****让回复变得容易。**有时，尽管尽了最大努力，还是部署了糟糕的配置。快速找到并恢复更改是解决此类问题的关键。我们的配置系统受到版本控制的支持，使得恢复更改变得容易。**

# **生产测试—发布**

**一旦服务在*部署*后被测试，它就需要*发布。***

**这里需要强调的是，在这个阶段，只有在出现*硬*故障模式时，才会发生回滚，例如:**

**—服务崩溃循环
—上游超时大量的连接，导致错误率激增
—错误的配置更改，例如环境变量中的秘密丢失，导致服务关闭(顺便说一下，最好避免环境变量，但这是另一篇博文的不同讨论)**

**理想情况下，如果在*部署*阶段的测试是彻底的，那么在*发布*阶段的任何意外在理想情况下都是最小的，甚至是不存在的。然而，仍然存在一套最佳实践来安全地*发布*新代码。**

## **卡纳灵**

***Canarying* 指的是一个服务部分*发布*到生产。生产的一个子集现在由金丝雀组成，在它们通过基本健康检查后，它们将被发送一小部分实际生产流量。金丝雀数量在为流量提供服务时受到监控，被监控的指标与非金丝雀指标(基线)进行比较，如果金丝雀指标不在可接受的阈值范围内，则会发生回滚。虽然伪装是在发布服务器端软件的背景下讨论的一种实践，但是[伪装客户端软件](https://cloudplatform.googleblog.com/2017/03/how-release-canaries-can-save-your-bacon-CRE-life-lessons.html)也变得越来越普遍。**

**至于金丝雀的流量是由多种因素决定的。在一些公司，金丝雀首先只获得内部用户流量(也称为狗食)。如果情况看起来不错，一小部分生产流量会被导向金丝雀，然后进行全面推广。一只坏金丝雀的回滚最好是自动化的，像 T2 Spinnaker T3 这样的工具内置了对自动化金丝雀分析和回滚的支持。**

**金丝雀并不是没有问题，这篇文章对金丝雀的问题做了一个很好的概述。**

## **监视**

**在生产展示的每个阶段，监控都是必不可少的，但在*发布*阶段尤为重要。“监控”最适合报告系统的整体健康状况。旨在“监控一切”可能被证明是一种反模式。为了使监控*有效，*能够识别系统的一小组硬故障模式或一组核心指标变得非常重要。这种故障模式的例子有:**

**—错误率增加
—对整个服务或特定端点的整体请求率下降，或者更糟，完全中断
—延迟增加**

**这些硬故障模式中的任何一种都保证了新发布的软件的立即回滚或前滚。需要记住的一件重要事情是，这个阶段的监控不一定要复杂和全面。有些人认为，理想的“监控”信号数量是 3-5 个，而*肯定不超过 7-10 个。正如脸书的北海巨妖白皮书所说:***

> **我们通过一个轻量级的可配置监控组件实现了这一目标，该组件带有两个 topline 指标，即 web 服务器的 99%响应时间和 HTTP 致命错误率，作为用户体验的可靠代理。**

**在一个版本中要监控的系统和应用程序度量的集合最好在系统设计的时候决定。**

## **异常跟踪**

**我将异常跟踪放在*发布*阶段，尽管公平地说，我认为它在*部署*阶段和发布后阶段具有同等的效用。异常跟踪器通常不像其他一些可观察性工具那样受到关注或大肆宣传，但是根据我的经验，我发现异常跟踪器非常有用。**

**像 [Sentry](https://github.com/getsentry/sentry) 这样的开源工具提供了关于传入请求、堆栈跟踪和局部变量的丰富细节，所有这些都使得调试比简单地查看日志容易得多。异常跟踪还有助于对需要修复但不保证完全回滚的问题进行分类(例如触发异常的边缘情况)。**

## **流量整形**

**流量整形或流量转移实际上与其说是一种独立的测试形式，不如说是一种有助于预测和新代码逐步发布的工具。本质上，流量转移是通过更新负载平衡器的配置来实现的，以逐渐将更多的流量路由到新发布的版本。**

**流量转移也是一种有用的技术，有助于独立于正常部署的新软件的逐步推出。在我之前的公司 [imgix](https://www.imgix.com) ，2016 年 6 月，我们需要推出一个全新的基础架构。在第一次使用一定百分比的暗流量测试新基础架构后，我们开始了生产部署，最初将大约 1%的生产流量路由到新堆栈。接下来的几周时间里，我们会逐渐增加新堆栈的流量(同时修复过程中出现的某些问题),直到我们的生产流量全部由新堆栈提供服务。**

**服务网格架构的流行导致了对代理的重新兴趣，旧的代理(nginx，HAProxy)和新的代理(Envoy，Linkerd)都发布了对新功能的支持，以在 post 中互相发送消息。对我来说，在产品发布期间自动流量从 0 转移到 100%的未来似乎并不遥远。**

# **生产中的测试—发布后**

**发布后测试采取验证的形式，一旦我们满意地*发布了*代码，就在*进行验证。在这个阶段，我们确信代码基本上是正确的，已经成功地*发布*到产品中，并且正在按照预期处理生产流量。部署的代码直接或间接地在实际使用中，为真实的客户服务，或者做一些对业务有意义影响的工作。***

*在此阶段，任何验证的目标都可以归结为验证系统在不同的工作负载和流量模式下是否能够正常工作。实现这一点的最好方法是捕获生产中发生的事情的书面记录，并将其用于调试目的以及系统的一般可理解性。*

## *功能标记或暗启动*

*我能找到的关于一家公司成功使用特征标志的最老的博客文章可以追溯到大约十年前。有一个网站 [featureflags.io](https://featureflags.io) 可以作为所有功能标记的综合指南。*

> *特性标记是一种方法，开发人员通过这种方法将新特性包装在 if/then 语句中，以获得对其发布的更多控制。通过用标志包装一个特性，可以隔离它对系统的影响，并独立于部署打开或关闭标志。这有效地将特性展示与代码部署分离开来。*

*一旦新代码被部署到特性标志后面的生产环境中，就可以在生产环境中根据需要对其正确性和性能进行测试。特征标记也是生产中更被接受的测试形式之一，因此是众所周知的，并且已经被广泛地[写成](https://codeascraft.com/2011/02/04/how-does-etsy-manage-development-and-operations/) [关于](https://zachholman.com/posts/deploying-software)。也许不太为人所知的是，特性标记可以扩展到测试[数据库迁移](https://featureflags.io/feature-flags-database-migrations/)或者桌面软件。*

*关于开发和发布特性标志的最佳实践，可能写得不多。在我之前工作的公司，特征标志爆炸一直是个问题。在一段时间后，如果不遵守修剪不使用的特性标志的规则，我们就会走上大规模清理几个月(有时甚至几年)之久的特性标志的道路。*

## *A/B 测试*

*[A/B 测试](https://en.wikipedia.org/wiki/A/B_testing)通常属于实验分析，不一定被视为生产中的一种测试形式。因此，它不仅被广泛使用(有时[有争议的](https://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/))，而且[也广泛研究了](https://dl.acm.org/citation.cfm?id=2926731)和[关于](https://hbr.org/2017/06/a-refresher-on-ab-testing)的文章(包括[什么是在线实验的好指标](http://www.exp-platform.com/Documents/2016KDDMetricDevelopmentLessonsDengShi.pdf))。可能不太常见的是用于测试不同硬件或虚拟机配置的 A/B 测试，但这些通常被称为“调优”(例如，JVM 调优)，而不是被视为 A/B 测试(即使原则上，调优在很大程度上可以被视为 A/B 测试的一种形式，当涉及到测量时，要以相同的严格程度进行)。*

## *日志/事件、指标和跟踪*

*日志、指标和分布式跟踪被称为“可观察性的三大支柱”,是我过去曾广泛写过的东西。*

## *压型*

*有时需要对生产中的应用程序进行分析，以诊断与性能相关的问题。根据语言和运行时支持的不同，剖析可以简单到只需在应用程序中添加一行代码(在 Go 中为`import _ "net/http/pprof "`),或者可能需要对应用程序进行大量的检测，或者通过对正在运行的进程进行黑盒检查，并使用像 [flamegraphs](http://www.brendangregg.com/flamegraphs.html) 这样的工具检查输出。*

## *球座*

*许多人认为 teeing 类似于 shadowing，因为在这两种情况下，生产流量都是针对非生产集群或进程重放的。在我看来，这种差异可以归结为这样一个事实:为测试目的重放生产流量与为调试目的重放流量略有不同。*

*[Etsy 在博客](https://codeascraft.com/2015/04/06/experimenting-with-hhvm-at-etsy/)中谈到了使用发球作为一种验证技术(这个用例似乎类似于阴影)*

> *你可以在这个意义上把“tee”想象成命令行上的 [tee](http://en.wikipedia.org/wiki/Tee_%28command%29) 。我们在 F5 负载平衡器上编写了一个 [iRule](https://devcentral.f5.com/articles/http-request-cloning-via-irules-part-1) 来克隆去往一个池的 HTTP 流量，并将其发送到另一个池。这使我们能够将生产流量发送到我们的 API 集群，并将其发送到我们的实验性 HHVM 集群，以及一个隔离的 PHP 集群进行比较。*
> 
> *这被证明是一个强有力的工具。它允许我们在完全相同的流量配置文件上比较两种不同配置的性能。*

*然而，在某些情况下，为了*调试*的目的，需要能够针对离线系统引导生产流量。在这种情况下，可以修改脱机系统以发出更多的诊断信息，或者进行不同的编译(例如，使用线程杀毒器)来帮助进行故障排除。在这种情况下，发球台更像是一个*调试*工具，而不是一个*验证*工具。*

*当我在 imgix 工作时，这种形式的调试并不常见，但也不是闻所未闻，尤其是在调试我们对延迟敏感的应用程序时。例如，下面引用的是 2015 年的一个这样的事件的事后分析(在我的前同事杰里米的允许下复制，他调试了这个)。我们很少发布 400 级别的错误，每当我们试图重现这个问题时，这个错误似乎就消失了。这种情况非常罕见(即每 10 亿次服务中只有几次请求)。我们一天只能看到一小撮。事实证明不可能可靠地重现，所以我们需要调试生产流量，以便有机会观察它的发生。*

> *TL；DR；我转到了一个内部库，但最终是在一个系统提供的库上构建的。系统版本偶尔会有一个问题，直到应用了足够的流量后才出现。但是真正的问题是被截断的头名。*
> 
> *在过去的两天里，我一直在挖掘一个错误 400 发生率越来越高的问题。这个问题只出现在极少数的请求中，而且这类问题通常很难诊断。这些通常是众所周知的大海捞针:在我们的案例中，这是十亿分之一的问题。*
> 
> *发现这个问题的第一步是捕获导致错误响应的完整的原始 HTTP 请求。我在呈现服务器上实现了一个 unix 套接字端点，将传入流量传递到套接字上的一个连接。这样做的目的是让我们能够轻松、廉价地打开和关闭暗流量，以便直接在开发人员的机器上进行测试。为了避免生产问题，如果观察到任何插座背压，连接将被关闭。也就是说，如果复制跟不上，就会被淘汰。这个套接字被证明在几种开发条件下是有用的。不过，在这种情况下，它用于收集一组服务器的所有传入流量，希望捕获足够多的传入请求，以识别导致错误 4xx 错误的模式。使用 dsh 和 netcat，我可以轻松地将传入的流量捕获到一个离线文件中。*
> 
> *周三下午的大部分时间都用来收集这些数据。有了大量的这些，我就可以使用 netcat 在一个重新配置的本地系统上重放它们，以产生大量的调试信息。没有出任何差错。下一步:以尽可能高的速度重放它们。在这种情况下，while 循环连续发送每个原始请求。大约两个小时后，我终于成功了。日志数据显示缺少一个标题！*
> 
> *我用一个红黑色来传递标题。这些结构将比较视为相同，当您对键有特殊要求时，这是一个很好的品质:在这种情况下，http 头不区分大小写。我的第一个想法是，我正在使用的库中可能存在叶节点问题。插入顺序确实会影响构建底层树的模式，而且红黑再平衡非常复杂，所以虽然不太可能，但也不是不可能。所以我换了一个不同的红黑树实现。几年前对它进行了修复，所以我选择直接嵌入到我们的源代码中，目的是提供我想要的确切版本。然而，构建选择了错误的版本，并且依赖于新版本，我现在触发了一些不正确的行为。*
> 
> *这导致渲染系统发出 500，导致机器被淘汰。这就是为什么它是随着时间的推移而建立起来的。在一些被循环之后，它们的流量被重新路由，这增加了该服务器的问题面。所以我关于图书馆有问题的假设是没有根据的，把它换回来解决了 500 人的问题。*
> 
> *回到 400 错误修复，我仍然有一个错误的问题，大约花了 2 个小时才浮出水面。库的改变显然没有解决问题，但是我有理由相信我选择的库是可靠的，并且在没有意识到库的选择是不正确的情况下，我不关心坚持使用它的选择。在深入研究之后，我意识到正确的值存储在单个字符头中(例如“h: 12345”)。它终于明白了“h”是 Content-Length 头的末尾。回过头来看，我发现“内容长度”的标题是空的。*
> 
> *当读取标题时，这一切都变成了一个接一个的错误。基本上，nginx/joyent http 解析器将发出部分数据，并且每当部分头字段少了一个字符时，我将发送没有值的头，并且我随后将收到具有正确值的单字符头字段。这是非常罕见的组合，这就是为什么它需要这么长时间来触发。因此，每当我有一个单字符头时，我就加入一些额外的日志记录，应用一个建议的修复，并成功地运行脚本几个小时！*
> 
> *当然，在提到的库问题中可能有一条蛇，但是这两个问题现在都解决了。*

*从事延迟敏感型应用的工程师需要轻松地使用捕获的实时流量进行测试，因为这些通常是单元测试无法重现的 bug，甚至是任何可观察性工具都无法发现的 bug(尤其是当日志记录受到延迟限制时)。*

## *混沌工程*

> *混沌工程是一门在分布式系统上进行实验的学科，目的是建立对系统承受生产中动荡条件能力的信心。*

*混沌工程学最初是由网飞的[混沌猴子](https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116)推广开来的，现在已经成为一门独立的学科。术语“混沌工程”可能是新的，但是故障注入测试已经存在很长时间了。*

*混沌测试指的是以下技术:*

*—杀死随机节点以查看系统是否对节点故障具有弹性
—注入错误条件(如延迟)以验证它们的正确处理
—迫使行为不当的网络查看服务如何反应*

*绝大多数组织缺乏能够着手进行混乱测试的操作复杂性。重要的是要说明，一旦建立了弹性基线，在系统中注入故障是最好的。[这份由](https://bit.ly/2G4clB9) [Gremlin](https://www.gremlin.com) (无从属关系)撰写的白皮书应该提供了关于混沌测试的很好的入门以及如何开始的指南。*

> *混沌工程的关键在于它被视为一门科学学科。它使用精确的工程流程来工作。*
> 
> *混沌工程的目标是**通过对你的系统进行实验来教** *你一些关于系统漏洞的新东西*。您试图在生产中可能出现的隐藏问题导致停机之前识别这些问题。只有这样，你才能解决系统的弱点，让你的系统具有容错能力。*

# *结论*

*生产中测试的目标不是完全消除各种系统故障。引用[约翰·奥斯鲍](https://twitter.com/allspaw)的话:*

> *虽然对系统弹性信心的任何增加都是积极的，但这只是增加，而不是完全的信心。任何复杂的系统都可能(也将会)以令人惊讶的方式失败。*

*起初，生产中的测试可能看起来相当令人生畏，远远超出了大多数工程组织的工资级别。虽然这并不容易，也不是完全没有风险，但小心翼翼地进行，它可以极大地帮助建立对复杂分布式系统可靠性的信心，这种系统在当今时代越来越普遍。*















