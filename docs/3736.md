# Twitter 背后的基础设施:规模

> 原文:[https://blog . Twitter . com/2017/the-infra structure-behind-Twitter-scale？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://blog.twitter.com/2017/the-infrastructure-behind-twitter-scale?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

#### 基础设施

# Twitter 背后的基础设施:规模

经过

Thursday, 19 January 2017

## Twitter 车队概述

当实体企业供应商的硬件统治数据中心时，Twitter 就成熟了。从那时起，我们不断设计和更新我们的车队，以利用技术和硬件效率方面的最新开放标准，从而提供最佳体验。

我们目前的硬件分布如下所示:

#### 网络信息流通量

我们在 2010 年初开始从第三方托管迁移，这意味着我们必须学习如何在内部构建和运行我们的基础设施，由于对核心基础设施需求的了解有限，我们开始迭代各种网络设计、硬件和供应商。

到 2010 年底，我们完成了第一个网络架构，该架构旨在解决我们在托管虚拟环境中遇到的规模和服务问题。我们有深度缓冲 tor 来支持突发服务流量和电信级核心交换机，在该层没有超额预订。这通过一些显著的工程成就支持了 Twitter 的早期版本，如我们在空中楼阁和 2014 年世界杯期间创下的 TPS 记录。

几年后，我们运行着一个网络，pop 遍布五大洲，数据中心拥有成千上万台服务器。在 2015 年初，由于不断变化的服务架构和不断增加的容量需求，我们开始经历一些成长的烦恼，当全网状拓扑不支持添加新机架所需的额外硬件时，最终达到了数据中心的物理可扩展性极限。此外，由于不断增加的路由规模和拓扑复杂性，我们现有的数据中心 IGP 开始出现意外行为。

为了解决这一问题，我们开始将现有的数据中心转换为 [Clos](https://tools.ietf.org/html/draft-ietf-rtgwg-bgp-routing-large-dc-09) 拓扑+[BGP](https://tools.ietf.org/html/rfc1105)-一种[转换](https://www.nanog.org/sites/default/files/20161016_Woodfield_Routing_Protocol_Migrations_v1.pdf)，这种转换必须在实时网络上完成，尽管很复杂，但在相对较短的时间内对服务的影响极小。网络现在看起来像这样:

新方法的亮点:

*   单个设备故障的爆炸半径更小。
*   横向带宽扩展能力。
*   降低路由引擎 CPU 开销；更高效地处理路由更新。
*   由于 CPU 开销较低，路由容量更高。
*   基于每个设备和链路对路由策略进行更精细的控制。
*   不再暴露于先前重大事故的几个已知根本原因:协议重新收敛时间增加、路由变更问题和固有 OSPF 复杂性的意外问题。
*   支持无影响的机架迁移。

下面我们来扩展一下我们的网络基础设施。

### 数据中心流量

#### 挑战

我们的第一个数据中心是通过模拟 colo 中已知系统的容量和流量概况而构建的。但仅仅几年后，我们的数据中心现在比原始设计大了 400%。现在，随着我们的应用程序堆栈的发展，Twitter 变得更加分散，流量特征也发生了变化。指导我们早期网络设计的最初假设不再适用。

我们的流量增长速度快于我们重新设计整个数据中心的速度，因此构建一个高度可扩展的体系结构非常重要，该体系结构将允许在叉车式迁移中以增量方式增加容量。

高扇出微服务需要能够处理各种流量的高度可靠的网络。我们的流量范围从长期的 TCP 连接到临时的 mapreduce 作业，再到非常短的微突发。对于这些不同的流量模式，我们最初的解决方案是部署具有深度数据包缓冲区的网络设备，但这也带来了一系列问题:更高的成本和更高的硬件复杂性。后来的设计使用了更标准的缓冲区大小和直通交换功能，以及更好的 TCP 堆栈服务器端，以更好地处理微突发。

#### 经验教训

这些年来，通过这些改进，我们学到了一些值得一提的东西:

*   如果流量趋向于您的设计容量上限，则超越最初的规范和要求进行设计，并做出快速大胆的更改。
*   依靠数据和指标做出正确的技术设计决策，并确保网络运营商能够理解这些指标——这在托管和云环境中尤为重要。
*   没有所谓的临时改变或变通办法:在大多数情况下，变通办法是技术债务。

### 主干流量

#### 挑战

我们的主干流量逐年大幅增长，在数据中心之间移动流量时，我们仍然会看到 3-4 倍于正常流量的突发流量。这给历史协议带来了独特的挑战，这些协议从未被设计来处理这种情况，例如 [MPLS](https://tools.ietf.org/html/rfc3031) [RSVP](https://tools.ietf.org/html/rfc3209) 协议，其中它假设某种形式的逐渐上升，而不是突然爆发。为了获得尽可能快的响应时间，我们不得不花费大量时间来调整这些协议。此外，为了应对流量高峰(尤其是存储复制)，我们实施了优先级排序。虽然我们需要始终保证客户流量的交付，但我们可以承受延迟交付 SLA 长达数天的低优先级存储复制作业。通过这种方式，我们的网络可以利用所有可用的容量，最大限度地提高资源利用率。客户流量总是比低优先级的后端流量更重要。此外，为了解决 [RSVP](https://tools.ietf.org/html/rfc3209) 自动带宽带来的装箱问题，我们实现了 [TE++](https://www.juniper.net/documentation/en_US/junos/topics/concept/dynamic-bandwidth-management-overview.html) ，当流量增加时，它会创建额外的[LSP](https://tools.ietf.org/html/rfc3031)，并在流量减少时删除它们。这允许我们有效地管理链路之间的流量，同时减少维护大量 LSP 的 CPU 负担。

虽然主干最初缺乏任何形式的流量工程，但它的加入有助于我们根据增长进行扩展。为了帮助实现这一点，我们完成了角色分离，以便让单独的路由器分别专用于核心和边缘路由。这也使我们能够以经济高效的方式进行扩展，因为我们不必购买具有复杂边缘功能的路由器。

在边缘，这意味着我们有一个核心来连接一切，并能够以非常水平的方式扩展(即每个站点有许多许多路由器，而不是只有一对，因为我们有一个核心层来连接一切)。

为了在我们的路由器中扩展 [RIB](https://tools.ietf.org/html/draft-ietf-i2rs-rib-info-model-08) ，我们不得不引入路由反射来满足我们的扩展需求，但是在这样做的过程中，并且转向分层设计，我们还使路由反射器成为它们自己的路由反射器的客户端！

#### 吸取的教训

在过去的一年中，我们将设备配置迁移到模板中，现在定期对其进行审核。

### 边缘流量

Twitter 的全球网络与全球许多数据中心的 3000 多个独特网络直接互联。直接流量交付是我们的首要任务。我们通过全球网络主干将 60%的流量传输到互连点和 pop，在那里我们有本地前端服务器终止客户端会话，这一切都是为了尽可能靠近客户。

#### 挑战

无法预测的世界事件会导致同样不可预测的突发流量。在大型活动(如体育、选举、自然灾害和其他有新闻价值的事件)期间，这些突发事件会给我们的网络基础设施(尤其是照片和视频)带来压力，而且几乎没有提前通知。我们为这些事件调配容量，并为较大的利用率峰值做好计划——通常是某个地区即将举行重大事件时正常峰值的 3-10 倍。由于我们的流量逐年显著增长，跟上容量增长可能是一个真正的挑战。

尽管我们尽可能与我们所有的客户网络保持密切联系，但这并非没有挑战。令人惊讶的是，网络或提供商往往更喜欢远离本土市场进行互连，或者由于其路由政策，导致流量到达市场之外的 pop。虽然 Twitter 公开与我们看到流量的所有主要眼球(客户)网络对等，但并不是所有的 ISP 都这样做。我们花了大量时间优化我们的路由策略，以便尽可能直接地向用户提供流量服务。

#### 吸取的教训

历史上，当有人请求“www.twitter.com”时，根据他们的 DNS 服务器的位置，我们会向他们传递不同的区域 IP，以将他们映射到特定的服务器集群。由于我们不能依赖用户映射到正确的 DNS 服务器，也不能依赖我们检测 DNS 服务器在世界上的物理位置的能力，这种方法“GeoDNS”在一定程度上是不准确的。此外，互联网的拓扑结构并不总是与地理位置相匹配。

为了解决这个问题，我们采用了“BGP Anycast”模式，在这种模式下，我们从所有位置发布相同的路由，并优化我们的路由，从客户到我们的 pop 选择最佳路径。通过这样做，我们可以在互联网拓扑结构的限制下获得最佳性能，而不必依赖于关于 DNS 服务器存在的不可预测的假设。

## 储存；储备

每天有数亿条推文被发送。它们被处理、存储、缓存、服务和分析。面对如此庞大的内容，我们需要相应的基础设施。存储和消息传递占 Twitter 基础设施的 45%。

存储和消息团队提供以下服务:

1.  [Hadoop](https://blog.twitter.com/2015/hadoop-filesystem-at-twitter) 同时运行计算和 HDFS 的集群
2.  [曼哈顿](https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale)所有低延迟关键价值商店的集群
3.  [图](https://blog.twitter.com/2010/introducing-flockdb)存储分片的 MySQL 集群
4.  所有大型对象(视频、图片、二进制文件……)的集群
5.  [缓存](https://blog.twitter.com/2012/caching-with-twemcache)簇
6.  [消息传递](https://blog.twitter.com/2015/building-distributedlog-twitter-s-high-performance-replicated-log-service)集群
7.  关系商店( [MySQL](https://blog.twitter.com/2015/another-look-at-mysql-at-twitter-and-incubating-mysos) ，PostgreSQL 和 [Vertica](https://blog.twitter.com/2016/discovery-and-consumption-of-analytics-data-at-twitter)

#### 挑战

虽然在这个规模上有许多不同的挑战，但多租户是我们必须克服的最显著的挑战之一。通常，客户会遇到一些会影响现有租户并迫使我们构建专用集群的困境。更多专用集群增加了保持正常运行的运营工作量。

我们的基础设施没有什么令人惊讶的地方，但有些有趣的地方如下:

*   Hadoop:我们有多个存储超过 500 PB 的集群，分为四组(实时、处理、数据仓库和冷存储)。我们最大的集群超过 1 万个节点。我们每天运行 15 万个应用程序，启动 1.3 亿个容器。
*   Manhattan(Tweets、直接消息、Twitter 帐户等的后端):我们针对不同的使用情形运行几个集群，例如大型多租户、非公共的小型集群、只读集群以及针对大量写入/大量读取流量模式的读取/写入集群。只读集群处理数十万个 QPS，而读/写集群处理数百万个 QPS。最高性能的集群，即我们的可观察性集群，可容纳在每个数据中心，处理超过数千万次写入。
*   Graph:我们传统的基于 Gizzard/MySQL 的分片集群，用于存储我们的图形。我们的社交图 Flock 可以管理超过数千万 QPS 的峰值，平均我们的 MySQL 服务器达到 30k - 45k QPS。
*   Blobstore:我们的图像、视频和大型文件存储，我们在其中存储了数千亿个对象。
*   缓存:我们的 Redis 和 Memcache 集群:缓存我们的用户、时间轴、推文等等。
*   SQL:这包括 MySQL、PostgreSQL 和 Vertica。MySQL/PosgreSQL 用于我们需要强大一致性的地方，管理广告活动、广告交换以及内部工具。Vertica 是一个专栏商店，通常用作 Tableau 的后端，支持销售和用户组织。

Hadoop/HDFS 也是我们基于 Scribe 的日志管道的后端，但在过渡到 [Apache Flume](https://flume.apache.org/) 的最后测试阶段，作为替代，以解决一些限制，如缺乏对聚合器的选择性客户端的速率限制/节流，缺乏对类别的交付保证，以及解决内存损坏问题。我们每天处理超过一万亿封邮件，所有这些邮件都被处理成 500 多个类别，经过整合，然后有选择地在我们的所有集群中进行复制。

### 年代演变

Twitter 建立在 MySQL 之上，最初所有的数据都存储在上面。我们从一个小型数据库实例发展到一个大型数据库实例，并最终发展成许多大型数据库集群。跨 MySQL 实例手动移动数据需要大量耗时的手工工作，因此在 2010 年 4 月，我们引入了[Gizzard](https://blog.twitter.com/2010/introducing-gizzard-a-framework-for-creating-distributed-datastores)——一个用于创建分布式数据存储的框架。

当时的生态系统是:

*   复制的 MySQL 集群
*   基于 Gizzard 的分片 MySQL 集群

继 2010 年 5 月发布 Gizzard 之后，我们推出了基于 Gizzard 和 MySQL 的图形存储解决方案 [FlockDB](https://blog.twitter.com/2010/introducing-flockdb) ，并于 2010 年 6 月推出了我们的唯一标识符服务 [Snowflake](https://blog.twitter.com/2010/announcing-snowflake) 。2010 年，我们还投资了 [Hadoop](https://blog.twitter.com/2010/hadoop-at-twitter) 。最初打算存储 MySQL 备份，现在大量用于分析。

大约在 2010 年，我们还添加了 Cassandra 作为存储解决方案，虽然它没有完全取代 MySQL，因为它缺乏自动增量功能，但它确实作为一种度量存储得到了采用。随着流量呈指数级增长，我们需要扩大集群，2014 年 4 月，我们推出了 [Manhattan](https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale) :我们的实时多租户分布式数据库。从那时起，曼哈顿就成了我们最常见的存储层之一，Cassandra 已经被弃用。

2012 年 12 月，Twitter 发布了一项功能，允许原生照片上传。在幕后，新的存储解决方案 [Blobstore](https://blog.twitter.com/2012/blobstore-twitter-s-in-house-photo-storage-system) 让这一切成为可能。

#### 吸取的教训

多年来，随着我们将数据从 MySQL 迁移到 Manhattan，以利用更好的可用性、更低的延迟和更容易的开发，我们还采用了额外的存储引擎(LSM、b+树……)来更好地服务于我们的流量模式。此外，我们已经从事件中吸取了教训，并开始通过发送背压信号和启用查询过滤来保护我们的存储层免遭滥用。

我们继续专注于为工作提供合适的工具，但这意味着合理地理解所有可能的用例。“一刀切”的解决方案很少奏效——避免为棘手情况建立捷径，因为没有什么比临时解决方案更永久的了。最后，不要过度推销解决方案。凡事有利有弊，需要带着现实主义的意识去采纳。

## 隐藏物

虽然缓存只占我们基础设施的 3%,但它对 Twitter 来说至关重要。它保护我们的后备存储免受大量读取流量的影响，并允许存储具有大量水合成本的对象。我们大规模使用一些缓存技术，比如 Redis 和 Twemcache。更具体地说，我们混合了专用和多租户 Twitter memcached (twemcache)集群以及 Nighthawk (sharded Redis)集群。我们已经将几乎所有的主要缓存从裸机迁移到 Mesos，以降低运营成本。

#### 挑战

规模和性能是缓存面临的主要挑战。我们运营着数百个集群，总数据包速率为 320M 数据包/s，向我们的客户提供超过 120GB/s 的数据包，我们的目标是即使在事件高峰时，也能以 99.9%和 99.99%的延迟限制提供每个响应。

为了满足我们的高吞吐量和低延迟服务水平目标(SLO ),我们需要不断测量系统的性能，并寻求效率优化。为了帮助我们做到这一点，我们编写了 [rpc-perf](https://github.com/twitter/rpc-perf) 来更好地了解我们的缓存系统是如何执行的。当我们从专用机器迁移到当前的 Mesos 基础设施时，这在容量规划中至关重要。由于这些优化工作，我们将每台机器的吞吐量提高了一倍以上，而延迟没有变化。我们仍然相信有很大的优化收益。

#### 经验教训

迁移到 Mesos 是一个巨大的运营胜利。我们整理了我们的配置，可以缓慢部署以保持命中率，避免给持久性存储带来麻烦，并以更高的信心增长和扩展这一层。

由于每个 twemcache 实例有数千个连接，任何进程重启、网络故障和其他问题都可能触发针对缓存层的 DDoS 式连接攻击。随着我们的扩展，这已经成为一个更大的问题，通过基准测试，我们已经实施了 uptakes 规则，以便在高重新连接率导致我们违反 SLO 时，单独限制到每个缓存的连接。

我们按照用户、推文、时间轴等对缓存进行逻辑分区。一般来说，每个缓存集群都针对特定需求进行了调整。根据集群的类型，它们处理 10 到 50 米的 QPS，运行数百到数千个实例。

### 晕

请允许我们介绍哈普罗。它是 Tweet 时间轴的主要缓存，并由 Redis 的定制版本提供支持(实现混合列表)。Haplo 在时间轴服务中是只读的，由时间轴服务和扇出服务写入。这也是我们为数不多的尚未迁移到 Mesos 的缓存服务之一。

*   每秒 40 米到 100 米之间的聚合命令
*   每台主机 100Mbps 的网络 IO
*   每秒 80 万个聚合服务请求

#### 进一步阅读

这些年来，姚月( [@thinkingfish](https://twitter.com/intent/user?screen_name=thinkingfish) )发表了几篇关于缓存的精彩演讲和论文，包括关于我们[使用 Redis](https://www.youtube.com/watch?v=rP9EKvWt0zo) 以及我们更新的 [Pelikan 代码库](https://www.youtube.com/watch?v=pLRztKYvMLk)。欢迎观看视频和最近的[博客文章](https://twitter.github.io/pelikan/2016/04/03/caching-in-datacenters.html)

### 大规模运行木偶

我们运行各种核心基础设施服务，如 Kerberos、Puppet、Postfix、Bastions、仓库和出口代理。我们专注于扩展、构建工具、管理这些服务，以及支持数据中心和接入点(POP)扩展。就在去年，我们将 POP 基础设施扩展到了许多新的地理位置，这需要对我们如何规划、引导和启动新位置进行全面的重新架构。

我们使用 Puppet 进行系统的所有配置管理和后 kickstart 包安装。本节详细介绍了我们已经克服的一些挑战，以及我们的配置管理基础架构的发展方向。

#### 挑战

在满足用户需求的过程中，我们很快就超越了标准工具和实践。我们每月有 100 多名提交者、500 多个模块和 1000 多个角色。最终，我们能够减少角色、模块和代码行的数量，同时提高代码库的质量。

#### 分支

我们有三个分支，Puppet 称之为环境。这些使我们能够正确地测试、发现并最终推动生产环境的变化。我们也允许定制分支，用于更隔离的测试。

将变更从测试转移到生产目前需要一些手工操作，但是我们正在向自动化集成/回退过程的更加自动化的 CI 系统转移。

#### 代码库

我们的 Puppet 库包含超过 100 万行代码，仅 Puppet 代码就超过每个分支 100，000 行。我们最近进行了大量的工作来清理我们的代码库，减少死代码和重复代码。

这个图表显示了我们从 2008 年到今天的全部代码行(不包括各种自动更新的文件)。

此图显示了我们从 2008 年至今的文件总数(不包括各种自动更新的文件)。

此图显示了我们从 2008 年至今的平均文件大小(不包括各种自动更新的文件)。

#### 大获全胜

我们代码库最大的胜利是代码林挺，风格检查挂钩，记录我们的最佳实践，以及保持正常的办公时间

有了林挺工具(木偶皮棉)，我们能够符合共同的社区林挺标准。我们将代码库中的林挺错误和警告减少了几十万行，并触及了超过 20%的代码库。

在最初的清理之后，在代码库中进行较小的更改现在变得更容易了，并且将自动化的样式检查作为版本控制挂钩已经极大地减少了我们代码库中的样式错误。

整个组织有超过 100 个傀儡提交者，记录内部和社区的最佳实践是一个力量倍增器。有一个单独的文档可以参考，这提高了代码发布的质量和速度。

在正常办公时间寻求帮助(有时通过邀请)可以实现一对一的帮助，因为票证和聊天渠道不能提供足够高的通信带宽，或者不能传达想要完成的事情的整体情况。结果，大多数提交者通过了解社区、最佳实践以及如何最好地应用变更，提高了他们的代码质量和速度。

#### 监视

系统指标通常并不有用(参见 Caitlin McCaffrey 的 Monitorama 的 2016 年演讲[此处](https://speakerdeck.com/caitiem20/tackling-alert-fatigue))，但确实为我们发现有用的指标提供了额外的背景。

我们提醒和绘制的一些最有用的指标是:

*   运行失败:不成功的木偶运行次数。
*   运行持续时间:Puppet 客户机运行完成所需的时间。
*   未运行:在我们预期的时间间隔内没有发生的木偶运行次数。
*   目录大小:目录的大小，以 MB 为单位。
*   目录编译时间:编译目录所花费的时间，以秒为单位。
*   目录编译:每个主服务器正在编译的目录数。
*   文件资源:正在获取的文件数量。

这些指标都是按主机收集的，并按角色汇总。这允许在特定角色、角色集或更广泛的影响事件中出现问题时发出即时警报并进行识别。

#### 影响

通过从 Puppet 2 迁移到 Puppet 3 并升级 Passenger(两个帖子都是另一个时间发布的)，我们能够将我们在 Mesos 集群上的平均 Puppet 运行时间从 30 多分钟减少到 5 分钟以下。

该图显示了我们在 Mesos 集群上的平均 Puppet 运行时间(秒)。

如果你有兴趣帮助我们的傀儡基础设施，[我们正在招聘](https://careers.twitter.com/en/work-for-twitter/site-reliability-engineer.html)！

如果您对更一般的系统供应流程、元数据数据库(Audubon)和生命周期管理(Wilson)感兴趣，供应工程团队最近在我们的 [#Compute](https://twitter.com/hashtag/Compute) 活动中展示了它，记录在[这里](https://www.youtube.com/watch?v=L3og0m9dRh8)。

没有 Twitter Engineering 所有人的辛勤工作和奉献，我们不可能取得这样的成就。向所有为 Twitter 的可靠性做出贡献的现任和前任工程师致敬。

特别感谢[布莱恩·马丁](https://twitter.com/brayniac)、[克里斯·伍德菲尔德](https://twitter.com/cwoodfield)、[大卫·巴尔、](https://twitter.com/davebarr)、[德鲁·罗斯坦](https://twitter.com/mediocrity)、[马特·盖蒂](https://twitter.com/aspen)、[帕斯卡尔·博吉诺](https://twitter.com/marsanfra)、[拉斐尔·瓦利戈拉](https://twitter.com/freakoverlord)、[斯科特·恩斯特罗姆](https://twitter.com/sengstrom)、[蒂姆·霍夫曼](https://twitter.com/hoffnz)、[约吉·夏尔马](https://twitter.com/yogstr)对这篇博文的贡献。