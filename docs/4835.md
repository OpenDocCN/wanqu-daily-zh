# 扩展 GitLab 数据库| GitLab

> 原文:[https://about . git lab . com/2017/10/02/scaling-the-git lab-database/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

![](../Images/806e84d2929bfa1dc330780f738130f0.png)

长期以来，GitLab.com 使用一台 PostgreSQL 数据库服务器和一个副本进行灾难恢复。在 GitLab.com 出现的最初几年，这种方式运行得相当好，但随着时间的推移，我们开始看到这种设置出现了越来越多的问题。在本文中，我们将看看我们为帮助 GitLab.com 和自我管理的 GitLab 实例解决这些问题所做的工作。

例如，数据库承受着持续的压力，CPU 利用率几乎一直在 70%左右徘徊。不是因为我们尽可能以最好的方式使用了所有可用的资源，而是因为我们用太多(优化不好的)查询轰炸了服务器。我们意识到我们需要一个更好的设置，使我们能够平衡负载，并使 GitLab.com 对主数据库服务器上可能出现的任何问题更有弹性。

当使用 PostgreSQL 处理这些问题时，基本上有四种技术可以应用:

1.  优化您的应用程序代码，使查询更有效(最好使用更少的资源)。
2.  使用连接池来减少必要的数据库连接(和相关资源)的数量。
3.  跨多个数据库服务器平衡负载。
4.  共享你的数据库。

优化应用程序代码是我们过去两年一直在积极努力的事情，但这不是最终的解决方案。即使您提高了性能，当流量也增加时，您可能仍然需要应用其他两种技术。为了这篇文章的缘故，我们将跳过这个特定的主题，而是把重点放在其他技术上。

## 连接池

在 PostgreSQL 中，连接是通过启动一个操作系统进程来处理的，这反过来需要大量的资源。连接(以及进程)越多，数据库使用的资源就越多。PostgreSQL 还强制执行在 [max_connections](https://www.postgresql.org/docs/9.6/static/runtime-config-connection.html#GUC-MAX-CONNECTIONS) 设置中定义的最大连接数。一旦达到这个限制，PostgreSQL 将拒绝新的连接。这种设置可以用下图来说明:

![PostgreSQL Diagram](../Images/327ca750a2f9a78179c412c9ac4031be.png)

这里我们的客户端直接连接到 PostgreSQL，因此每个客户端需要一个连接。

通过连接池，我们可以让多个客户端连接重用 PostgreSQL 连接。例如，如果没有池，我们需要 100 个 PostgreSQL 连接来处理 100 个客户端连接；使用连接池，根据我们的配置，我们可能只需要 10 个左右的 PostgreSQL 连接。这意味着我们的连接图将看起来像下面这样:

![Connection Pooling Diagram](../Images/c8e109f5dcbb6bf7138044f8cb9368d8.png)

这里我们展示了一个例子，四个客户端连接到 pgbouncer，但是我们只需要其中两个，而不是使用四个 PostgreSQL 连接。

对于 PostgreSQL，有两种最常用的连接池:

pgpool 有点特殊，因为它不仅仅是连接池:它有一个内置的查询缓存机制，可以跨多个数据库平衡负载，管理复制等等。

另一方面，pgbouncer 要简单得多:它所做的只是连接池。

## 数据库负载平衡

数据库级的负载平衡通常是通过利用 PostgreSQL 的"[热备用](https://www.postgresql.org/docs/9.6/static/hot-standby.html)"特性来完成的。与不允许执行任何 SQL 查询的常规备用数据库相反，热备用数据库是一个 PostgreSQL 副本，允许您运行只读 SQL 查询。为了平衡负载，您需要设置一个或多个热备用服务器，并在这些主机上平衡只读查询，同时将所有其他操作发送到主服务器。扩展这样的设置相当容易:随着只读流量的增加，只需添加更多的热备用服务器(如果需要的话)。

这种方法的另一个好处是拥有更具弹性的数据库集群。即使主服务器出现问题，仅使用辅助服务器的 Web 请求也可以继续运行；当然，如果这些请求最终使用了主服务器，您仍然可能会遇到错误。

然而，这种方法很难实现。例如，显式事务必须在主节点上执行，因为它们可能包含写入。此外，在写入之后，我们希望继续使用主服务器一段时间，因为在使用异步复制时，热备用服务器上的更改可能还不可用。

## 分片

分片是对数据进行水平分区的行为。这意味着数据驻留在特定的服务器上，并使用碎片密钥进行检索。例如，您可以为每个项目划分数据，并使用项目 ID 作为分片键。当您有非常高的写入负载时(因为除了多主机设置之外没有其他简单的方法来平衡写入)，或者当您有许多数据并且您不能再以传统的方式存储它时(例如，您不能将它们全部放在一个磁盘上)，对数据库进行分片是很有趣的。

不幸的是，建立一个分片数据库的过程是一项巨大的任务，即使使用像 [Citus](https://www.citusdata.com/) 这样的软件。您不仅需要建立基础设施(其复杂程度取决于您是自己运行它还是使用托管解决方案)，还需要调整应用程序的大部分以支持分片。

### 反对分享的案例

在 GitLab.com 上，写负载通常非常低，大多数数据库查询都是只读查询。在非常特殊的情况下，我们可能会达到每秒 1500 个元组写入，但大多数时候我们只能勉强超过每秒 200 个元组写入。另一方面，我们可以很容易地在任何给定的二级上每秒读取多达 1000 万个元组。

在存储方面，我们也没有使用那么多数据:只有大约 800 GB。这些数据中有很大一部分是正在后台迁移的数据。一旦这些迁移完成，我们预计我们的数据库将会大大缩小。

然后是调整应用程序所需的工作量，以便所有查询都使用正确的碎片键。虽然我们的很多查询通常包含一个项目 ID，我们可以将它用作一个分片键，但是也有很多查询不是这样。分片还会影响向 GitLab 贡献变更的过程，因为每个贡献者现在都必须确保他们的查询中存在一个分片键。

最后，还有使所有这些工作所需的基础设施。必须设置服务器，必须增加监控，必须培训工程师，以便他们熟悉这种新的设置，这样的例子不胜枚举。虽然托管解决方案可以消除管理您自己的服务器的需要，但它不能解决所有问题。工程师仍需接受培训，账单(很可能非常昂贵)仍需支付。在 GitLab，我们也非常喜欢发布我们需要的工具，以便社区可以使用它们。这意味着如果我们要共享数据库，我们必须将它(或者至少它的一部分)放在我们的混合包中。你能确保你发布的东西工作的唯一方法是自己运行它，这意味着我们不能使用托管的解决方案。

最终，我们决定不分割数据库，因为我们觉得这是一个昂贵、耗时、复杂的解决方案。

## GitLab 的连接池

对于连接池，我们有两个主要要求:

1.  它必须运行良好(显然)。
2.  它必须易于在我们的综合包中发布，这样我们的用户也可以利用连接池。

对这两个解决方案(pgpool 和 pgbouncer)的审查分两步进行:

1.  执行各种技术测试(它能工作吗，配置起来有多容易，等等)。
2.  了解该解决方案的其他用户的体验，他们遇到了什么问题，以及他们是如何处理这些问题的，等等。

pgpool 是我们研究的第一个解决方案，主要是因为基于它提供的所有特性，它看起来非常有吸引力。我们测试的一些数据可以在[这个](https://gitlab.com/gitlab-com/infrastructure/issues/259#note_23464570)评论中找到。

基于许多因素，我们最终决定不使用 pgpool。例如，pgpool 不支持粘性连接。当执行写入并(试图)立即显示结果时，这是有问题的。想象一下，创建了一个问题并被重定向到页面，却遇到了 HTTP 404 错误，因为用于任何只读查询的服务器还没有数据。解决这个问题的一种方法是使用同步复制，但是这会带来许多其他问题；我们更想避免的问题。

另一个问题是，pgpool 的负载平衡逻辑与您的应用程序是分离的，并且通过解析 SQL 查询并将它们发送到正确的服务器来运行。因为这发生在应用程序之外，所以您很难控制哪个查询在哪里运行。这实际上可能对某些人有利，因为您不需要额外的应用程序逻辑，但这也阻止了您在必要时调整路由逻辑。

由于配置选项的数量太多，配置 pgpool 也被证明是相当困难的。也许棺材上的最后一颗钉子是我们从过去使用过 pgpool 的人那里得到的反馈。我们收到的关于 pgpool 的反馈通常是负面的，尽管在大多数情况下不是很详细。虽然大多数抱怨似乎与 pgpool 的早期版本有关，但它仍然让我们怀疑使用它是否是正确的选择。

反馈加上上述问题最终导致我们决定不使用 pgpool，而使用 pgbouncer。我们用 pgbouncer 进行了一组类似的测试，对它非常满意。它相当容易配置(而且一开始就没有那么多需要配置的东西)，相对容易发布，只关注连接池(而且做得非常好)，并且几乎没有(如果有的话)明显的开销。也许我唯一的抱怨是 pgbouncer 网站可能有点难以导航。

使用 pgbouncer，我们能够通过使用事务池将活动 PostgreSQL 连接的数量从几百个减少到 10-20 个。我们选择使用事务池，因为 Rails 数据库连接是持久的。在这样的设置中，使用会话池会使我们无法减少 PostgreSQL 连接的数量，从而带来很少(如果有的话)好处。通过使用事务池，我们能够将 PostgreSQL 的`max_connections`设置从 3000(这个特定值的原因一直不清楚)降到 300。pgbouncer 以这样一种方式配置，即使在峰值容量时，我们也只需要 200 个连接；为额外的连接提供了一些空间，如`psql`控制台和维护任务。

使用事务池的一个副作用是不能使用准备好的语句，因为`PREPARE`和`EXECUTE`命令可能会在不同的连接中运行；结果产生错误。幸运的是，当禁用预准备语句时，我们没有测量到响应时间的任何增加，但是我们*确实*测量到数据库服务器上的内存使用减少了大约 20 GB。

为了确保 web 请求和后台作业都有可用的连接，我们设置了两个单独的池:一个池有 150 个连接用于后台处理，另一个池有 50 个连接用于 web 请求。对于 web 请求，我们很少需要超过 20 个连接，但是对于后台处理，我们可以很容易地达到 100 个连接，这仅仅是因为 GitLab.com 上运行着大量的后台进程。

今天，我们将 pgbouncer 作为 GitLab EE 的高可用性包的一部分。有关更多信息，您可以参考[“Omnibus git lab PostgreSQL 高可用性”](https://docs.gitlab.com/ee/administration/postgresql/index.html)

## GitLab 的数据库负载平衡

随着 pgpool 及其负载平衡特性的消失，我们需要其他东西来将负载分散到多个热备用服务器上。

对于(但不限于)Rails 应用程序，有一个名为 [Makara](https://github.com/taskrabbit/makara) 的库，它实现了负载平衡逻辑，并包括 ActiveRecord 的默认实现。然而，Makara 有一些问题，这是我们的交易破坏者。例如，它对粘性连接的支持是非常有限的:当您执行写操作时，连接将使用 cookie 以固定的 TTL 连接到主连接。这意味着，如果复制延迟大于 TTL，您可能仍然会在没有所需数据的主机上运行查询。

Makara 还要求您进行大量配置，例如所有数据库主机及其角色，没有服务发现机制(我们当前的解决方案还不支持这一点，尽管它计划在不久的将来实现)。makara[似乎也不是线程安全的](https://github.com/taskrabbit/makara/issues/151)，这是个问题，因为 Sidekiq(我们使用的后台处理系统)是多线程的。最后，我们希望尽可能地控制负载平衡逻辑。

除了 Makara，还有内置了一些负载平衡机制的章鱼。然而，Octopus 面向数据库分片，而不仅仅是平衡只读查询。因此，我们没有考虑使用八达通。

最终，这导致我们将自己的解决方案直接构建到 GitLab EE 中。添加初始实现的合并请求可以在[这里](https://gitlab.com/gitlab-org/gitlab-ee/merge_requests/1283)找到，尽管后来应用了一些更改、改进和修复。

我们的解决方案基本上是通过用一个处理查询路由的代理对象替换`ActiveRecord::Base.connection`来工作的。这确保了我们可以对尽可能多的查询进行负载平衡，即使查询不是直接来自我们自己的代码。这个代理对象又根据所调用的方法决定将查询发送到哪个主机，从而消除了解析 SQL 查询的需要。

### 粘性连接

通过在执行写操作时存储一个指向当前 PostgreSQL WAL 位置的指针来支持粘性连接。然后，在请求结束时，这个指针会在 Redis 中存储一段时间。每个用户都有自己的密钥，这样一个用户的行为就不会影响到所有其他用户。在下一个请求中，我们获取指针，并将其与所有辅助节点进行比较。如果所有辅助节点的 WAL 指针都超过了我们的指针，我们就知道它们是同步的，我们可以安全地使用辅助节点进行只读查询。如果一个或多个辅助节点尚未同步，我们将继续使用主节点，直到它们同步。如果 30 秒内未执行任何写入，并且所有辅助节点仍未同步，我们将恢复使用辅助节点，以防止有人永远在主节点上运行查询。

检查辅助设备是否跟上非常简单，在`Gitlab::Database::LoadBalancing::Host#caught_up?`中实现如下:

```
def caught_up?(location)
  string = connection.quote(location)

  query = "SELECT NOT pg_is_in_recovery() OR " \
    "pg_xlog_location_diff(pg_last_xlog_replay_location(), #{string}) >= 0 AS result"

  row = connection.select_all(query).first

  row && row['result'] == 't'
ensure
  release_connection
end 
```

这里的大部分代码都是运行原始查询和获取结果的标准 Rails 代码。最有趣的部分是查询本身，如下所示:

```
SELECT NOT pg_is_in_recovery()
OR pg_xlog_location_diff(pg_last_xlog_replay_location(), WAL-POINTER) >= 0 AS result" 
```

这里的`WAL-POINTER`是 PostgreSQL 函数`pg_current_xlog_insert_location()`返回的 WAL 指针，它在主服务器上执行。在上面的代码片段中，指针作为参数传递，然后被引用/转义并传递给查询。

使用函数`pg_last_xlog_replay_location()`我们可以得到一个次节点的 WAL 指针，然后我们可以使用`pg_xlog_location_diff()`将它与主节点的指针进行比较。如果结果大于 0，我们知道辅助节点处于同步状态。

添加检查`NOT pg_is_in_recovery()`是为了确保当我们正在检查的辅助节点*刚刚*升级为主节点，而我们的 GitLab 进程还不知道这一点时，查询不会失败。在这种情况下，我们只需返回`true`,因为主节点总是与自身同步。

### 后台处理

我们的后台处理代码*总是*使用主进程，因为在后台执行的大部分工作都是写操作。此外，我们无法可靠地使用热备用，因为我们无法知道某个作业是否应该使用主服务器，因为许多作业与用户没有直接联系。

### 连接错误

为了处理连接错误，我们的负载平衡器不会使用被视为离线的辅助节点，此外，任何主机(包括主节点)上的连接错误都会导致负载平衡器重试几次操作。这确保了在出现问题或数据库故障转移时，我们不会立即显示错误页面。虽然我们也在负载均衡器级别处理[热备用冲突](https://www.postgresql.org/docs/current/static/hot-standby.html#HOT-STANDBY-CONFLICT)，但我们最终在辅助节点上启用了`hot_standby_feedback`，因为这样做解决了所有热备用冲突，而不会对表膨胀产生任何负面影响。

我们使用的过程非常简单:对于辅助节点，我们将重试几次，中间没有延迟。对于主服务器，我们将使用指数回退重试该操作几次。

要了解更多信息，您可以参考 GitLab EE 中的源代码:

GitLab 9.0 首次引入了数据库负载平衡，只有*支持 PostgreSQL。更多信息可以在 [9.0 发布帖](/2017/03/22/gitlab-9-0-released/)和[文档](https://docs.gitlab.com/ee/administration/database_load_balancing.html)中找到。*

## 杂乱的数据

在努力实现连接池和负载平衡的同时，我们还在处理[易碎的数据](https://www.crunchydata.com/)。直到最近，我还是唯一的[数据库专家](/handbook/engineering/infrastructure/database/)，这意味着我有很多工作要做。此外，我对 PostgreSQL 内部及其广泛设置的了解是有限的(或者至少在当时是如此)，这意味着我能做的也就这么多。因此，我们雇佣 Crunchy 来帮助我们识别问题，调查缓慢的查询，提出模式优化，优化 PostgreSQL 设置，等等。

在合作期间，大部分工作都是在保密的情况下进行的，所以我们可以共享私人数据，比如日志文件。随着合作接近尾声，我们已经移除了其中一些问题的敏感信息，并向公众开放。主要问题是[git lab-com/infra structure # 1448](https://gitlab.com/gitlab-com/infrastructure/issues/1448)，这反过来又导致了许多独立问题的产生和解决。

这种合作的好处是巨大的，因为它帮助我们发现并解决了许多问题，如果我必须独自完成这些工作，我可能需要几个月才能发现并解决这些问题。

幸运的是，我们最近设法雇佣了我们的第二名数据库专家，我们希望在未来几个月里进一步壮大这个团队。

## 结合连接池和数据库负载平衡

将连接池和数据库负载平衡结合起来，使我们能够大幅减少运行数据库集群所需的资源数量，并将负载分散到热备用服务器上。例如，我们的主服务器现在的 CPU 利用率几乎不变，为 70%，通常徘徊在 10%到 20%之间，而我们的两台热备用服务器大部分时间都徘徊在 20%左右:

![CPU Percentage](../Images/702317e4f6ff4f9b3e648bf494374cd9.png)

这里`db3.cluster.gitlab.com`是我们的主主机，而另外两台主机是我们的辅助主机。

其他与负载相关的因素，如平均负载、磁盘使用量和内存使用量，也得到显著改善。例如，主服务器的平均负载不是 20 左右，而是刚刚超过 10:

![CPU Percentage](../Images/96534b253281021a3bf3328f7c31e45e.png)

在最繁忙的时候，我们的辅助服务器每秒处理大约 12 000 个事务(大约每分钟 740 000 个)，而主服务器每秒处理大约 6 000 个事务(大约每分钟 340 000 个):

![Transactions Per Second](../Images/eac66cca0f14059195f2a504c2a92ff6.png)

不幸的是，在部署 pgbouncer 和我们的数据库负载平衡器之前，我们没有任何关于事务率的数据。

我们的 PostgreSQL 统计数据的最新概述可以在我们的 [public Grafana dashboard](https://dashboards.gitlab.com/dashboard/db/postgresql-overview?refresh=5m&orgId=1) 中找到。

我们为 pgbouncer 设置的一些设置如下:

| 环境 | 价值 |
| --- | --- |
| 默认池大小 | One hundred |
| 预留池大小 | five |
| 预留池超时 | three |
| 最大客户连接 | Two thousand and forty-eight |
| 池模式 | 交易 |
| 服务器空闲超时 | Thirty |

尽管如此，仍有一些工作要做，例如:实现服务发现( [#2042](https://gitlab.com/gitlab-org/gitlab-ee/issues/2042) )，改进我们检查辅助节点是否可用的方式( [#2866](https://gitlab.com/gitlab-org/gitlab-ee/issues/2866) )，以及忽略远远落后于主节点的辅助节点( [#2197](https://gitlab.com/gitlab-org/gitlab-ee/issues/2197) )。

值得一提的是，我们目前没有任何计划将我们的负载平衡解决方案转变为一个可以在 GitLab 之外使用的独立库，相反，我们的重点是为 GitLab EE 提供一个可靠的负载平衡解决方案。

如果这引起了你的兴趣，你喜欢使用数据库，提高应用程序性能，并向 GitLab 添加与数据库相关的功能(如[服务发现](https://gitlab.com/gitlab-org/gitlab-ee/issues/2042))，你一定要查看[的职位空缺](/job-families/engineering/database-engineer/)和[数据库专家手册条目](/handbook/engineering/infrastructure/database/)以了解更多信息。

