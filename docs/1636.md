# Spotify 后端基础设施- Spotify 工程:Spotify 工程

> 原文:[https://labs . Spotify . com/2013/03/15/back end-infra structure-at-Spotify/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://labs.spotify.com/2013/03/15/backend-infrastructure-at-spotify/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

# Spotify 的后端基础设施

![](../Images/55ea483017cebbf7d90ea03bf474e87d.png)

2013 年 3 月 15 日 由 Spotify 工程 发布

Spotify 的后端基础设施。我们的后端基础设施正在进行中，在某些领域我们已经取得了很大进展，而在其他领域我们才刚刚起步。

为了理解我们为什么要建立这个基础设施，我们需要了解一些 Spotify 开发组织如何工作的背景。我们目前在 Spotify 有大约 300 名工程师，而且我们正在快速增长。

# 背景

增长——在 Spotify，事物总是在增长。日常用户数量、支持服务的后端节点数量、我们客户运行的硬件平台数量、使用我们产品的开发团队数量、我们平台上托管的外部应用数量、我们目录中的歌曲数量。

速度——随着我们的成长，我们必须小心翼翼地绕开许多可能降低我们开发速度的事情。我们尽最大努力消除团队之间的依赖，并从我们的架构中移除不必要的复杂性。

自治团队 Spotify 的一个关键理念是每个开发团队都应该是自治的。一个开发团队(或 Spotify 行话中的“团队”——见[http://blog . crisp . se/2012/11/14/henrikkniberg/scaling-agile-at-Spotify](http://blog.crisp.se/2012/11/14/henrikkniberg/scaling-agile-at-spotify))应该总是能够独立于其他团队行动。即使两支球队之间存在依赖，依赖的球队总会有前进的道路。为了让所有的团队都取得进步，即使他们依赖于另一个团队，我们有一些战略想法，我们试图在任何地方应用:我们的透明代码模型和我们的自助服务基础设施。

透明代码模型 Spotify 的所有代码都以透明的代码模型提供给所有开发人员。这意味着 Spotify 客户端、Spotify 后端和 Spotify 基础设施中的所有代码可供 Spotify 的所有开发人员阅读或更改。如果一个团队阻止另一个团队对某些代码进行修改，他们总是可以选择自己进行修改。

实际上，Spotify 的透明代码模式适用于所有共享同一个集中式 git 服务器的团队。每个 git repo 都有一个专门的系统所有者来管理代码，确保它不会腐烂。透明的代码模型确保每个人都可以一直进步，并且每个人都可以访问每个人的代码。这让 Spotify 一直向前，给人一种积极开放的工作环境。

自助服务基础架构—所需的所有基础架构都应作为自助服务实体提供。这样，就不需要等待另一个团队来获取硬件、设置存储集群或进行配置更改。Spotify 后端基础设施由几层硬件和软件组成，从物理机器到消息和存储解决方案。

开源——我们尽可能使用开源工具。由于 Spotify 不断推动我们在后端使用的软件的可扩展性极限，我们需要能够改进我们在关键领域使用的软件。我们为我们使用的许多开源项目做出了贡献，例如 Apache Cassandra 和 ZMQ。我们几乎不使用专有软件，仅仅是因为我们无法相信我们能够根据不断增长的需求来定制软件。

文化——在 Spotify，我们坚信被授权的个人。我们用自治团队在我们的组织中反映这一点。对于工程师来说，有很多可能性可以转移到 Spotify 的其他领域工作，以确保每个人都对自己的工作保持热情。我们定期举办黑客日，人们可以尝试他们的任何想法。

# 体系结构

任何需要处理 Spotify 用户量的架构都需要解决这个问题。Spotify 架构以几种不同的方式解决这个问题。首先，按特征划分。一个有点过于简单的描述是，我们客户端中所有页面和视图的所有物理屏幕区域都归某个团队所有。Spotify 客户端的所有功能都属于一个特定的团队。该团队负责所有平台上的这一功能——从它在 iOS 设备或浏览器上的显示方式，到 Spotify 后端处理的实时请求，再到在我们的 Hadoop 集群中进行的面向批处理的数据处理，以支持推荐、广播和搜索等功能。

如果一个功能失效，我们客户端的其他功能是独立的，将继续工作。如果功能之间的依赖性很弱，一个功能的故障有时可能会导致另一个功能的服务降级，但不会导致整个 Spotify 服务失败。

由于所有用户不会同时使用所有功能，因此特定功能的后端必须处理的用户数量通常远远小于整个 Spotify 服务的用户数量。

由于围绕某一特定功能的所有知识都集中在一个团队中，因此很容易 A/B 测试功能、查看收集的数据并与所有相关人员一起做出明智的决定。

特性划分提供了可伸缩性、可靠性和聚焦团队工作的有效方式。

# 后端基础设施

在将我们的问题按功能划分后，给一个高技能的跨职能团队一个任务来处理和使用该功能，现在的问题变成了，我们如何构建基础设施来有效地支持该团队？

我们如何确保团队能够以极快的速度开发他们的功能，而不冒被其他团队阻止的风险？我们的基础设施如何解决全球扩展的难题？我已经谈到了我们的透明代码模型，它总是允许一个团队前进，但是除了特性开发团队之外，还有组织的其他部分。

在许多组织中，您有数据库管理员来管理数据库及其模式，您通常需要通过运营部门来获得数据中心的硬件分配，等等。当有 100 个团队同时需要他们的服务时，组织中的这些特殊功能就成了瓶颈。为了解决这个问题，我们正在 Spotify 上开发一个完全自助服务的后端基础设施。完全自助服务意味着任何团队都可以在现场环境中开始开发和迭代服务，而不必与组织的其余部分进行交互。

为了实现这一目标，我们需要解决几个不同领域的一系列问题。我将在这里讨论几个重要的问题。

供应–在开发新功能时，一个团队通常需要在几个地方部署该服务。我们正在建设基础设施，以使团队能够自行决定该服务是否应该部署在 Spotify 自己的数据中心，或者该功能是否可以使用公共云产品。Spotify 基础设施努力将在我们自己的数据中心和公共云上运行之间的差异最小化。简而言之，您可以在我们自己的数据中心获得更好的延迟和更稳定的环境。在公共云上，您可以获得更快的硬件配置和更多的动态扩展可能性。

Spotify 客户端连接到最近的数据中心。

存储——大多数功能都需要某种存储，播放列表和“关注”功能就是明显的例子。为数百万人将使用的功能构建存储解决方案并不是一件容易的事情，需要考虑的事情很多:访问模式、站点间的故障转移、容量、一致性、备份、站点间净拆分情况下的性能下降等。没有简单的方法可以通用地满足所有这些需求。对于每项功能，该团队都必须创建一个满足特定服务需求的存储解决方案。Spotify 基础设施提供了几种不同的存储选项:Cassandra、PostgreSQL 和 memcached。

如果特性的数据需要被分割，那么团队必须在他们的服务中实现分片，然而许多服务依赖于 Cassandra 在站点之间做数据的完全复制。设置具有站点间复制和故障转移功能的完整存储集群非常复杂，因此我们正在构建基础架构，以将多站点 Cassandra 或 postgreSQL 集群作为一个单元进行设置和维护。对于在 Spotify API 上构建应用程序的人来说，将有一个存储即服务选项，不需要任何集群设置。存储即服务选项将仅限于非常简单的键值存储。

消息传递——Spotify 客户端和后端服务使用以下范例进行通信:请求-回复、消息传递和 pubsub。我们已经构建了自己的低延迟、低开销消息传递层，并计划通过高交付保证、故障转移路由和更复杂的负载平衡来扩展它。

容量规划 Spotify 的增长推动大量流量流向后端。每个小队必须确保他们的功能总是与当前的负载相适应。该团队可以选择通过监控其服务的流量来手动跟踪这种情况，并识别和修复瓶颈，根据需要进行扩展。我们还在构建一个基础设施，允许团队根据负载自动扩展服务。自动扩展通常只对你知道的瓶颈有效，所以总是有一定程度的人工监控需要团队来处理。我们的基础设施允许轻松创建图表和警报来支持这一点。

[![Image](../Images/40f2bb2bf55683f33f55bb25d4b3a582.png)](https://storage.googleapis.com/production-eng/1/2013/03/graph.png)

与其他服务隔离——随着新功能和服务的开发，它们倾向于以非平凡的方式相互调用。非常重要的一点是，所有小队都觉得自己可以全速运行，同时将对 Spotify 其他部分产生负面影响的风险降至最低。为了避免这种情况，我们的消息传递层有一个速率限制和权限系统。速率限制有一个默认的阈值——这允许小队调用其他服务来尝试一些东西。如果预计交通异常繁忙，各小组需要协调并同意如何一起处理这种情况。不同的功能总是在不同的服务器或虚拟机上运行，以避免一个行为不当的服务关闭另一个。

# 包裹

正如我在这篇文章的开头提到的，很多工作正在进行中，还有很多非常有趣的挑战等待着我们。我在这里展示的观点代表了我们现在对 Spotify 的看法，由于我们沉迷于持续改进，明天我们很可能会改变一些事情…

当然，如果你觉得这很有趣，可以看看我们的[公开职位](http://www.spotify.com/us/jobs/view/oGaNWfwj/)。

标签:

[Apache Cassandra](https://engineering.atspotify.com/tag/apache-cassandra/)

,

[architecture](https://engineering.atspotify.com/tag/architecture/)

,

[PostgreSQL](https://engineering.atspotify.com/tag/postgresql/)