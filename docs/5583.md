# 价格情报:抓取 1000 亿个产品页面

> 原文：<https://blog.scrapinghub.com/web-scraping-at-scale-lessons-learned-scraping-100-billion-products-pages?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

 如今，网络抓取看起来似乎很容易。有许多开源库/框架、可视化抓取工具和[数据提取工具](/data-extraction/)，使得从网站抓取数据变得非常容易。然而，当你想要大规模抓取网站时，事情就变得非常棘手，非常快。尤其是当涉及到[价格情报](/price-intelligence/)时，规模和质量非常重要。

在这一系列文章中，我们将与您分享自 2010 年以来我们从超过 1000 亿个产品页面中获得的经验教训，让您深入了解从电子商务商店中大规模提取产品数据时将面临的挑战，并与您分享一些应对这些挑战的最佳实践。

在这篇系列文章的第一篇中，我们将概述你在大规模抓取产品数据时将面临的主要挑战，以及 Zyte(以前的 Scrapinghub)从抓取 1000 亿个产品页面中学到的经验。

Zyte(前身为 Scrapinghub)成立于 2010 年，是领先的数据提取公司之一，也是当今最强大、最受欢迎的网络抓取框架 Scrapy 的作者。目前，Zyte(以前的 Scrapinghub)每月为世界上许多最大的电子商务公司抓取超过 80 亿个页面(其中 30 亿个是产品页面)。

对于那些对大规模抓取网页感兴趣，但正在纠结是否应该在内部建立一个专门的网页抓取团队或将其外包给专门的网页抓取公司的人，请务必查看我们的免费指南， [**企业网页抓取:大规模抓取网页指南**](/whitepaper-ebook/scale-your-web-scraping/) 。

## **刮垢时有什么重要的？**

与标准的 web 抓取应用程序不同，大规模抓取电子商务产品数据有一系列独特的挑战，使 web 抓取变得更加困难。

本质上，这些挑战可以归结为两件事:**速度**和**数据质量**。

由于时间通常是一个限制性的约束，大规模的抓取需要你的爬虫在不影响数据质量的情况下以非常高的速度抓取网页。这种对速度的需求使得收集大量产品数据变得非常困难。

## **挑战 1——草率且总是改变网站格式**

这可能是显而易见的，也可能不是最性感的挑战，但草率和总是改变网站格式是你在大规模提取数据时将面临的最大挑战。不一定是因为任务的复杂程度，而是你要花在处理这件事上的时间和资源。

如果你花过时间为电子商务商店构建爬虫，你会知道在电子商务商店中有一种流行的草率代码。这不仅仅是 HTML 格式良好或偶尔的字符编码问题。这些年来，我们遇到了各种各样的问题——误用 HTTP 响应代码、损坏的 JavaScripts 或误用 Ajax:

*   网站升级后，当商店停止生产产品并突然开始在其 404 错误处理程序上返回 200 个响应代码时，会删除页面。
*   错误转义的 JSON 数据会破坏某些页面上的 javascript(例如' b0rk 'd ')，导致您需要使用正则表达式来抓取这些数据。
*   滥用 Ajax 调用的商店如此之多，以至于您只能通过呈现页面(导致爬行速度慢得多)或模仿 API 调用(导致更多的开发工作)来获得您想要的信息。

像这样草率的代码会使编写蜘蛛程序变得很痛苦，但也会使可视化的抓取工具或自动提取工具变得不可行。

当进行大规模抓取时，你不仅需要浏览数百个代码松散的网站，还需要处理不断发展的网站。一个很好的经验法则是，期望你的目标网站每 2-3 个月做出改变，打破你的蜘蛛(下降数据提取覆盖面或质量)。

这听起来可能没什么大不了的，但是当你大规模刮削时，这些事件真的会越积越多。例如，Zyte 的一个较大的电子商务项目有大约 4000 个蜘蛛，目标是大约 1000 个电子商务网站，这意味着他们每天可以经历 20-30 个蜘蛛失败。

地区性和多语言网站的网站布局变化、A/B 分割测试和包装/定价变化也产生了一系列问题，经常会破坏蜘蛛。

### **没有简单的解决方案**

不幸的是，没有灵丹妙药可以彻底解决这些问题。很多时候，这只是随着规模的扩大，向项目投入更多资源的问题。再次以之前的项目为例，该项目有一个由 18 名全职爬虫工程师和 3 名专门的 QA 工程师组成的团队，以确保客户端始终拥有可靠的数据馈送。

然而，随着经验的积累，你的团队将学会创建更加健壮的蜘蛛，能够检测和处理你的目标网站格式中的怪癖。

目标网站可能会使用多个蜘蛛来处理所有可能的布局，而不是使用多个蜘蛛，最佳实践是只使用一个产品提取蜘蛛来处理不同页面布局使用的所有可能的规则和方案。你的蜘蛛越可配置越好。

虽然这些做法会让你的蜘蛛更复杂(我们的一些蜘蛛长达数千行)，但它们会确保你的蜘蛛更容易维护。

## **挑战 2:可扩展架构**

您将面临的下一个挑战是构建一个爬行基础设施，它将随着每天请求数量的增加而扩展，而不会降低性能。

当大规模提取产品数据时，一个简单的连续抓取和抓取数据的网络爬虫是不够的。通常，一个串行的 web scraper 会在一个循环中一个接一个地发出请求，每个请求需要 2-3 秒才能完成。

如果您的爬虫每天只需要发出少于 40，000 个请求(每 2 秒的请求等于每天 43，200 个请求)，这种方法就可以了。然而，过了这一步，您将需要过渡到一个爬行架构，它将允许您在不降低性能的情况下每天抓取数百万个请求。

由于这个主题本身需要一篇文章，在接下来的几周里，我们将发表一篇专门的文章，讨论如何设计和构建您自己的高吞吐量抓取架构。然而，在本节的剩余部分，我们将讨论一些更高层次的原则和最佳实践。

正如我们已经讨论过的，在大规模收集产品数据时，速度是关键。你需要确保你能在规定的时间内(通常是一天)找到并抓取所有需要的产品页面。为此，您需要执行以下操作:

### **将产品发现与产品提取分开**

为了大规模收集产品数据，你需要将产品发现蜘蛛和产品提取蜘蛛分开。

产品发现蜘蛛的目标应该是导航到目标产品类别(或“货架”)，并为产品提取蜘蛛存储该类别中产品的 URL。当产品发现蜘蛛将产品 URL 添加到队列中时，产品提取蜘蛛从该产品页面中抓取目标数据。

这可以借助爬行前沿来完成，比如 Zyte 开发的开源爬行前沿 [Frontera](https://github.com/scrapinghub/frontera) 。虽然 Frontera 最初是为 Scrapy 设计的，但它是完全不可知的，可以用于任何其他爬行框架或独立项目。在本指南中，我们分享你如何[使用 Frontera 进行大规模刮削](http://frontera.readthedocs.io/en/latest/topics/cluster-setup.html)。

### **为产品提取分配更多资源**

由于每个产品类别“货架”可以包含 10 到 100 个产品，并且提取产品数据比提取产品 URL 更耗费资源，因此发现蜘蛛通常比产品提取蜘蛛运行得更快。在这种情况下，对于每个发现蜘蛛，您需要有多个提取蜘蛛。一个好的经验法则是为每个大约 100，000 页的存储桶创建一个单独的提取蜘蛛。

## **挑战 3:保持吞吐量性能**

大规模刮擦很容易与一级方程式赛车相提并论，在一级方程式赛车中，你的目标是以速度的名义，从汽车上刮去每一克不必要的重量，并从发动机上挤出最后一点马力。大规模的网页抓取也是如此。

当提取大量数据时，您总是在寻找最小化请求周期时间和最大化可用硬件资源的蜘蛛性能的方法。所有这一切都是希望您可以为每个请求节省几毫秒的时间。

要做到这一点，您的团队需要深入了解 web 抓取框架、代理管理和您正在使用的硬件，以便您可以调整它们以获得最佳性能。您还需要关注:

### **抓取效率**

在大规模抓取时，您应该始终专注于在尽可能少的请求中提取您需要的确切数据。任何额外的请求或数据提取都会降低你抓取网站的速度。在设计蜘蛛时，请记住这些提示:

*   只有在万不得已的情况下，才使用 Splash 或 Puppeteer 等无头浏览器来渲染 javascript。爬行时用无头浏览器渲染 javascript 是非常耗费资源的，并且严重影响了爬行的速度。
*   如果你能从货架页面获得你需要的数据(如产品名称、价格、评级等)。)而不请求每个单独的产品页面，那么就不要请求产品页面。
*   除非万不得已，否则不要请求或提取图像。

## **挑战 4:反 Bot 对策**

如果你正在大规模地搜索电子商务网站，你肯定会遇到采用反机器人对策的网站。

对于大多数较小的网站，他们的反机器人对策将是非常基本的(禁止 IP 地址发出过多的请求)。但是，较大的电子商务网站如亚马逊等。利用复杂的反 bot 对策，如 Distil Networks、Incapsula 或 Akamai，这使得提取数据变得更加困难。

### **代理人**

记住这一点，任何大规模收集产品数据的项目的第一个也是最基本的要求是使用代理 IP。在大规模抓取时，您将需要一个相当大的代理列表，并且需要实现必要的 IP 轮换、请求节流、会话管理和黑名单逻辑，以防止您的代理被阻止。

除非你已经有或者愿意委托一个相当大的团队来管理你的代理，否则你应该外包这部分的搜集过程。有大量的代理服务可以提供不同级别的服务。

然而，我们的建议是选择一个代理提供商，它可以为代理配置提供一个端点，并隐藏管理代理的所有复杂性。大规模的抓取是资源密集型的，不需要通过开发和维护自己的内部代理管理基础设施来重新发明轮子。

这是大多数大型电子商务公司使用的方法。许多世界上最大的电子商务公司使用 Zyte 开发的智能下载器 Zyte 智能代理管理器(以前的 Crawlera) ，完全外包他们的代理管理。当你的爬虫每天发出 2000 万个请求时，专注于分析数据比管理代理更有意义。

### **超越代理**

不幸的是，仅仅使用代理服务并不足以确保你可以在大型电子商务网站上规避 bot 对策。越来越多的网站正在使用复杂的反机器人对策，监控你的爬虫行为，以检测它不是一个真正的人类访问者。

这些反机器人措施不仅使抓取电子商务网站变得更加困难，如果处理不当，克服这些措施会大大降低爬虫的性能。

这些 bot 对策中有很大一部分使用 javascript 来确定请求是来自爬虫还是人类(Javascript 引擎检查、字体枚举、WebGL 和 Canvas 等。).

然而，如前所述，在大规模抓取时，您需要限制使用可脚本化的无头浏览器，如 Splash 或 Puppeteer，它们会在页面上呈现任何 javascript，因为它们占用大量资源，降低了您抓取网站的速度。

这意味着，为了确保您可以从您的蜘蛛获得必要的吞吐量来交付日常产品数据，您通常需要煞费苦心地对网站上使用的反机器人对策进行逆向工程，并设计您的蜘蛛来抵消它们，而不使用无头浏览器。

## **挑战 5:数据质量**

从数据科学家的角度来看，任何 web 抓取项目最重要的考虑因素是提取的数据的质量。大规模搜集只会让对数据质量的关注变得更加重要。

当每天提取数百万个数据点时，不可能手动验证您的所有数据都是干净和完整的。脏的或不完整的数据很容易渗入到您的数据馈送中，破坏您的数据分析工作。

在同一商店的多个版本(不同语言、地区等)上抓取产品时尤其如此。)或者单独开店。

在构建网络蜘蛛的设计阶段，除了仔细的质量保证过程之外，网络蜘蛛的代码经过同行评审和测试，以确保它以最可靠的方式提取所需的数据。确保最高数据质量的最佳方法是开发自动化质量保证监控系统。

作为任何数据提取项目的一部分，您需要计划和开发一个监视系统，该系统将提醒您任何数据的不一致性和 spider 错误。在 Zyte，我们开发了机器学习算法，旨在检测:

*   **数据验证错误** -每个数据项都有一个定义的数据类型和值，它们遵循一致的模式。我们的数据验证算法将向项目 QA 团队标记任何与该数据类型的预期不一致的数据项，从这一点开始，数据将被手动检查和警报验证或标记为错误。
*   **产品变异错误** -从同一网站的多个版本(不同语言、地区等)抓取相同产品数据时。)可变值和假定固定值(如产品重量或尺寸)可能会发生变化。这可能是一个网站的反机器人对策给你的一个或多个爬虫虚假信息的结果。同样，您需要有适当的算法来识别和标记任何此类事件。
*   **基于卷的不一致性-** 另一个关键的监控脚本是检测返回的记录数量的任何异常变化的脚本。这可能意味着该网站已经发生了变化，或者您的爬虫正在接受伪造的信息。
*   **站点变化-** 目标网站发生结构变化是爬虫崩溃的主要原因。这是由我们的专用监控系统，相当积极地监测。该工具对目标站点执行频繁的检查，以确保自上次爬网以来没有发生任何变化。如果发现更改，它会发出相同的通知。

所有这些我们将在后面专门讨论自动化质量保证的文章中讨论。

## **收拾东西**

正如您所见，大规模收集产品数据会带来一系列独特的挑战。希望这篇文章让你更加清楚你将面临的挑战，以及你应该如何着手解决它们。

然而，这只是这个系列的第一篇文章，所以如果你有兴趣在下一篇文章发表后马上阅读，请务必注册我们的电子邮件列表。

对于那些对大规模抓取网页感兴趣，但正在纠结是否应该在内部建立一个专门的网页抓取团队或将其外包给专门的网页抓取公司的人，请务必查看我们的指南， **[企业网页抓取:内部建立或外包](/whitepaper-ebook/web-scraping-become-too-hard-to-handle/)** 。

在 Zyte，我们专注于将非结构化网络数据转化为结构化数据。如果您想了解更多关于如何在您的业务中使用网络搜集的产品数据，请随时[联系我们的销售团队，](/data-extraction/)他们将向您介绍我们为初创公司和财富 100 强公司提供的服务。

在 Zyte，我们总是喜欢听我们的读者对我们内容的看法，以及你可能提出的任何问题。所以请在下面留下你对这篇文章的看法和你正在做的事情。