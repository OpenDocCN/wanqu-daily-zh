# Parse 的 AWS 和 MongoDB 基础设施:经验教训| Felix ge sert | Speed Kit 博客

> 原文：<https://medium.baqend.com/parse-is-gone-a-few-secrets-about-their-infrastructure-91b3ab2fcf71?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

# Parse 的 AWS 和 MongoDB 基础设施:经验教训

这是一个评论的扩展形式，在 Hackernews 上引起了一些兴趣。一年的宽限期过后，Parse 现在离线了。这是一个学习和技术决策的集合，可能对其他运行云服务的公司有用。至少，它直接影响了我们自己的后端即服务的设计。

这里有一些不太为人所知或没有发表的事实和琐事，是我通过与现在在脸书工作的解析工程师交谈收集到的。因为我不确定他们是否被允许分享这些信息，所以我不会提到他们的名字。



Parse is [offline](https://status.parse.com/incidents/6mpkbscqw6p9)



## 用户和牵引力

*   **部署了 100 万个应用**来解析。
*   最大的 Parse 应用拥有 4000 万用户。



The largest Parse customer only used it for Push notifications



*   Parse 是世界上最大的 MongoDB 用户之一
*   《王者冲突》对**推送通知**使用了 Parse，大约占了通过 Parse 的所有推送的一半。出于可伸缩性的考虑，他们从不移动任何其他部分来解析。
*   脸书收购 Parse 的最初原因是**推广他们的移动 SDK**并创造与移动广告的协同效应。Parse 经常和脸书广告公司一起打包出售。
*   以每秒保证的**个请求**来衡量的静态定价模式效果不佳。
*   商业问题:人们倾向于留在自由层。
*   技术问题一:复杂的**限速**。如果一分钟内超过限制 60 倍，请求将被丢弃。使用共享的 **Memcache** 实例跟踪限制。结果:当开发人员在 API 中遇到速率限制时，他们增加了重试次数。重试在解析后端产生了巨大的负载。
*   技术问题 II:真正的问题和瓶颈不是 API 服务器，而是几乎总是共享的 MongoDB 数据库集群。

## 解析服务器

*   一开始服务器是 Rails(最多 24 个线程)。并发性)，每台服务器的吞吐量非常小(每秒大约 15–30 个请求)
*   服务器后来在 Go 中**被重写。开源的[解析服务器](https://github.com/ParsePlatform/parse-server)是用 Node.js 编写的，缺少 Go 中原始解析服务器的很多功能。**
*   后端完全在亚马逊网络服务上
*   计划将 Parse 迁移到**脸书的基础设施**(例如[草堆](https://code.facebook.com/posts/685565858139515/needle-in-a-haystack-efficient-storage-of-billions-of-photos/)、[陶](https://www.usenix.org/node/174510)、 [F4](https://www.usenix.org/conference/osdi14/technical-sessions/presentation/muralidhar) 、[扩展阿帕奇吉拉夫](https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/)、[大猩猩](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf))但是该项目被放弃
*   大约 8 名开发人员开发 SDK，8 名开发服务器，8 名开发人员+一些工程师

## 数据库ˌ资料库

*   超过 40 个 MongoDB 副本集，每个副本集有 3 个节点



Parse went for RocksDB as their primary storage engine.



*   存储引擎: **RocksDB** (即 [MongoRocks](https://github.com/mongodb-partners/mongo-rocks) )，一个基于[日志结构合并树](https://en.wikipedia.org/wiki/Log-structured_merge-tree)的仅追加引擎(类似于例如 Cassandra、HBase、CouchDB、LevelDB、WiredTiger、TokuDB)。原因:在 WiredTiger 的合同中，对多个集合有更好的处理，wired tiger 为每个集合使用一个文件。就空间而言，压缩率提高了 2–3 倍。在延迟/滞后方面，写入和复制也更加高效。从 MMap 到 MongoRocks 的迁移是通过添加一个 MongoRocks 的副本来完成的，这个副本后来被提升为新的主副本。
*   仅使用了带有 SSD 的**实例存储，没有 EBS。**
*   无分片:使用 MongoDB 的主数据库逻辑，每个租户被静态映射到**恰好一个副本集**。
*   Mongo **写问题是 1** (!)，即写入在复制之前得到确认。一些人抱怨丢失的数据和陈旧的读取
*   **出于性能原因，允许从机读取**
*   **部分更新**有问题，因为对大文档的小更新在写入操作日志时会得到[写放大](https://en.wikipedia.org/wiki/Write_amplification)
*   AWS EC2 上频繁(每日)**大师改选**。回滚文件被丢弃，导致数据丢失
*   开发了一个特殊的“**闪回”工具**,用于记录工作负载，这些工作负载可以在以后重新运行以进行内部负载和功能测试
*   JS 在**分叉 V8 引擎**中运行，对用户提供的代码实施 15 秒的执行限制
*   **无分片自动化**:面向大客户的手动、易错流程
*   **索引未公开**:从慢速查询日志自动生成基于规则的内容。对于较大的应用程序效果不佳。
*   **慢速查询**被轮询 Mongos currentOp 并维护每(API-key，查询模板)组合限制的 cron 作业杀死
*   **备份**:如果重要客户因人为错误丢失数据，脸书的工程师将从定期备份中手动恢复
*   **对象级 ACL 系统**效率非常低。需要 Numerours 索引，有时可能超过实际数据大小 3-4 倍。
*   由于没有**并发控制**的机制(除了对计数器之类的东西的最小支持)，应用程序经常不一致

# 哪些解析应该有所不同

Parse 做了很多正确的事情。文档很棒，移动 SDK 很可靠，web UIs 设计得很好。然而，他们有一个不言而喻的价值体系，不信任他们的用户处理复杂的数据库和架构问题。

从数据库背景来看，我们的想法是开发人员应该了解模式和索引之类的细节(事后看来，解析工程师非常赞同)。此外，我们认为后端服务并不局限于移动应用，而是对网络非常有用。

我认为提供商应该公开他们的基础设施和权衡，Parse 只是在它已经失败之后才这样做。

如果你对这个想法感兴趣，可以看看 [Baqend](https://www.baqend.com/) 。它是一个高性能的 BaaS，通过透明缓存关注 web 性能，通过自动分片和多语言持久性关注可伸缩性。



We strongly believe that architecture should not be a secret.

