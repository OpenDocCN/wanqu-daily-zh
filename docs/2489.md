# 谷歌 SRE 图书

> 原文：<https://danluu.com/google-sre-book/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

<main>

这本书以一个故事开始，故事发生在阿波罗计划时期，玛格丽特·哈米尔顿带着她的小女儿来到美国国家航空航天局。在一次模拟任务中，她的女儿在模拟任务中按下了一些导致预发射程序运行的键，导致任务崩溃。Hamilton 提交了一个更改请求，以添加错误检查代码来防止错误再次发生，但该请求被拒绝，因为该错误案例不应发生。

在接下来的阿波罗 8 号任务中，出现了确切的错误情况，一个本来可以通过一个微不足道的检查来防止的潜在致命问题花了美国宇航局的工程师 9 个小时来解决。

这听起来很熟悉——我已经记不清有多少开发人员的验尸报告具有相同的基本结构。

对我来说，这是从两个方面做笔记的实验。首先，我通常用笔和纸做笔记，然后扫描给后代。第二，我通常不在网上发布我的笔记，但我受到了杰米·布兰登关于他读过的书的笔记的启发。我的手写笔记是一系列要点，可能无法很好地转化为 markdown。一个问题是我的 markdown 渲染器不能处理多于一层的嵌套，所以事情会被人为地弄平。可能还有更多问题。让我们来看看它们是什么！以防不明显，我的旁白是斜体的。

### 第一章:引言

本章的所有内容将在后面更详细地介绍。

雇佣人员管理系统稳定性的两种方法:

#### 传统方法:系统管理员

*   组装现有组件并部署以产生服务
*   在事件和更新发生时做出响应
*   随着服务的增长，发展团队以承担更多的工作
*   赞成的意见
    *   易于实施，因为它是标准的
    *   可供招聘的大型人才库
    *   许多可用的软件
*   骗局
    *   对变更管理和事件处理的手动干预导致团队规模随着系统负载的增加而扩大
    *   Ops 从根本上与 dev 不一致，这可能导致对变化的病态抵制，从而导致 dev 做出类似的病态反应，将“启动”重新分类为“增量更新”、“标志翻转”等。

#### 谷歌的方法:SREs

*   让软件工程师做操作
*   候选人应该能够通过或接近通过正常的开发人员招聘栏，并可能拥有一些开发人员中罕见的额外技能(例如，L1 - L3 网络或 UNIX 系统内部)。
*   职业进展与发展职业轨迹相当
*   结果
    *   手工做任务会让 SREs 感到无聊
    *   拥有自动化任务所需的技能
    *   做与运营团队相同的工作，但是用自动化代替手工劳动
*   为了避免导致团队规模随服务负载而扩大的人工陷阱，Google 为 SREs 设置了 50%的“运营”工作量上限
    *   上限。ops 的实际工作量预计会低得多
*   赞成的意见
    *   规模化成本更低
    *   绕过开发/运营分割
*   骗局
    *   很难雇到
    *   在需要管理支持的方面可能是非正统的(例如，产品团队可能会因为错误预算耗尽而反对停止该季度发布的决定)

我真的不明白这是一个如何规避开发/运营分离的例子。从某种意义上来说，我明白这是怎么回事，但是因为错误预算而停止所有发布的例子似乎与团队反对发布的“系统管理员”的例子没有本质上的不同。看起来 sre 有更多的政治资本可以花费，在给出的具体例子中，sre 可能更合理，但是没有理由认为系统管理员不合理。

#### SRE 的信条

*   SRE 团队负责延迟、性能、效率、变更管理、监控、应急响应和容量规划

#### 确保对工程的持久关注

*   50%的运营上限意味着额外的运营工作在溢出时被重新分配给产品团队
*   向产品团队提供反馈机制，并降低负载
*   目标是每 8-12 小时值班最多 2 次事件
*   所有严重事故的尸检，即使它们没有触发寻呼
*   无可指责的尸检

每班最多 2 个事件，但平均是多少？预计每周有多少随叫随到的事件会从 SRE 团队发送到开发团队？

如何从一种无可指责的事后文化转变为一种无可指责的事后文化？既然每个人都知道你应该进行无可指责的尸检，那么每个人都会声称要做。有点像拥有良好的测试和部署实践。我很幸运地一直处于待命状态，从未被传呼过，但当我与最近加入并待命的人交谈时，他们没有那么好的指责、垃圾话和推卸责任的经历。事实上，每个人都知道你应该是无可指责的，这似乎让指责变得更难，而不是更容易。

#### 在不破坏 SLO 的情况下快速移动

*   误差预算。100%对于基本上所有事情来说都是错误的可靠性目标
*   从 5 个 9 到 100%的可靠性对大多数用户来说是不明显的，并且需要巨大的努力
*   设定一个目标，承认这种权衡，并留下一个误差预算
*   错误预算可以花在任何事情上:推出特性，等等。
*   误差预算允许讨论如何分阶段推出和 1%的实验可以保持可容忍的误差水平
*   SRE 团队的目标不是“零停机”——SRE 和产品开发人员被激励去花费错误预算来获得最大的特性速度

虽然没有明确说明，但对于需要“快速行动”的团队来说，持续出现在错误预算之下可能被视为团队在可靠性上花费太多精力的标志。

我很喜欢这个想法，但当我和杰西卡·克尔讨论这个问题时，她拒绝了这个想法，因为也许你只是在你的错误预算之内，因为你很幸运，一个非常糟糕的事件就可能毁掉你未来十年的错误预算。后续问题:你如何对你的风险模型有足够的信心，可以有目的地消耗错误预算来加快行动，而不用担心下游(及时)的坏事件会使你超出预算？Nat Welch(前谷歌 SRE)对此回应说，你可以通过模拟灾难和其他测试来建立信心。

#### 监视

*   监控不应该要求人解释警报域的任何部分
*   三种有效的监控输出
    *   警报:人类需要立即采取行动
    *   门票:人类最终需要采取行动
    *   日志记录:无需任何操作
    *   请注意，例如，图形是一种日志

#### 应急响应

*   可靠性是 MTTF(平均故障时间)和 MTTR(平均恢复时间)的函数
*   为了评估回应，我们关心 MTTR
*   人类增加了潜伏期
*   由于 MTTR 较低，不需要人工响应的系统将具有较高的可用性
*   拥有“剧本”会降低 3 倍的 MTTR
    *   拥有能应对一切的英雄多面手是可行的，但拥有剧本更好

我个人同意，但是我们喜欢随叫随到的英雄。我想知道我们如何培养一种文件文化。

#### 变更管理

*   70%的停机是由于实时系统的变化造成的。缓解措施:
    *   实施渐进式推广
    *   监视
    *   反转
*   将人从循环中移除，避免重复性任务中的标准人为问题

*   直截了当，但是数量惊人的团队/服务不这样做

#### 准备金提取

*   增加容量比转移负载风险更大，因为这通常涉及到增加新的实例/位置，对现有系统(配置文件、负载平衡器等)进行重大更改。)
*   昂贵到只有在必要的时候才应该做；必须尽快完成
    *   如果你不知道你实际需要什么，过度配置会花钱

#### 效率和性能

*   负载会降低系统速度
*   SREs 供应以满足具有特定响应时间目标的容量目标
*   效率=金钱

### 第二章:从 SRE 的角度看谷歌的生产环境

这一章没有注释，因为我已经很熟悉了。TODO:也许回去更详细地阅读这一章。

### 第三章:拥抱风险

*   例如:如果用户使用 99%可靠性的智能手机，他们无法区分 99.99%和 99.999%的可靠性

#### 管理风险

*   可靠性在成本上不是线性的。获得一个额外的可靠性增量很容易花费 100 倍以上的成本
    *   冗余设备的相关成本
    *   相对于“正常”功能，为可靠性增加功能的成本
    *   目标:使系统足够可靠，但不要太可靠！

#### 衡量服务风险

*   标准实践:确定代表要优化的系统属性的度量
*   可能的指标=正常运行时间/(正常运行时间+停机时间)
    *   对于全球分布式服务是有问题的。正常运行时间到底意味着什么？
*   总可用性=成功的请求数/请求总数
    *   Obv，不是所有的请求都是相等的，但是总可用性是一个一阶近似
*   通常设定季度目标

#### 服务的风险承受能力

*   通常客观上不明显
*   sre 与产品所有者合作，将业务目标转化为明确的目标

#### 确定消费者服务的风险承受能力

*TODO:也许在第二遍*时详细阅读

#### 确定基础设施服务的风险承受能力

##### 目标可用性

*   运行 ex: Bigtable
    *   一些消费者服务直接从 Bigtable 提供数据——需要低延迟和高可靠性
    *   一些团队使用 bigtable 作为离线分析的后备存储——更关心吞吐量而不是可靠性
*   过于昂贵，无法满足所有需求
    *   例如:Bigtable 实例
    *   低延迟 Bigtable 用户需要低队列深度
    *   面向吞吐量的 Bigtable 用户需要中等至高的队列深度
    *   成功和失败在这两种情况下是截然相反的！

##### 费用

*   划分基础架构并提供不同级别的服务
*   除了 obv。好处，允许服务将提供不同服务级别的成本外部化(例如，预期面向延迟的服务比面向吞吐量的服务更昂贵)

#### 错误预算的动机

没有关于这个的注释，因为我已经相信了这一切。如果有争论的话，也许可以回头再读一遍。

### 第 4 章:服务水平目标

*注:跳过术语部分的注释。*

*   例:Chubby 计划内停机
    *   谷歌发现，Chubby 一直处于 SLO 之上，全球 Chubby 宕机会导致谷歌出现异常严重的宕机
    *   Chubby 是如此可靠，以至于团队错误地认为它永远不会停机，并且未能设计出导致 Chubby 失败的系统
    *   解决方案:当 Chubby 在一个季度内远远高于其 SLO，无法向团队“展示”Chubby 可以下降时，在全球范围内让 Chubby 下降

#### 你和你的用户关心什么？

*   指标太多:难以关注
*   指标太少:可能会忽略重要的行为
*   不同类别的服务应有不同的指标
    *   面向用户:可用性、延迟、吞吐量
    *   存储:延迟、可用性、耐用性
    *   大数据:吞吐量、端到端延迟
*   所有系统都关心正确性

#### 收集指标

*   通常可以从服务器自然完成，但有时需要客户端指标。

#### 聚合

*   使用分布而不是平均值
*   用户研究表明，人们通常喜欢较慢的平均值和较好的尾部延迟
*   基于常见定义进行标准化，例如 1 分钟内的平均值、集群中任务的平均值等。
    *   可以有例外，但是有合理的默认值会使事情变得更容易

#### 选择目标

*   不要根据当前表现选择目标
    *   目前的表现可能需要巨大的努力
*   保持简单
*   避免绝对化
    *   谈论“无限”规模或“总是”可用是不合理的
*   尽量减少 SLO 的数量
*   完美可以等待
    *   可以随时重新定义 SLO
*   SLO 设定期望
    *   保持安全余量(内部 SLO 的定义可以比外部 SLO 更宽松)
*   不要超额完成
    *   见上面 Chubby 的例子
    *   另一个例子是确保系统在轻负载下不会太快

### 第五章:消除辛劳

卡拉·盖瑟:“如果一个人类操作员在正常操作中需要接触你的系统，你就有一个 bug。随着系统的增长，正常变化的定义。”

*   定义:辛劳
    *   不仅仅是“我不想做的工作”
    *   指南
    *   重复的
    *   自动化
    *   战术的
    *   没有持久的价值
    *   O(n)随着服务增长
*   在调查中，发现平均 33%的辛劳
    *   数字可以低至 0%,高至 80%
    *   Toil > 50%表明经理应该更平均地分配 Toil 负载
*   辛劳总是坏事吗？
    *   可预测和重复的任务可以让人平静
    *   可以产生成就感，可以进行低风险/低压力的活动

*关于为什么辛劳不好的章节。跳过该部分的笔记。*

### 第 6 章:监控分布式系统

*   为什么要监控？
    *   分析长期趋势
    *   随着时间的推移进行比较或做实验
    *   发信号
    *   构建仪表板
    *   排除故障

正如亚历克斯·克莱默常说的，我们的问题不是我们行动太慢，而是我们建造了错误的东西。我想知道我们如何才能从今天的状况发展到拥有足够的仪器，以便在构建新系统时能够做出明智的决策。

#### 设定合理的期望值

*   监控并非无足轻重
*   10-12 人 SRE 团队通常有 1-2 人负责构建和维护监控
*   随着时间的推移，由于工具/库/集中监控基础设施的改进，数量有所减少
*   更简单/更快速的监测系统的总体趋势，以及更好的事后分析工具
*   避免“神奇”的系统
*   复杂依赖关系层次结构的成功有限(例如，“如果数据库运行缓慢，则向数据库发出警报，否则向网站发出警报”)。
    *   常用(仅？)对于系统非常稳定的部分
*   为人类生成警报的规则应该易于理解，并代表明显的失败

*避魔包括避 ML？*

*   大量的白盒监控
*   一些针对关键内容的黑盒监控
*   四个黄金信号
    *   潜伏
    *   交通
    *   错误
    *   浸透

来自 Bigtable 和 Gmail 的有趣例子来自未转录的章节。许多关于保持警报简单的重要性的信息也没有被转录。

#### 长期来看

*   长期可用性和短期可用性之间经常存在矛盾
*   有时可以通过英勇的努力来修复不可靠的系统，但这是一种精疲力竭的风险，也是一种失败的风险
*   对短期可靠性进行有控制的打击通常是更好的交易

### 第 7 章:谷歌自动化的发展

*   “自动化是力量倍增器，而不是万灵药”
*   自动化的价值
    *   一致性
    *   展开性
    *   MTTR
    *   更快的非维修动作
    *   节约时间

笔记中跳过了多个有趣的案例研究和解释。

### 第 8 章:发布工程

*   这是谷歌的一项特定工作职能

#### 发布工程师角色

*   发布工程师与 SWE 和 sre 一起定义软件是如何发布的
    *   允许开发团队专注于开发工作
*   定义最佳实践
    *   编译器标志、构建 ID 标签的格式等。
*   自动发布
*   不同团队的模型有所不同
    *   可以“推进绿色”并部署每个构建
    *   可能是每小时构建和部署一次
    *   等等。
*   密封建筑
    *   构建相同的版本号应该总是给出相同的结果
    *   自包含——这包括对编译器使用的所有内容进行版本控制
    *   可以根据旧版本挑选修复程序来修复生产软件吗
*   几乎所有的变更都需要代码审查
*   分支
    *   主分支中的所有代码
    *   释放是分支的
    *   修复可以从主服务器到分支服务器
    *   分支从未合并回来
*   测试
    *   海峡群岛
    *   发布过程创建一个运行测试并显示测试通过的审计跟踪
*   配置管理
*   许多可能的方案(都涉及到在源代码控制中存储配置并进行严格的配置审查)
*   使用配置主线-配置在头部维护并立即应用
    *   最初用于博格(以及博格之前的系统)
    *   二进制版本和配置变化解耦！
*   在同一个包中包含配置文件和二进制文件
    *   简单的
    *   对于配置文件很少或者配置很少改变的项目，紧密耦合二进制和 config - ok
*   将配置打包成“配置包”
    *   与代码的密封原理相同
*   发布工程不应该是事后的想法！
    *   开发周期开始时的预算资源

### 第九章:简单

*   稳定性与敏捷性
    *   通过冷冻可以使事情稳定——需要平衡两者
    *   可靠的系统可以提高敏捷性
    *   可靠的部署使得将变更与错误联系起来变得更加容易
*   无聊之德！
*   [本质复杂性与意外复杂性](https://en.wikipedia.org/wiki/No_Silver_Bullet)
    *   当引入意外的复杂性时，SREs 应该向后推
*   代码是一种负担
    *   删除死代码或其他膨胀
*   最小 API
    *   更小的 API 更容易测试，更可靠
*   模块性
    *   API 版本控制
    *   与代码相同，在代码中应该避免 misc/util 类
*   放
    *   较小的释放更容易测量
    *   如果我们一起发布 100 个变更，会发生什么

### 第 10 章:改变时序数据

#### 博格蒙

*   类似于[普罗米修斯](https://prometheus.io/)
*   日志记录的通用数据格式
*   用于仪表板和警报的数据
*   形式化了遗留数据格式“varz ”,它允许通过 HTTP 查看指标
*   添加指标只需要代码中的一个声明
    *   添加新指标的低用户成本
*   Borgmon 定期从每个目标获取/varz
    *   还包括综合数据，如健康检查、名称是否已解析等。,
*   时间序列竞技场
    *   数据存储在内存中，带有指向磁盘的检查点
    *   固定大小分配
    *   垃圾收集满时最早的条目过期
    *   概念上是一个二维数组，一个轴上是时间，另一个轴上是项目
    *   一个数据点 24 字节-> 12 小时内 1 分钟间隔的 1M 唯一时间序列= 17 GB
*   博格曼规则
    *   代数表达式
    *   从其他时间序列计算时间序列
    *   在线程池中并行计算的规则
*   计数器与仪表
    *   定义:计数器是非递减的
    *   Def:可以取任何值
    *   计数器优于仪表，因为仪表会因采样间隔而丢失信息
*   变更
    *   博格曼规则可以触发警报
    *   具有最小持续时间以防止“拍打”
    *   通常设置为两个持续周期，以便错过的收集不会触发警报
*   缩放比例
    *   Borgmon 可以从其他 Borgmon 获取时间序列数据(使用二进制流协议，而不是基于文本的 varz 协议)
    *   可以有多层过滤器
*   探查者
    *   监视用户所见内容的黑盒监视
    *   可以用 varz 查询或者直接向 Altertmanager 发送警报
*   配置
    *   规则定义和被监控目标之间的分离

### 第十一章:随叫随到

*   典型响应时间
    *   5 分钟用于面向用户或其他时间紧迫的任务
    *   30 分钟用于对时间不太敏感的内容
*   与 SLO 相关的响应时间
    *   例如:一个季度的 99.99%是 13 分钟的停机时间；显然不能有超过 13 分钟的响应时间
    *   SLO 较宽松的服务的响应时间可能在几十分钟(或更长？)
*   主要与次要随叫随到
    *   工作分配因团队而异
    *   在某些情况下，辅助服务器可以作为主服务器的备份
    *   在其他情况下，辅助处理非紧急/非寻呼事件，主处理寻呼
*   平衡待命
    *   定义:数量:待命时间的百分比
    *   定义:质量:呼叫过程中发生的事故数量

这太棒了。我们应该这样做。人们有时会连续几次遇到非常糟糕的随叫随到，考虑到随叫随到的频率很低，没有理由期望这种情况会在一两年内随机平衡。

*   数量平衡
    *   > = SRE 50%的时间花在工程上
    *   其余部分，不超过 25%用于待命
*   偏好多站点团队
    *   夜班对健康有害，多站点团队允许取消夜班
*   质量平衡
    *   平均而言，处理一个事件(包括根本原因分析、补救、事后分析、修复 bug 等)。)需要 6 个小时。
    *   = >在 12 小时的值班中，不应发生超过 2 起事故
    *   为了保持在上限内，希望页面分布非常平坦，中值为 0
*   补偿-随叫随到的额外报酬(休假或现金)

### 第 12 章:有效的故障排除

*本章没有注释。*

### 第十三章:紧急应变

*   测试引发的紧急情况
*   例如:想要清除分布式 MySQL 数据库上隐藏的依赖关系
    *   计划:阻止访问 1/100 的数据库
    *   响应:依赖服务报告他们无法访问关键系统
    *   SRE 回应:SRE 中止演习，试图撤销许可变更
    *   回滚尝试失败
    *   尝试恢复对副本的访问有效
    *   1 小时后恢复正常运行
    *   进展顺利:依赖团队立即上报问题，能够恢复访问
    *   我们所学到的:对系统及其与其他系统的相互作用理解不足，未能遵循事件响应，这将通知客户停机，没有在测试环境中测试回滚程序
*   变化引发的紧急情况
    *   更改会导致失败！
*   周五推送的滥用预防基础架构的配置更改触发了崩溃循环错误
    *   几乎所有面向外部的系统都依赖于此，变得不可用
    *   许多内部系统也有依赖性，变得不可用
    *   警报在几秒钟内开始触发
    *   在配置推送的 5 分钟内，推送更改的工程师回滚了更改，服务开始恢复
    *   什么进展顺利:监控立即启动，事件管理运行良好，带外通信系统让人们在许多系统停机的情况下保持最新状态，运气(推动变革的工程师跟踪实时通信渠道，这不是发布过程的一部分)
    *   我们所学到的:push to canary 没有触发同样的问题，因为它没有命中特定的配置关键字组合；推送被认为是低风险的，并通过了不太严格的金丝雀流程，在停机期间警报太吵
*   过程引发的紧急情况

*没有关于过程诱发示例的注释。*

### 第 14 章:管理事故

这是一个我们看起来非常擅长的领域。本章没有注释。

### 第 15 章:事后文化:从失败中学习

*[我非常同意这一章](//danluu.com/postmortem-lessons/)的大部分内容。没有笔记。*

### 第 16 章:跟踪停机

*   自动扶梯:集中系统，跟踪对警报的确认，必要时通知其他人，等等。
*   Outalator:提供多个队列的通知的时间交错视图
    *   还保存相关的电子邮件，允许将一些邮件标记为“重要”，可以折叠不重要的邮件等。

我们的自动扶梯看起来不错。不过，我们真的可以使用类似 Outalator 的东西。

### 第 17 章:可靠性测试

向唱诗班布道。此部分没有注释。不过，我们真的可以做得更好。

### 第十八章:SRE 的软件工程

*   例如:Auxon，容量规划自动化工具
*   背景:传统的容量规划周期
    *   1)收集需求预测(提前几个季度到几年)
    *   2)计划分配
    *   3)评审计划
    *   4)部署和配置资源
*   传统方法的缺点
    *   许多事情会影响计划:效率的提高，采用率的提高，集群交付日期的延误，等等。
    *   即使很小的变化也需要重新检查分配计划
    *   较大的变化可能需要完全重写计划
    *   劳动强度大且容易出错
*   谷歌解决方案:基于意图的产能规划
    *   指定需求，而不是实现
    *   编码需求并自动生成容量规划
    *   除了节省劳动力，求解器可以比人工生成的解决方案做得更好= >节省成本
*   越来越多基于意图的计划的例子
    *   1)想要在集群 X、Y 和 Z 中部署 50 个内核，为什么要在这些集群中部署这些资源？
    *   2)希望在该地区的任意 3 个集群中拥有 50 个内核—为什么要那么多资源，为什么要 3 个？
    *   3)希望通过 N+2 冗余来满足需求-为什么是 N+2？
    *   4)想要 5 个 9 的可靠性。可以发现，例如，N+2 是不够的
*   发现最大的收获来自于去(3)
    *   一些复杂的服务可能会支持(4)
*   将约束放入工具中可以使权衡在整个车队中保持一致
    *   而不是作出个别的临时决定
*   生长素输入
    *   要求(例如，“每个洲的服务必须是 N+2”，“前端服务器与后端服务器的距离不超过 50 毫秒”)
    *   属国
    *   预算优先事项
    *   性能数据(服务如何扩展)
    *   需求预测数据(注意，像 Colossus 这样的服务是从依赖服务中获得预测的)
    *   资源供应和定价
*   输入进入求解器(混合整数或线性规划求解器)

没有关于为什么 SRE 软件，如何组建一个团队等的说明。TODO:重读这一章的一半，如果与我直接相关，做笔记。

### 第 19 章:前端负载平衡

*本节无注释。看起来与我们的高层次目标非常相似，这一章没有涉及低层次的细节。值得注意的是，他们做的[编辑]与我们不同。对于更低层次细节的更多信息，有[磁悬浮论文](https://www.usenix.org/system/files/conference/nsdi16/nsdi16-paper-eisenbud.pdf)。*

### 第 20 章:数据中心的负载平衡

*   流控制
*   需要避免不健康的任务
*   不健康任务的简单流程控制
    *   跟踪对后端的请求数量
    *   当达到阈值时，将后端视为不健康
    *   缺点:总体来说很糟糕
*   基于健康的流量控制
    *   后端任务可能处于以下三种状态之一:{健康、拒绝连接、跛脚鸭}
    *   跛脚鸭状态仍然可以接受连接，但会向所有客户端发送背压请求
    *   跛脚鸭状态简化了干净关机
*   Def: subsetting:限制客户端任务可以与之交互的后端任务池
    *   RPC 系统中的客户端维护到后端的连接池
    *   与在需要时设置/拆除相比，使用池可以减少延迟
    *   非活动连接相对便宜，但不是免费的，即使在“非活动”模式下(减少健康检查，UDP 而不是 TCP，等等。)
*   选择正确的子集
    *   典型:20-100，根据工作量选择
*   子集选择:随机
*   子集选择:循环
    *   顺序被置换；每一轮都有自己的排列
*   负载平衡
    *   子集选择是为了连接平衡，但是我们仍然需要平衡负载
*   负载平衡:循环调度
    *   在实践中，观察最大负载和最小负载之间的 2 倍差异
    *   实际上，最昂贵的请求可能比最便宜的请求贵 1000 倍
    *   此外，请求中存在随机的不可预测的变化
*   负载平衡:负载最小的循环调度
    *   正如它听起来的那样:负载最小的后端之间的循环
    *   负载似乎是根据连接数来衡量的；可能并不总是最佳指标
    *   这是针对每个客户端的，而不是全局的，因此有可能向后端发送来自其他客户端的请求
    *   实际上，对于大型服务，负载最重的任务使用的 CPU 是负载最小的任务的两倍；类似于普通循环赛
*   负载平衡:加权循环法
    *   同上，但权重考虑了其他因素
    *   实际上，比负载最少的循环调度更好的负载分配

我想知道 Heroku 对说唱天才的回应是什么意思，他们说“经过大量的研究和实验，我们还没有找到一个理论模型或实际的实现，可以击败随机路由到支持多个并发连接的 web 后端的简单性和健壮性”。

### 第二十一章:处理霸王

*   即使有“良好的”负载平衡，系统也会过载
*   典型的策略是提供降级的响应，但是在非常高的负载下，这可能是不可能的
*   将容量建模为 QPS 或请求的函数(例如，请求读取多少个键)容易失败
    *   这些通常变化很慢，但也可能变化很快(例如，由于一次签入)
*   更好的解决方案:直接测量可用资源
*   CPU 利用率通常是一个很好的供应信号
    *   使用 GC，内存压力转化为 CPU 利用率
    *   对于其他系统，可以调配其他资源，因此 CPU 可能会成为限制因素
    *   如果过度配置 CPU 成本过高，请考虑其他资源

*一般那样过度配置 CPU 要花多少钱？*

*   客户端节流
    *   当客户达到限额时，后端开始拒绝请求
    *   请求仍然使用资源，即使被拒绝——如果没有节流，后端会将大部分资源用于拒绝请求
*   危险程度
    *   似乎是优先事项，但名称不同？
    *   RPC 系统中的一级概念
    *   客户端调节为每个重要级别保留单独的统计数据
    *   默认情况下，临界状态通过后续的 RPC 传播
*   处理过载错误
    *   如果 DC 过载，将负载转移到其他 DC
    *   如果 DC 正常，但一些后端过载，则将负载转移到其他后端
*   客户端在收到过载响应时会重试
    *   每请求重试预算(3)
    *   每客户端重试预算(10%)
    *   客户端重试失败导致“过载；“不重试”响应将被返回到上游

得到“不要重试”的回答是“显而易见的”，但在实践中相对较少。许多真实的系统都有一个问题，失败的重试会导致更多的重试。当跨越硬件/软件边界时尤其如此(例如，文件系统读取导致在 DVD/SSD/旋转磁盘上的许多重试，失败，然后在文件系统级别重试)，但在纯软件中似乎也是如此。

### 第 22 章:解决级联故障

*   典型的故障场景？
*   服务器霸主
*   例如:有两台服务器
    *   一个过载，失败
    *   另一个现在获得所有流量，但也失败了
*   资源枯竭
    *   CPU/内存/线程/文件描述符/等等。
*   例如:资源之间的依赖关系
    *   1) Java 前端的 GC 参数调得不好
    *   2)由于 GC，前端耗尽了 CPU
    *   3) CPU 耗尽会降低请求速度
    *   4)队列深度增加会使用更多 RAM
    *   5)整个前端的固定内存分配意味着可用于缓存的内存减少
    *   6)较低的命中率
    *   7)更多请求进入后端
    *   8)后端耗尽 CPU 或线程
    *   9)健康检查失败，开始级联故障
    *   大修期间难以确定原因
*   注意:避免服务错误的服务器的策略会使事情变得更糟
    *   可用的后端越来越少，导致请求太多，然后变得不可用
*   防止服务器霸王
    *   负载测试！必须有现实的环境
    *   提供降级的结果
    *   过载时，过早廉价地发生故障
    *   让更高级别的系统拒绝请求(在反向代理、负载平衡器和任务级别)
    *   执行容量规划
*   队列管理
    *   队列在稳定状态下什么也不做
    *   排队的请求会消耗内存并增加延迟
    *   如果流量比较稳定，最好保持较小的队列大小(比如说，线程池大小的 50%或更少)
    *   例如:当线程已满时，Gmail 使用无队列服务器进行故障转移
    *   对于突发工作负载，队列大小应该是线程数量、每次请求时间、突发大小/频率的函数
    *   参见，[自适应后进先出法和代码法](http://queue.acm.org/detail.cfm?id=2839461)
*   优雅的退化
    *   请注意，测试适度降级路径很重要，可以通过定期运行一小组接近过载的服务器，因为这种路径在正常情况下很少使用
    *   最好保持简单易懂
*   重试次数
    *   总是使用随机指数补偿
    *   请参阅上一章仅在单个级别重试
    *   考虑有一个服务器范围的重试预算
*   最后期限
    *   不要做已经错过最后期限的工作(级联失败的常见主题)
    *   在每一个阶段，检查最后期限是否已经到来
    *   应宣传截止日期(例如，甚至通过 RPC)
*   双峰潜伏期
    *   例:期限过长的问题
    *   假设前端有 10 台服务器，每台有 100 个线程(总线程数为 1k)
    *   正常操作:1k QPS，请求花费 100 毫秒=> 100 个工作线程被占用(1k QPS * .1s)
    *   假设 5%的操作没有完成，并且有一个 100 秒的期限
    *   消耗 5k 个线程(50 个 QPS * 100 个线程)
    *   前端超额认购 5 倍。成功率= 1k / (5k + 95) = 19.6% => 80.4%错误率

使用截止日期而不是超时是很棒的。我们真的应该对此更系统化。

通过设置合理的截止时间，不允许系统被无意义的僵尸请求填满是“显而易见的”,但是很多真实的系统似乎在合理的整数(30 秒、60 秒、100 秒等)上有任意的超时。)而不是考虑到负载/级联故障而指定的截止日期。

*   尽量避免层内通信
    *   更简单，避免了可能的级联故障路径
*   测试级联故障
    *   加载测试组件！
    *   负载测试既揭示了断裂，也指出了在负载下会完全倒下的组件
    *   确保单独测试每个组件
    *   测试非关键后端(例如，确保搜索的拼写建议不会妨碍关键路径)
*   立即采取措施解决级联故障
    *   增加资源
    *   暂时停止健康检查失败/死亡
    *   重启服务器(只有在有帮助的情况下——例如，在 GC 死亡螺旋或死锁的情况下)
    *   降低流量-激烈的，最后的手段
    *   进入降级模式——要求之前已将其内置到服务中
    *   消除批量装载
    *   消除不良交通

### 第 23 章:可靠性的分布式共识

*   我们如何就像…这样的问题达成一致
    *   哪一个过程是一组过程的领导者？
    *   组中的进程集是什么？
    *   消息是否已成功提交到分布式队列？
    *   一个进程持有一个特定的租约吗？
    *   特定键在数据存储中的值是多少？
*   例 1:裂脑
    *   服务在不同的机架中复制了文件服务器
    *   必须避免同时写入一组中的两个文件服务器，以避免数据损坏
    *   每对文件服务器都有一个领导者和一个跟随者
    *   服务器通过心跳相互监控
    *   如果一个服务器无法联系到另一个，它会发送一个 STONITH(击中另一个节点的头部)
    *   但是如果网络很慢或者数据包丢失会发生什么呢？
    *   如果两台服务器都发出 STONITH，会发生什么情况？

这让我想起了我最喜欢的分布式数据库事后分析之一。数据库被配置成一个环，其中每个节点与 5 个服务器组成的“邻居”通信并将数据复制到其中。如果邻居中的一些机器出现故障，其他服务器会加入邻居，数据会得到适当的复制。

听起来不错，但是如果一个服务器坏了，并且确定没有数据存在，并且它的所有邻居都坏了，它可以比它的任何邻居更快地返回结果，并且告诉它的邻居它们都坏了。因为坏的服务器没有数据，所以它非常快，并且可以比它的邻居更快地报告它是坏的。哎呦！

*   例 2:故障转移需要人工干预
    *   一个高度分片的数据库对于每个分片都有一个主数据库，这个主数据库复制到另一个 DC 中的一个辅助数据库
    *   外部运行状况检查决定主节点是否应该故障转移到辅助节点
    *   如果主节点看不到辅助节点，它会使自己不可用，以避免“Ex1”带来的问题
    *   这增加了操作负荷
    *   问题是相互关联的，当人们忙于其他事情时，这相对容易遇到问题
    *   如果存在网络问题，没有理由认为人类会比系统中的机器更好地了解世界的状态
*   例 3:错误的组成员算法
    *   *听起来像什么。此部分无注释*
*   不可能的结果
    *   [CAP](https://en.wikipedia.org/wiki/CAP_theorem) : P 在现实网络中是不可能的，所以选择 C 或者 A
    *   [FLP](http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/) :异步分布式共识无法保证不可靠网络的进步

*   提议的顺序，这些提议可能被大多数进程接受，也可能不被接受
    *   未接受= >失败
    *   每个建议的序号在整个系统中必须是唯一的
*   建议
    *   建议者向接受者发送序列号
    *   如果没有看到更高的序列号，Acceptor 会同意
    *   建议者可以用更高的序列号再试一次
    *   如果提议者接受多数人同意，它通过发送带有值提交消息来提交
    *   接受者在接受时必须记录到持久存储中

#### 模式

*   分布式共识算法是一种低级的原语
*   可靠的复制状态机
*   可靠的替代数据和配置存储
    *   基于非分布式共识的系统通常使用时间戳:这是有问题的，因为无法保证时钟同步
    *   参见[扳手纸](http://research.google.com/archive/spanner.html)中使用分布式共识的示例
*   领导人选举
    *   相当于分布式共识
    *   在领导者的工作可以由一个进程执行或分割的情况下，领导者选举模式允许编写分布式系统，就像编写一个简单的程序一样
    *   例如，由 GFS 和 Colussus 使用
*   分布式协调和锁定服务
    *   例如，在 MapReduce 中使用的屏障，用于确保在 Reduce 继续之前完成映射
*   分布式队列和消息传递
    *   队列:可以容忍来自工作节点的故障，但是系统需要确保所声明的任务得到处理
    *   可以使用租约而不是从队列中删除
    *   使用 RSM 意味着即使队列关闭，系统也可以继续处理
*   表演
    *   共识算法不能用于高吞吐量低延迟系统的传统观点是错误的
    *   许多谷歌系统的核心是分布式共识
    *   与大多数其他公司相比，规模让谷歌的情况变得更糟，但它仍然有效
*   多派克斯
    *   强领导流程:除非尚未选出领导或失败发生，否则只需一次往返就能达成共识
    *   请注意，组中的另一个流程可以随时提出建议
    *   能来回乒乓和伪活锁吗
    *   对多人来说并不奇怪，
    *   标准解决方案是选举提议人流程或使用轮流提议人
*   扩展读取密集型工作负载
    *   例如:光子允许从任何副本读取
    *   从过时的副本中读取需要额外的工作，但不会产生不正确的结果
    *   要确保读取是最新的，请执行以下操作之一:
    *   1)执行只读一致性操作
    *   2)从副本中读取保证是最新的数据(稳定的领导者可以提供这种保证)
    *   3)使用法定租约
*   [法定租约](http://www.pdl.cmu.edu/PDL-FTP/associated/CMU-PDL-14-105_abs.shtml)
    *   副本可以被授予系统中部分(或全部)数据的租用权
*   Fast Paxos
    *   旨在比广域网更快
    *   每个客户可以直接发送`Propose`给一组接收者中的每个成员，而不是通过领导者
    *   [不一定比传统的 Paxos](http://www.sysnet.ucsd.edu/sysnet/miscpapers/hotdep07.pdf) 更快-如果 RTT 到接受者的距离很长，我们通过慢速链接交换一条消息，并通过快速链接并行交换 N 条消息，通过慢速链接交换 N 条消息
*   稳定的领导人
    *   “几乎所有在设计时就考虑到性能的分布式共识系统都使用单一稳定领导模式或轮流领导系统”

TODO:完成这一章？

### 第 24 章:分布式 cron

TODO:回去更详细地阅读，做笔记。

### 第 25 章:数据处理管道

*   这方面的例子有 MapReduce 或 Flume
*   方便和容易推理快乐的情况，但脆弱
    *   初始安装通常是可以的，因为工人规模、分块、参数都经过仔细调整
    *   随着时间的推移，负载会发生变化，从而导致问题

### 第 26 章:数据完整性

*   定义不一定明显
    *   如果一个界面错误导致 Gmail 无法显示邮件，从用户的角度来看，这与数据丢失是一样的
    *   99.99%的正常运行时间意味着每年 1 小时的停机时间。对大多数应用程序来说可能没问题
    *   2GB 文件中 99.99%的好字节意味着 200K 损坏。对大多数应用程序来说可能不太好
*   备份并不简单
    *   可能混合了事务性和非事务性备份和恢复
    *   不同版本的业务逻辑可能同时存在
    *   如果服务是独立版本的，可能会有许多版本的组合
    *   副本是不够的-副本可能会同步损坏
*   对谷歌 19 项数据恢复工作的研究
    *   最常见的用户可见的数据丢失是由软件错误导致的删除或参照完整性丢失引起的
    *   最棘手的案件是几周或几个月后发现的低级腐败

#### 纵深防御

*   第一层:软删除
    *   用户应该能够删除他们的数据
    *   但这意味着用户可能会意外删除他们的数据
    *   还有，账号劫持等。
    *   错误也可能导致意外删除
    *   软删除将实际删除延迟一段时间
*   第二层:备份
    *   需要确定在恢复过程中可以丢失多少数据，恢复需要多长时间，以及需要进行多长时间的备份
    *   希望备份永远恢复，因为损坏可能会在数月(或更长时间)内不为人知
    *   但是，对代码和模式的更改会使恢复旧备份变得非常昂贵
    *   谷歌通常有 30 到 90 天的窗口，这取决于服务
*   第三层:早期检测
    *   带外完整性检查
    *   很难做好这件事！
    *   正确的更改会导致检查器失败
    *   但是放松检查会导致失败被遗漏

没有关于这两个有趣案例研究的注释。

### 第 27 章:大规模可靠的产品发布

*本章没有特别的注释。这些材料中有很多被其他章节的材料所涵盖，或者至少被暗示。不过，在考虑发布策略之前，至少应该看看示例清单项目和行动项目。另请参见附录 E，发布协调清单。*

### 第 28-32 章:关于管理的各个章节

*没有关于这些的注释。*

### 笔记上的笔记

我非常喜欢这本书。如果你关心构建可靠的系统，通读这本书，看看你周围的团队没有做什么似乎是一个很好的练习。也就是说，这本书并不完美。对我来说，两个大的缺点源于同一个问题:这是那种由不同的人收集章节的书。有些编辑比其他人更好，这意味着有些章节比其他章节更清晰，因为这些章节似乎被设计成独立的章节，所以如果你只是直接通读，书中有相当多的冗余。取决于你打算如何使用这本书，这可能是积极的，但对我来说是消极的。但即使包括他的缺点，我也要说这是我在过去一年中读过的最有价值的技术书籍，我已经涵盖了这套笔记中大约 20%的内容。如果你真的喜欢这些笔记，你可能会想读一读《T2》的全部内容。

如果你觉得这套笔记太枯燥，也许可以试试[这本完全不同的书](http://nostalgebraist.tumblr.com/post/142489665564/brazenautomaton-nostalgebraist-the-book-bad)上更有趣的笔记。如果你觉得这只是有点太枯燥，也许可以试试[这套关于尸检中常见错误的笔记](http://danluu.com/postmortem-lessons/)。在任何情况下，[我都希望得到关于这些笔记的反馈](https://twitter.com/danluu)。写笔记对我来说是一种尝试。如果人们觉得这些有用，我会试着在我经常阅读的书上写笔记。如果没有，我可能会尝试不同的方法来写笔记或其他类型的帖子。

</main>