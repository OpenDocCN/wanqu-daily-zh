# 日志:关于实时数据的统一抽象，每个软件工程师都应该知道什么

> 原文:[https://engineering . LinkedIn . com/distributed-systems/log-what-every-a-software-engineer-should-know-on-real-time-datas-unified？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

大约六年前，我在一个特别有趣的时候加入了 LinkedIn。我们刚刚开始碰到我们的单片、集中式数据库的限制，并且需要开始过渡到专门的分布式系统的组合。这是一次有趣的经历:我们构建、部署并运行了一个分布式图形数据库、一个分布式搜索后端、一个 Hadoop 安装，以及第一代和第二代键值存储。

在这一切中，我学到的最有用的一件事是，我们建造的许多东西的核心都有一个非常简单的概念:日志。有时称为预写日志或提交日志或事务日志，日志几乎与计算机一样长，是许多分布式数据系统和实时应用程序架构的核心。

不了解日志就无法完全理解数据库、NoSQL 存储、键值存储、复制、paxos、hadoop、版本控制，或者几乎任何软件系统；然而，大多数软件工程师并不熟悉它们。我想改变这一点。在这篇文章中，我将带您了解关于日志的一切，包括什么是日志以及如何使用日志进行数据集成、实时处理和系统构建。

## 第一部分:什么是日志？

日志可能是最简单的存储抽象。它是一个只附加的、完全有序的记录序列，按时间排序。看起来是这样的:

记录被附加到日志的末尾，读取从左到右进行。每个条目都分配有一个唯一的顺序日志条目编号。

记录的排序定义了“时间”的概念，因为左边的条目被定义为比右边的条目更老。日志条目编号可以被认为是条目的“时间戳”。将这种排序描述为时间的概念起初似乎有点奇怪，但它有一个方便的特性，即它与任何特定的物理时钟无关。随着我们进入分布式系统，这个特性将变得至关重要。

记录的内容和格式对于本次讨论并不重要。此外，我们不能一直向日志中添加记录，因为我们最终会耗尽空间。我过一会儿会回到这个话题。

因此，日志与文件或表并没有什么不同。文件是一个字节数组，表是一个记录数组，而日志实际上只是一种按时间对记录进行排序的表或文件。

在这一点上，你可能想知道为什么值得谈论这么简单的事情？只追加的记录序列与数据系统有什么关系？答案是日志有一个特定的目的:它们记录发生了什么，什么时候发生的。对于分布式数据系统来说，这在很多方面都是问题的核心。

但在我们走得太远之前，让我澄清一些有点令人困惑的事情。每个程序员都熟悉日志记录的另一个定义——应用程序可能使用 syslog 或 log4j 将非结构化的错误消息或跟踪信息写到本地文件中。为了清楚起见，我将称之为“应用程序日志”。应用程序日志是我所描述的日志概念的退化形式。最大的区别在于，文本日志主要是供人阅读的，而我所描述的“日志”或“数据日志”是为编程访问而构建的。

(实际上，如果你仔细想想，人类阅读单个机器上的日志的想法有点不合时宜。当涉及许多服务和服务器时，这种方法很快成为一种难以管理的策略，日志的目的很快变成查询和图形的输入，以了解许多机器上的行为——对于这种情况，文件中的英文文本远不如这里描述的那种结构化日志合适。)

### 数据库中的日志

我不知道圆木的概念是从哪里来的——可能是像二分搜索法那样的东西太简单了，以至于发明者意识不到这是一项发明。早在 IBM 的 [System R](http://www.cs.berkeley.edu/~brewer/cs262/SystemR.pdf) 就有了。数据库中的使用与在崩溃时保持各种数据结构和索引的同步有关。为了实现原子性和持久性，数据库在将修改应用到它维护的各种数据结构之前，使用日志写出关于它们将要修改的记录的信息。日志是所发生事情的记录，每个表或索引都是这个历史到一些有用的数据结构或索引的投影。因为日志是立即持久化的，所以它被用作在崩溃事件中恢复所有其他持久化结构的权威来源。

随着时间的推移，日志的用途从 ACID 的一个实现细节发展成为一种在数据库之间复制数据的方法。事实证明，数据库上发生的更改序列正是保持远程副本数据库同步所需要的。Oracle、MySQL 和 PostgreSQL 包含日志传送协议，将部分日志传送到充当从属数据库的副本数据库。oracle 已经将日志产品化，作为非 Oracle 数据订阅者的通用数据订阅机制，他们的[xstream](http://docs.oracle.com/cd/E11882_01/server.112/e16545/xstrm_intro.htm)和 [GoldenGate](http://www.oracle.com/technetwork/middleware/goldengate/overview/index.html) 以及 MySQL 和 PostgreSQL 中的类似工具是许多数据架构的关键组件。

由于这个原因，机器可读日志的概念在很大程度上被限制在数据库内部。使用日志作为数据订阅的机制似乎是偶然出现的。但是这种抽象非常适合支持各种消息传递、数据流和实时数据处理。

### 分布式系统中的日志

日志解决的两个问题——排序更改和分发数据——在分布式数据系统中更为重要。这些系统的核心设计问题包括同意订购更新(或同意不同意并处理副作用)。

分布式系统的以日志为中心的方法源于一个简单的观察，我称之为状态机复制原则:

如果两个相同的确定性过程开始于相同的状态，并以相同的顺序获得相同的输入，它们将产生相同的输出，并以相同的状态结束。

这可能看起来有点迟钝，所以让我们深入了解它的含义。

[确定性](http://en.wikipedia.org/wiki/Deterministic_algorithm)意味着处理与时间无关，不会让任何其他“带外”输入影响其结果。例如，一个程序的输出受特定的线程执行顺序或对`gettimeofday`的调用或其他一些不可重复的事情的影响，通常最好被认为是非确定性的。

流程的*状态*是在处理结束时，机器上保留的任何数据，无论是内存还是磁盘。

关于以相同的顺序获得相同的输入应该有所启发——这就是日志的用武之地。这是一个非常直观的概念:如果您向两个确定性代码片段提供相同的输入日志，它们将产生相同的输出。

分布式计算的应用是非常明显的。您可以将让多台机器都做相同事情的问题简化为实现一个分布式一致日志来提供这些流程输入的问题。这里的日志的目的是从输入流中挤出所有的不确定性，以确保处理该输入的每个副本保持同步。

当你理解它的时候，这个原理并没有什么复杂或深刻的:它或多或少等于说“确定性处理是确定性的”。尽管如此，我认为它是分布式系统设计的更通用的工具之一。

这种方法的优点之一是索引日志的时间戳现在充当副本状态的时钟——您可以用一个数字来描述每个副本，即它处理的最大日志条目的时间戳。该时间戳与日志相结合，唯一地捕获了副本的整个状态。

根据日志中的内容，有多种方法可以将这一原则应用到系统中。例如，我们可以记录对服务的传入请求，或者服务在响应请求时经历的状态变化，或者它执行的转换命令。理论上，我们甚至可以记录每个副本要执行的一系列机器指令，或者每个副本上要调用的方法名和参数。只要两个进程以相同的方式处理这些输入，这些进程将在副本之间保持一致。

不同的人群似乎对日志的用途有不同的描述。数据库人员通常区分*物理*和*逻辑*日志记录。物理日志记录意味着记录每一行被更改的内容。逻辑日志记录意味着不记录更改的行，而是记录导致行更改的 SQL 命令(insert、update 和 delete 语句)。

分布式系统文献通常区分两种主要的处理和复制方法。“状态机模型”通常指的是主动-主动模型，在这种模型中，我们记录传入的请求，每个副本处理每个请求。对此稍作修改，称为“主备份模型”，即选举一个副本作为领导者，并允许该领导者按照请求到达的顺序处理请求，并记录处理请求时对其状态的更改。其他副本按领导者状态变化的顺序应用，以便它们保持同步，并准备好在领导者出现故障时接管领导者的工作。

![](../Images/0e2afeb2ac538a6688ddd0e6c3dc016e.png)

为了理解这两种方法的区别，让我们看一个玩具问题。考虑一个复制的“算术服务”,它维护一个数字作为其状态(初始化为零),并对该值应用加法和乘法。主动-主动方法可能会记录要应用的转换，比如“+1”、“2”等。每个复制品将应用这些变换，并因此经历相同的一组值。“主动-被动”方法会让单个主机执行转换，并记录下结果，比如“1”、“3”、“6”等。这个例子也清楚地说明了为什么排序是确保副本之间一致性的关键:对加法和乘法重新排序将产生不同的结果。

分布式日志可以被视为对一致性问题建模的数据结构。毕竟，日志代表了对“下一个”要添加的值的一系列决定。你必须眯着眼睛才能看到 Paxos 算法家族中的日志，尽管日志构建是它们最常见的实际应用。对于 Paxos，这通常是通过使用称为“multi-paxos”的协议扩展来完成的，该协议将日志建模为一系列共识问题，日志中的每个槽一个共识问题。日志在其他协议中更为突出，如[ZAB](http://www.stanford.edu/class/cs347/reading/zab.pdf)T3】、T5】RAFT 和[viewtempted Replication](http://pmg.csail.mit.edu/papers/vr-revisited.pdf)，它们直接模拟了维护分布式、一致日志的问题。

我的怀疑是，我们对这一点的看法有点偏离历史的轨迹，也许是由于分布式计算的理论超过了它的实际应用的几十年。现实中，共识问题有点太简单了。计算机系统很少需要决定单个值，它们几乎总是处理一系列请求。所以日志，而不是简单的单值寄存器，是更自然的抽象。

此外，对算法的关注掩盖了底层日志抽象系统的需求。我怀疑我们最终会把更多的注意力放在日志上，把它当作一个商品化的构件，而不考虑它的实现，就像我们经常谈论哈希表一样，而不关心我们是指线性探测的杂音哈希还是其他变体。日志将成为某种商品化的接口，许多算法和实现竞相提供最佳保证和最佳性能。

### 变更日志 101:表和事件是双重的

让我们稍微回到数据库。在变化记录和表格之间有一种令人愉快的二元性。日志类似于所有借贷和银行流程的列表；一张表是所有往来账户的余额。如果您有一个变更日志，您可以应用这些变更来创建捕捉当前状态的表。该表将记录每个键的最新状态(截至特定的日志时间)。从某种意义上说，日志是更基本的数据结构:除了创建原始表之外，您还可以转换它来创建各种派生表。(是的，对于非关系型的人来说，表可能意味着键控数据存储。)![](../Images/626e5fb25751d4cabf4bdb9ae652b43b.png)

这个过程也可以反过来进行:如果您有一个进行更新的表，您可以记录这些更改，并发布一个包含表状态所有更新的“changelog”。这个 changelog 正是您支持近实时副本所需要的。因此，从这个意义上讲，您可以将表和事件看作是双重的:表支持静态数据，日志捕捉变化。日志的神奇之处在于，如果它是一个*完整的*变更日志，它不仅保存表的最终版本的内容，还允许重新创建可能已经存在的所有其他版本。实际上，这是对桌面上每个先前状态的*的一种备份。*

这可能会让您想起源代码版本控制。源代码管理和数据库之间有着密切的关系。版本控制解决了一个与分布式数据系统必须解决的问题非常相似的问题——管理分布式的、并发的状态变化。版本控制系统通常对补丁序列建模，这实际上是一个日志。您可以直接与当前代码的签出“快照”进行交互，它类似于表格。您会注意到，在版本控制系统中，与在其他分布式有状态系统中一样，复制是通过日志进行的:当您更新时，您只需下载补丁，并将它们应用到您当前的快照。

一些人最近已经从一家销售以日志为中心的数据库的公司那里看到了这些想法。这个[演示](https://www.youtube.com/watch?v=Cym4TZwTCNU)很好地概述了他们是如何在他们的系统中应用这个想法的。当然，这些想法并不是这个系统所独有的，因为十多年来它们一直是分布式系统和数据库文献的一部分。

这可能看起来有点理论化。不要绝望！我们会很快进入实际问题。

### 下一步是什么

在本文的剩余部分，我将尝试给出日志的好处，它超越了分布式计算或抽象分布式计算模型的内部。这包括:

1.  *数据集成*—使组织的所有数据在其所有存储和处理系统中轻松可用。
2.  *实时数据处理*—计算导出的数据流。
3.  *分布式系统设计*—如何通过以日志为中心的设计简化实际系统。

这些用途都围绕着日志作为独立服务的想法。

在每种情况下，日志的有用性都来自于日志提供的简单功能:生成持久的、可重复播放的历史记录。令人惊讶的是，这些问题的核心是让许多机器以它们自己的速率以确定的方式回放历史的能力。

## 第二部分:数据集成

让我首先说明我所说的“数据集成”是什么意思，以及为什么我认为它很重要，然后我们将看到它如何与日志相关联。

数据集成使一个组织的所有服务和系统中的所有数据都可用。

“数据集成”这个词并不常见，但我不知道还有更好的说法。更容易识别的术语 [ETL](http://en.wikipedia.org/wiki/Extract,_transform,_load) 通常只涵盖数据集成的有限部分——填充关系数据仓库。但是我所描述的大部分内容可以被认为是 ETL 的概括，涵盖了实时系统和处理流程。![](../Images/1873b112555f2d8479071fcfdff8329c.png)

在围绕*大数据*的所有令人窒息的兴趣和炒作中，你不会听到太多关于数据集成的内容，但尽管如此，我相信这个“使数据可用”的平凡问题是一个组织可以关注的更有价值的事情之一。

数据的有效使用遵循一种马斯洛需求层次理论。金字塔的基础包括捕获所有相关数据，能够在适用的处理环境中将其放在一起(无论是花哨的实时查询系统还是文本文件和 python 脚本)。这些数据需要以统一的方式建模，以便于阅读和处理。一旦以统一方式捕获数据的这些基本需求得到满足，就有理由在基础设施上工作，以各种方式处理这些数据—MapReduce、实时查询系统等。

值得注意的是显而易见的:如果没有可靠和完整的数据流，Hadoop 集群就只不过是一个非常昂贵且难以组装的空间加热器。一旦数据和处理可用，人们就可以将注意力转移到更好的数据模型和一致的易于理解的语义的更精细的问题上。最后，注意力可以转移到更复杂的处理上——更好的可视化、报告、算法处理和预测。

根据我的经验，大多数组织在这个金字塔的底部都有巨大的漏洞——他们缺乏可靠的完整数据流——但是想要直接跳到高级数据建模技术。这完全是倒退。

所以问题是，我们如何在一个组织的所有数据系统中建立可靠的数据流？

### 数据集成:两个复杂因素

两个趋势使得数据整合更加困难。

**事件数据消防水管**

第一个趋势是事件数据的上升。事件数据记录发生的事情，而不是已经发生的事情。在 web 系统中，这意味着用户活动日志记录，还意味着可靠地操作和监控数据中心的机器所需的机器级事件和统计数据。人们倾向于称之为“日志数据”，因为它经常被写到应用程序日志中，但是这混淆了形式和功能。这些数据是现代网络的核心:毕竟，谷歌的财富是由建立在点击和印象(即事件)基础上的相关性管道产生的。

这种东西并不局限于网络公司，只是网络公司已经完全数字化，所以它们更容易被工具化。长期以来，金融数据一直以事件为中心。 [RFID](http://en.wikipedia.org/wiki/RFID) 给实物增加了这种追踪。我认为随着传统商业和活动的数字化，这种趋势将会继续下去。

这种类型的事件数据记录发生了什么，并且往往比传统数据库使用的数据大几个数量级。这给加工带来了巨大的挑战。

**专业数据系统的爆炸**

第二个趋势来自于专用数据系统的爆炸式增长，在过去的五年里，这些系统变得流行起来，并且经常可以免费获得。专门的系统有 [OLAP](https://github.com/metamx/druid/wiki) 、[搜索](http://www.elasticsearch.org/)、[简单](http://www.rethinkdb.com/) [在线](http://www.slideshare.net/amywtang/espresso-20952131) [存储](http://cassandra.apache.org/)、[批量处理](http://hadoop.apache.org/)、[图形分析](http://graphlab.org/)、[所以](http://redis.io/) [上](http://spark.incubator.apache.org/)。

更多种类的更多数据的组合以及将这些数据放入更多系统的愿望导致了巨大的数据集成问题。

### 日志结构数据流

日志是处理系统间数据流的自然数据结构。食谱非常简单:

获取组织的所有数据，并将其放入一个中央日志中进行实时订阅。

每个逻辑数据源都可以被建模为自己的日志。数据源可以是记录事件(比如点击或页面浏览)的应用程序，也可以是接受修改的数据库表。每个订阅系统尽可能快地读取这个日志，将每个新记录应用到它自己的存储中，并提升它在日志中的位置。订户可以是任何类型的数据系统——缓存、Hadoop、另一个站点中的另一个数据库、搜索系统等。![](../Images/68c9b040d97092ea8230d6fee3a17aa1.png)

例如，日志概念为每个更改提供了一个逻辑时钟，所有订阅者都可以根据该时钟进行测量。这使得关于不同用户系统相对于彼此的状态的推理简单得多，因为每个用户系统都有一个它们已经读取的“时间点”。

为了更具体，考虑一个简单的例子，其中有一个数据库和一组缓存服务器。该日志提供了一种将更新同步到所有这些系统的方法，并对这些系统的时间点进行推理。假设我们用日志条目 X 写一条记录，然后需要从缓存中读取。如果我们想保证我们不会看到过时的数据，我们只需要确保我们不会从任何尚未复制到 x 的缓存中读取数据。

日志还充当缓冲区，使数据生产与数据消费异步。这一点很重要，原因有很多，尤其是当有多个用户可能以不同的费率消费时。这意味着订阅系统可能会崩溃或停机进行维护，并在恢复时赶上进度:订阅者以自己控制的速度消费。诸如 Hadoop 或数据仓库之类的批处理系统可能只消耗每小时或每天的数据，而实时查询系统可能需要实时更新。原始数据源和日志都不了解各种数据目标系统，因此可以在不改变管道的情况下添加和删除消费者系统。

[![](../Images/b12b03429fb76175a1bb93e724b053cb.png)](http://en.wikipedia.org/wiki/Anna_Karenina_principle) 

特别重要的是:目标系统只知道日志，而不知道源系统的任何细节。消费者系统不需要关心数据是来自 RDBMS(一种新式的键值存储),还是在没有任何实时查询系统的情况下生成的。这似乎是一个小问题，但实际上是至关重要的。

我在这里使用术语“日志”而不是“消息传递系统”或“发布订阅”,因为它在语义上更加具体，并且更加接近地描述了您在实际实现中需要什么来支持数据复制。我发现“发布-订阅”并不意味着更多的间接消息寻址——如果您比较任何两个承诺发布-订阅的消息传递系统，您会发现它们保证非常不同的事情，并且大多数模型在这个领域都没有用。您可以将日志看作是一种具有持久性保证和强有序语义的消息传递系统。在分布式系统中，这种通信模型有时被称为(有点可怕的)原子广播。

值得强调的是，日志仍然只是基础设施。掌握数据流的故事还没有结束:故事的其余部分是关于元数据、模式、兼容性以及处理数据结构和演化的所有细节。但是，除非有一种可靠的、通用的方法来处理数据流的机制，否则语义细节是次要的。

### 在 LinkedIn

随着 LinkedIn 从一个集中式关系数据库转移到一系列分布式系统，我看到了这个数据集成问题的快速出现。

目前，我们的主要数据系统包括:

其中每一个都是专门的分布式系统，在其专业领域提供高级功能。

![](../Images/bbbd82112a91ae1aa19e9efd33ea5b9c.png)

甚至在我来到 LinkedIn 之前，这种使用日志进行数据流的想法就已经在 LinkedIn 上流传了。我们开发的最早的基础设施之一是一个名为 [databus](https://github.com/linkedin/databus) 的服务，它在我们早期的 Oracle 表之上提供了一个日志缓存抽象，以扩展对数据库更改的订阅，这样我们就可以提供我们的社交图和搜索索引。

我会讲一点历史来提供背景。在 2008 年左右，我们发布了我们的键值存储后，我开始参与其中。我的下一个项目是尝试建立一个有效的 Hadoop 系统，并将我们的一些推荐流程迁移到那里。由于在这方面没有什么经验，我们很自然地安排了几周的时间来获取和输出数据，其余的时间用于实现复杂的预测算法。于是开始了漫长的跋涉。

我们最初计划只是从现有的 Oracle 数据仓库中抓取数据。第一个发现是，从 Oracle 中快速获取数据是一种黑暗的艺术。更糟糕的是，数据仓库处理不适合我们为 Hadoop 计划的生产批处理—许多处理是不可逆转的，并且特定于正在进行的报告。我们最终避开了数据仓库，直接进入源数据库和日志文件。最后，我们实现了另一个管道来[将数据加载到我们的键值存储](http://data.linkedin.com/blog/2009/06/building-a-terabyte-scale-data-cycle-at-linkedin-with-hadoop-and-project-voldemort)中以提供结果。

这种普通的数据复制最终成为最初开发的主要项目之一。更糟糕的是，任何时候任何管道出现问题，Hadoop 系统基本上都是无用的——对坏数据运行复杂的算法只会产生更多的坏数据。

尽管我们以一种相当通用的方式构建了一些东西，但是每个新的数据源都需要定制配置来设置。它也被证明是大量错误和失败的根源。我们在 Hadoop 上实现的站点功能变得很受欢迎，我们发现自己有一长串感兴趣的工程师。每个用户都有一个他们想要集成的系统列表和一长串他们想要的新数据源列表。

[![](../Images/02e53a0b2cee8719f8d34252f36481eb.png)](http://en.wikipedia.org/wiki/Sisyphus) 

古希腊的 ETL。没什么变化。

我慢慢明白了一些事情。

首先，我们建造的管道虽然有点乱，但实际上非常有价值。仅仅是在新的处理系统(Hadoop)中提供数据的过程就开启了许多可能性。对数据进行新的计算是可能的，这在以前是很难做到的。许多新产品和分析仅仅来自于将之前被锁定在专门系统中的多种数据整合在一起。

第二，很明显，可靠的数据加载需要数据管道的深度支持。如果我们捕获了我们需要的所有结构，我们就可以使 Hadoop 数据加载完全自动化，这样就不会增加添加新数据源或处理模式更改的手动工作——数据会神奇地出现在 HDFS 中，并且会自动为具有适当列的新数据源生成 Hive 表。

第三，我们的数据覆盖率仍然很低。也就是说，如果你看看 LinkedIn 在 Hadoop 中可用的数据的整体百分比，它仍然是非常不完整的。考虑到运营每个新数据源所需的工作量，完成这项工作并不容易。

我们一直在进行的方式，为每个数据源和目的地构建定制的数据负载，显然是不可行的。我们有几十个数据系统和数据仓库。连接所有这些将导致在每对系统之间构建定制管道，如下所示:

![](../Images/21a7ca6475bc2d3a509d88f2883e8e12.png)

请注意，数据通常是双向流动的，因为许多系统(数据库、Hadoop)既是数据传输的来源，也是目的地。这意味着我们最终将为每个系统构建两条管道:一条用于输入数据，另一条用于输出数据。

这显然需要一大群人来建造，而且永远不可行。当我们接近完全连接时，我们最终会有 O(N <sup>2</sup> 条管道。

相反，我们需要一些通用的东西，比如:

![](../Images/70f305924544b901e1f73e2f74697932.png)

我们需要尽可能地将每个消费者与数据源隔离开来。理想情况下，他们应该只与一个单一的数据存储库集成，这样他们就可以访问任何东西。

这个想法是，添加一个新的数据系统——无论是数据源还是数据目的地——应该只创建一个连接到单个管道的集成工作，而不是连接到每个数据消费者。

这一经历让我专注于构建 Kafka，将我们在消息传递系统中看到的东西与数据库和分布式系统内部流行的日志概念结合起来。我们希望某种东西首先作为所有活动数据的中心管道，最终用于许多其他用途，包括 Hadoop 之外的数据部署、监控数据等。

很长一段时间，Kafka 作为一个基础设施产品有点独特(有些人会说很奇怪)——既不是数据库，也不是日志文件收集系统，也不是传统的消息传递系统。但最近亚马逊提供了一项与卡夫卡非常相似的服务，名为 [Kinesis](http://aws.amazon.com/kinesis) 。这种相似性一直延续到分区的处理方式、数据的保留方式，以及 Kafka API 在高级和低级消费者之间相当奇怪的划分。对此我很高兴。你已经创建了一个好的基础设施抽象的标志是 AWS 提供它作为服务！他们对此的愿景似乎与我描述的完全相似:这是连接他们所有分布式系统的管道——DynamoDB、RedShift、S3 等。—也是使用 EC2 进行分布式流处理的基础。

### 与 ETL 和数据仓库的关系

让我们谈一谈数据仓库。数据仓库是一个干净的、集成的数据仓库，用来支持分析。这是个好主意。对于不了解情况的人来说，数据仓库方法包括定期从源数据库中提取数据，将其转换成某种可理解的形式，并将其加载到中央数据仓库中。拥有一个包含所有数据的干净副本的中心位置，对于数据密集型分析和处理来说是一笔非常宝贵的资产。从高层次来看，无论您使用 Oracle、Teradata 或 Hadoop 等传统数据仓库，这种方法都不会有太大变化，尽管您可能会[改变](http://searchdatamanagement.techtarget.com/definition/Extract-Load-Transform-ELT)加载和管理的顺序。

包含干净、集成的数据的数据仓库是一种非凡的资产，但是获取这种资产的机制有点过时了。

![](../Images/e4ce5e7ec06f8a04a5cfc55010e0af48.png)

以数据为中心的组织的关键问题是将干净的集成数据耦合到数据仓库。数据仓库是一个批处理查询基础设施，非常适合于多种报告和特别分析，特别是当查询涉及简单的计数、聚合和过滤时。但是，让批处理系统成为干净完整的数据的唯一存储库意味着数据对于需要实时馈送的系统(实时处理、搜索索引、监控系统等)是不可用的。

在我看来，ETL 实际上是两件事。首先，它是一个提取和数据清理的过程——本质上是释放组织中各种系统中锁定的数据，并删除特定于系统的无意义数据。第二，为数据仓库查询重新构造数据(即，使其适合关系数据库的类型系统，强制形成星型或雪花型模式，可能分解成高性能的[列](http://parquet.io/) [格式](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html)等)。把这两件事混为一谈是个问题。干净、集成的数据存储库应该实时可用，以便在其他实时存储系统中进行低延迟处理和索引。

我认为这有额外的好处，使数据仓库 ETL 在组织上更具可伸缩性。数据仓库团队的典型问题是，他们负责收集和清理组织中每个其他团队生成的所有数据。激励机制并不一致:数据生产者通常不太清楚数据仓库中数据的用途，最终创建的数据很难提取，或者需要大量、难以扩展的转换才能变成可用的形式。当然，中央团队从来没有成功地扩展到与组织其他部分的步伐相匹配，所以数据覆盖总是不稳定的，数据流是脆弱的，并且变化是缓慢的。

更好的方法是使用一个中央管道，即日志，以及一个定义良好的 API 来添加数据。与这个管道集成并提供一个干净的、结构良好的数据馈送的责任在于这个数据馈送的生产者。这意味着，作为系统设计和实现的一部分，他们必须考虑将数据取出并以良好的结构形式交付给中央管道的问题。添加新的存储系统对数据仓库团队来说无关紧要，因为他们有一个集成中心。数据仓库团队只处理更简单的问题，即从中央日志加载结构化的数据，并执行特定于他们系统的转换。

![](../Images/61476b6f882b44931155cbdd31b91228.png)

当考虑在传统数据仓库之外采用额外的数据系统时，关于组织可伸缩性的这一点变得尤为重要。比方说，一个人希望提供对组织的完整数据集的搜索能力。或者，假设有人希望提供具有实时趋势图和警报的亚秒级数据流监控。在这两种情况下，传统数据仓库甚至 Hadoop 集群的基础设施都不合适。更糟糕的是，为支持数据库加载而构建的 ETL 处理管道可能对这些其他系统毫无用处，这使得引导这些基础设施就像采用数据仓库一样困难。这可能是不可行的，并且可能有助于解释为什么大多数组织不能轻松地为他们的所有数据提供这些功能。相比之下，如果组织已经构建了统一的、结构良好的数据源，那么让任何新系统完全访问所有数据只需要一点集成管道就可以了。

这个体系结构还为特定的清理或转换提供了一组不同的选项:

1.  这可以由数据制作者在将数据添加到公司范围的日志之前完成。
2.  它可以作为对日志的实时转换来完成(这又会产生新的转换后的日志)
3.  这可以作为加载到某个目标数据系统的过程的一部分来完成

最好的模型是在数据发布者将数据发布到日志之前，让*清理*。这意味着要确保数据是规范形式的，并且不会保留产生数据的特定代码或维护数据的存储系统的任何遗留内容。这些细节最好由创建数据的团队来处理，因为他们最了解自己的数据。此阶段应用的任何逻辑都应该是无损和可逆的。

任何可以实时完成的增值转换都应该作为对生成的原始日志馈送的后处理来完成。这将包括事件数据的会话化，或者添加其他普遍感兴趣的派生字段。原始日志仍然可用，但这种实时处理会产生包含扩充数据的派生日志。

最后，在加载过程中，只应执行特定于目标系统的聚合。这可能包括将数据转换成特定的星型或雪花型模式，以便在数据仓库中进行分析和报告。因为这个阶段最自然地映射到传统的 ETL 过程，现在是在一组更干净、更统一的流上完成的，所以它应该被大大简化。

### 日志文件和事件

让我们谈一谈这个架构的附带好处:它支持解耦的、事件驱动的系统。

在 web 行业中，处理活动数据的典型方法是将数据记录到文本文件中，在文本文件中可以将数据放入数据仓库或 Hadoop 中进行聚合和查询。这个问题与所有批处理 ETL 的问题是一样的:它将数据流与数据仓库的能力和处理时间表联系起来。

在 LinkedIn，我们以日志为中心的方式构建了我们的事件数据处理。我们使用 Kafka 作为中心的多用户事件日志。我们已经定义了几百种事件类型，每一种都捕获了关于特定类型动作的独特属性。这涵盖了从页面浏览、广告展示和搜索到服务调用和应用程序异常的所有内容。

为了理解这样做的好处，想象一个简单的事件——在工作页面上显示一个工作发布。作业页面应该只包含显示作业所需的逻辑。然而，在一个相当动态的站点中，这很容易被与显示工作无关的附加逻辑所混淆。例如，假设我们需要集成以下系统:

1.  我们需要将这些数据发送到 Hadoop 和数据仓库进行离线处理
2.  我们需要对视图进行计数，以确保查看者没有尝试某种内容抓取
3.  我们需要汇总这个视图，以显示在招聘海报的分析页面上
4.  我们需要记录视图，以确保我们正确地为该用户的任何工作推荐设置印象上限(我们不想一遍又一遍地显示相同的内容)
5.  我们的推荐系统可能需要记录视图，以正确跟踪该工作的受欢迎程度
6.  等等

很快，展示工作的简单行为变得相当复杂。随着我们添加其他显示工作的地方——移动应用程序，等等——这种逻辑必须延续下去，复杂性也随之增加。更糟糕的是，我们需要与之交互的系统现在有些纠结——负责展示工作的人需要了解许多其他系统和功能，并确保它们被正确集成。这只是问题的一个玩具版本，任何真正的应用程序都会更加复杂。

“事件驱动”风格提供了一种简化的方法。job display 页面现在只显示一个作业，并记录显示了一个作业以及该作业的相关属性、查看器和有关该作业显示的任何其他有用信息。其他每个感兴趣的系统——推荐系统、安全系统、工作海报分析系统和数据仓库——都只是订阅提要并进行处理。显示代码不需要知道这些其他系统，如果添加了新的数据消费者，也不需要更改。

### 构建可伸缩日志

当然，将发布者和订阅者分开并不是什么新鲜事。但是，如果您希望保存一个提交日志，作为一个消费者规模的网站上发生的所有事情的多订户实时日志，可伸缩性将是一个主要挑战。如果我们不能构建一个足够快速、廉价、可伸缩的日志来实现大规模应用，那么使用日志作为一种通用的集成机制将永远不会超过一个优雅的幻想。

系统人员通常认为分布式日志是一种缓慢、沉重的抽象(通常只将其与 Zookeeper 可能适用的“元数据”联系起来)。但是，对于专注于记录大型数据流的深思熟虑的实现，这不一定是真的。在 LinkedIn，我们目前每天通过 Kafka 运行超过 600 亿条独特的消息写入(如果算上来自数据中心之间的[镜像的写入，有几千亿条)。](http://kafka.apache.org/documentation.html#datacenters)

我们在卡夫卡中使用了一些技巧来支持这种规模:

1.  对日志进行分区
2.  通过批处理读取和写入优化吞吐量
3.  避免不必要的数据拷贝

为了允许水平扩展，我们将日志分成几个分区:

每个分区都是一个完全有序的日志，但是分区之间没有全局排序(除了您可能在消息中包含的一些挂钟时间)。将消息分配到特定分区是可由作者控制的，大多数用户选择通过某种关键字(例如，用户 id)进行分区。分区允许在碎片之间不协调的情况下进行日志追加，并允许系统的吞吐量随 Kafka 集群大小线性扩展。

每个分区被复制到可配置数量的副本上，每个副本都有分区日志的相同副本。在任何时候，他们中的一个人都会成为领导者；如果主服务器失败，其中一个副本服务器将接管主服务器。

缺少跨分区的全局顺序是一个限制，但是我们没有发现这是一个主要的限制。事实上，与日志的交互通常来自成百上千个不同的进程，因此谈论它们行为的整体顺序是没有意义的。相反，我们提供的保证是每个分区都是保序的，Kafka 保证来自单个发送者的附加到特定分区的内容将按照它们被发送的顺序被传送。

日志和文件系统一样，很容易针对线性读写模式进行优化。日志可以将较小的读取和写入组合成较大的高吞吐量操作。卡夫卡积极地追求这种优化。在发送数据时，在写入磁盘时，在服务器之间复制时，在向消费者传输数据时，以及在确认提交的数据时，都会发生从客户端到服务器的批处理。

最后，Kafka 使用简单的二进制格式，在内存日志、磁盘日志和网络数据传输之间进行维护。这允许我们利用众多优化，包括[零拷贝数据传输](https://www.ibm.com/developerworks/library/j-zerocopy)。

这些优化的累积效果是，您通常可以以磁盘或网络支持的速率读写数据，即使维护的数据集大大超出了内存。

这篇文章并不打算主要写卡夫卡，所以我就不赘述了。你可以在这里阅读 LinkedIn 方法的更详细概述，在这里阅读卡夫卡设计的全面概述[。](http://kafka.apache.org/documentation.html#design)

## 第三部分:日志和实时流处理

到目前为止，我只描述了一种将数据从一个地方复制到另一个地方的奇特方法。但是在存储系统之间传输字节并不是故事的结尾。原来“日志”是“流”的另一个词，日志是[流处理](http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/)的核心。

但是，等等，流处理到底是什么？

如果您是 90 年代末和 21 世纪初[数据库](http://cs.brown.edu/research/aurora/vldb03_journal.pdf) [文献](http://db.cs.berkeley.edu/papers/cidr03-tcq.pdf)或半成功的[数据](http://www-03.ibm.com/software/products/us/en/infosphere-streams) [基础设施](http://en.wikipedia.org/wiki/StreamBase_Systems) [产品](http://en.wikipedia.org/wiki/Truviso)的粉丝，您可能会将流处理与构建 SQL 引擎或用于事件驱动处理的“盒子和箭头”接口的工作联系起来。

如果您关注开源数据系统的发展，您可能会将流处理与这个领域的一些系统联系起来——例如， [Storm](http://storm-project.net/) 、 [Akka](http://akka.io/) 、 [S4](http://incubator.apache.org/s4) 和 Samza。但是大多数人认为这是一种异步消息处理系统，与支持集群的 RPC 层没有太大的不同(事实上，这个领域的一些东西正是如此)。

这两种观点都有点局限。流处理与 SQL 无关。也不局限于实时处理。没有什么内在的理由不能使用各种不同的语言来表达计算，处理昨天或一个月前的数据流。![](../Images/f5a037262562d1dff1168b1461226dc4.png)

我认为流处理是更广泛的东西:连续数据处理的基础设施。我认为计算模型可以像 MapReduce 或其他分布式处理框架一样通用，但具有产生低延迟结果的能力。

处理模型的真正驱动力是数据收集的方法。批量收集的数据自然会批量处理。当数据被连续收集时，自然会被连续处理。

美国人口普查为批量数据收集提供了一个很好的例子。人口普查定期开始，通过让人们挨家挨户地走动，对美国公民进行暴力发现和计数。这在 1790 年人口普查首次开始时很有意义。当时的数据收集本质上是面向批量的，它包括骑在马背上并在纸上写下记录，然后将这批记录运送到一个中心位置，在那里人类将所有计数相加。如今，当你描述人口普查过程时，人们马上会想，为什么我们不记录出生和死亡人数，或者连续地或者以任何需要的粒度来进行人口统计。

这是一个极端的例子，但是许多数据传输过程仍然依赖于定期转储、批量传输和集成。处理批量转储的唯一自然方法是批处理。但是，随着这些过程被连续馈送所取代，人们自然开始转向连续处理，以消除所需的处理资源并减少延迟。

比如 LinkedIn，几乎完全没有批量数据采集。我们的大部分数据要么是活动数据，要么是数据库更改，这两者都是连续发生的。事实上，当你考虑任何业务时，基本机制几乎总是一个连续的过程——事件实时发生，正如杰克·鲍尔告诉我们的那样。当数据被批量收集时，几乎总是由于一些人工步骤或缺乏数字化，或者是一些非数字化过程的自动化遗留下来的历史遗迹。当机械装置是邮件并且人类做处理的时候，数据的传输和反应曾经是非常慢的。自动化的第一步总是保留原始过程的形式，所以这通常会持续很长时间。

每天运行的生产“批处理”作业通常有效地模拟了一种窗口大小为一天的连续计算。当然，基础数据总是在变化的。这些在 LinkedIn 中非常普遍(在 Hadoop 中让它们工作的机制非常复杂)，以至于我们实现了一个完整的[框架](http://engineering.linkedin.com/datafu/datafus-hourglass-incremental-data-processing-hadoop)来管理递增的 Hadoop 工作流。

从这个角度来看，很容易对流处理有不同的看法:它只是处理，包括正在处理的底层数据中的时间概念，不需要数据的静态快照，因此它可以以用户控制的频率产生输出，而不是等待到达数据集的“结尾”。从这个意义上说，流处理是批处理的一种推广，而且，考虑到实时数据的普及，这是一种非常重要的推广。

那么，为什么传统的观点认为流处理是一个利基应用呢？我认为最大的原因是缺乏实时数据收集使得连续处理成为一个学术问题。

我认为缺乏实时数据收集可能是导致商业流处理系统失败的原因。他们的客户仍然在为 ETL 和数据集成进行面向文件的日常批处理。构建流处理系统的公司专注于提供附加到实时数据流的处理引擎，但事实证明，当时很少有人真正拥有实时数据流。实际上，在我在 LinkedIn 工作的早期，一家公司试图向我们出售一个非常酷的流处理系统，但由于当时我们所有的数据都是以每小时一次的文件收集的，我们能想到的最好的应用程序是在一小时结束时将每小时一次的文件传输到流系统中！他们指出这是一个相当普遍的问题。这个例外实际上证明了这里的规则:金融这个流处理取得了一些成功的领域，正是实时数据流已经成为规范并且处理已经成为瓶颈的领域。

即使存在一个健康的批处理生态系统，我认为流处理作为一种基础设施风格的实际适用性是相当广泛的。我认为它弥补了实时请求/响应服务和离线批处理之间的基础设施差距。对于现代互联网公司，我认为他们大约 25%的代码属于这一类。

事实证明，日志解决了流处理中的一些最关键的技术问题，我将对此进行描述，但它解决的最大问题只是使数据在实时多订户数据源中可用。对于那些对更多细节感兴趣的人，我们已经开源了 Samza，这是一个流处理系统，明确地建立在许多这些想法之上。我们在文档[这里](http://samza.incubator.apache.org/learn/documentation/0.7.0)中更详细地描述了这些应用。

### 数据流图表

![](../Images/26f132882cb0c937f8aa832be20ca822.png)

流处理最有趣的方面与流处理系统的内部无关，而是与它如何扩展我们对早期数据集成讨论中的数据馈送的概念有关。我们主要讨论了原始数据的提要或日志——各种应用程序执行过程中产生的事件和数据行。但是流处理允许我们包含从其他提要计算出来的提要。对于消费者来说，这些派生的提要与计算它们的原始数据的提要没有什么不同。这些派生的提要可以封装任意的复杂性。

让我们深入研究一下。对于我们的目的来说，流处理作业是从日志中读取并将输出写入日志或其他系统的任何东西。它们用于输入和输出的日志将这些过程连接成一个处理阶段图。事实上，以这种方式使用集中式日志，您可以将组织的所有数据捕获、转换和流程视为一系列日志和写入这些日志的流程。

一个流处理器根本不需要一个奇特的框架:它可以是从日志中读取和写入的任何进程或进程集，但是可以提供额外的基础设施和支持来帮助管理处理代码。

集成中日志的目的有两个。

首先，它使每个数据集成为多订户和有序的。回忆一下我们的“状态复制”原则，记住顺序的重要性。为了更具体地说明这一点，考虑一个来自数据库的更新流——如果我们在处理过程中对同一个记录的两个更新重新排序，我们可能会产生错误的最终输出。这种顺序比 TCP 之类的东西提供的更持久，因为它不局限于单个点对点链路，并且在进程失败和重新连接之后仍然存在。

第二，日志为进程提供缓冲。这是非常基本的。如果处理以不同步的方式进行，很可能会发生上游数据产生作业产生数据的速度比另一个下游作业消耗数据的速度快。当这种情况发生时，处理必须阻塞、缓冲或丢弃数据。丢弃数据可能不是一个选项；阻塞可能会导致整个处理图陷入停顿。日志充当一个非常非常大的缓冲区，允许进程重新启动或失败，而不会减慢处理图的其他部分。当将这种数据流扩展到更大的组织时，这种隔离尤其重要，因为在更大的组织中，处理工作是由许多不同的团队完成的。我们不能让一个错误的工作导致反压力，停止整个处理流程。

暴风和[萨姆扎](http://samza.incubator.apache.org/)都是以这种方式建造的，可以使用卡夫卡或其他类似的系统作为它们的日志。

### 有状态实时处理

一些实时流处理只是无状态的一次记录转换，但许多用途是更复杂的计数、聚合或流中窗口的连接。例如，人们可能希望用关于进行点击的用户的信息来丰富事件流(比如说点击流)，实际上是将点击流加入到用户帐户数据库中。不可避免地，这种处理最终需要处理器维护某种状态:例如，在计算计数时，您需要维护到目前为止的计数。如果处理器本身可能出现故障，如何正确维护这种状态？

最简单的选择是将状态保存在内存中。然而，如果进程崩溃，它将失去其中间状态。如果状态仅在一个窗口上维护，则流程可能会退回到日志中窗口开始的点。然而，如果一个人做一个小时以上的计数，这可能是不可行的。

另一种方法是简单地将所有状态存储在远程存储系统中，并通过网络连接到该存储。这样做的问题是没有数据的局部性和大量的网络往返。

我们如何支持像“表”这样的东西，用我们的处理进行分区？

我们回想一下关于表和日志的二元性的讨论。这为我们提供了工具，能够将流转换为与我们的处理放在一起的表，以及为这些表处理容错的机制。

流处理器可以将其状态保存在本地“表”或“索引”中——一个 [bdb](http://www.oracle.com/technetwork/products/berkeleydb) 、 [leveldb](https://code.google.com/p/leveldb) ，甚至是更不寻常的东西，如 [Lucene](http://lucene.apache.org/) 或 [fastbit](https://sdm.lbl.gov/fastbit) 索引。这个 this store 的内容来自它的输入流(可能在第一次应用任意转换之后)。它可以为自己保存的本地索引记录一个 changelog，以便在崩溃和重启时恢复其状态。这种机制提供了一种通用机制，可以将任意索引类型中的共同分区状态保持在传入流数据的本地。

当进程失败时，它从 changelog 中恢复其索引。日志是在备份时将本地状态转换成一种增量记录。

这种状态管理方法有一个很好的特性，即处理器的状态也作为日志来维护。我们可以把这个日志看作是数据库表的变更日志。事实上，处理器维护着一个非常类似于联合分区表的东西。因为这个状态本身就是一个日志，所以其他处理器可以订阅它。在处理的目标是更新最终状态并且该状态是处理的自然输出的情况下，这实际上非常有用。

当出于数据集成的目的与来自数据库的日志相结合时，日志/表二元性的威力就变得很明显了。可以从数据库中提取变化日志，并由各种流处理器以不同的形式进行索引，以结合事件流。

我们在 Samza 中给出了这种管理有状态处理方式的更多细节，以及更多实际例子[这里](http://samza.incubator.apache.org/learn/documentation/0.7.0/container/state-management.html)。

### 原木压实

当然，我们不能希望永远保留所有状态变化的完整日志。除非想要使用无限的空间，否则必须以某种方式清理日志。我会简单讲一下这一点在卡夫卡中的实现，让它更具体。在 Kafka 中，根据数据是包含键控更新还是事件数据，清理有两个选项。对于事件数据，Kafka 只支持保留一个数据窗口。通常，这被配置为几天，但是窗口可以根据时间或空间来定义。但是，对于键控数据，完整日志的一个很好的特性是，您可以重放它来重新创建源系统的状态(可能在另一个系统中重新创建它)。

但是，随着时间的推移，保留完整的日志会占用越来越多的空间，重播的时间也会越来越长。因此，在卡夫卡那里，我们支持一种不同类型的保留。我们不是简单地丢弃旧的日志，而是删除过时的记录——即主键有更新的记录。通过这样做，我们仍然可以保证日志包含源系统的完整备份，但是现在我们不能再重新创建源系统的所有先前状态，只能重新创建最近的状态。我们称这个特性为[日志压缩](https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction)。

## 第四部分:制度建设

我想讨论的最后一个主题是日志在在线数据系统的数据系统设计中的作用。

日志在分布式数据库中为数据流服务的角色和它在大型组织中为数据集成服务的角色之间有相似之处。在这两种情况下，它负责数据流、一致性和恢复。如果不是一个非常复杂的分布式数据系统，组织到底是什么？

### 松绑？

因此，如果您稍微眯起眼睛，您可能会将您组织的整个系统和数据流视为一个分布式数据库。您可以将所有单个面向查询的系统(Redis、SOLR、Hive 表等)视为数据的特定索引。您可以将像 Storm 或 Samza 这样的流处理系统视为一个非常成熟的触发器和视图物化机制。我注意到，传统的数据库人员非常喜欢这种观点，因为它最终向他们解释了人们究竟在用所有这些不同的数据系统做什么——它们只是不同的索引类型！

不可否认，现在数据系统的类型激增，但实际上，这种复杂性一直存在。即使在关系数据库的全盛时期，组织也有很多很多的关系数据库！因此，自大型机以来，当所有数据都在一个地方时，真正的集成可能就不存在了。将数据隔离到多个系统中有许多动机:规模、地理、安全性和性能隔离是最常见的。但这些问题可以通过一个好的系统来解决:例如，一个组织可以拥有一个 Hadoop 集群，其中包含所有数据，并为一个庞大而多样化的群体提供服务。

因此，在向分布式系统转移的过程中，数据处理已经有了一种可能的简化:将每个系统的许多小实例合并成几个大集群。许多系统还不够好，不允许这样做:它们没有安全性，或者不能保证性能隔离，或者伸缩性不够好。但是这些问题都是可以解决的。

我的观点是，不同系统的爆炸是由构建分布式数据系统的困难引起的。通过缩减到单个查询类型或用例，每个系统都能够将其范围缩小到可以构建的事物集合。但是运行所有这些系统会产生太多的复杂性。

我认为这在未来有三个可能的方向。

第一种可能性是现状的延续:系统的分离或多或少会持续很长时间。这可能是因为分发的困难太难克服，或者是因为这种专门化为每个系统提供了新的便利和能力。只要这是真的，数据集成问题将仍然是成功使用数据的最重要的事情之一。在这种情况下，集成数据的外部日志将非常重要。

第二种可能性是，可能会有一次重新整合，一个具有足够通用性的单一系统开始将所有不同的功能合并成一个单一的超级系统。这个超级系统可能表面上看起来像关系数据库，但它在一个组织中的用途将会大不相同，因为你只需要一个大系统，而不是无数个小系统。在这个世界上，除了这个系统内部解决的问题，没有真正的数据整合问题。我认为建立这样一个系统的实际困难使得这不太可能。

不过，还有另一种可能的结果，作为一名工程师，我确实觉得这很有吸引力。新一代数据系统的一个有趣的方面是，它们实际上都是开源的。开源提供了另一种可能性:数据基础设施可以被分解成一组服务和面向应用的系统 API。您已经在 Java 堆栈中看到了这种情况:

如果你把这些东西堆成一堆，稍微眯着眼睛看，它开始看起来有点像乐高版的分布式数据系统工程。你可以把这些成分拼凑在一起，创造出大量可能的系统。这显然不是一个与最终用户相关的故事，他们可能更关心 API，而不是它是如何实现的，但这可能是在一个不断发展的更加多样化和模块化的世界中获得单一系统的简单性的一条途径。如果因为可靠、灵活的构建模块的出现，分布式系统的实现时间从几年到几周，那么合并成一个单一的整体系统的压力就消失了。

### 日志在系统架构中的位置

假设外部日志存在的系统允许单个系统放弃它们自己的大量复杂性并依赖共享日志。以下是我认为日志可以做的事情:

*   通过对节点的并发更新进行排序来处理数据一致性(无论是最终的还是即时的)
*   提供节点之间的数据复制
*   向编写器提供“提交”语义(即，只有当您的写入保证不会丢失时才确认)
*   从系统提供外部数据订阅源
*   提供恢复丢失数据的失败复制副本或引导新复制副本的能力
*   处理节点之间的数据重新平衡。

这实际上是分布式数据系统的一个重要部分。事实上，剩下的大部分都与最终的面向客户端的查询 API 和索引策略有关。这正是因系统而异的部分:例如，全文搜索查询可能需要查询所有分区，而按主键查询可能只需要查询负责该键数据的单个节点。

这是如何工作的。系统分为两个逻辑部分:日志和服务层。该日志按顺序捕获状态变化。服务节点存储查询所需的任何索引(例如，键值存储可能有 btree 或 sstable 之类的东西，搜索系统可能有一个倒排索引)。写操作可以直接写入日志，尽管它们可以由服务层代理。写入日志会产生一个逻辑时间戳(比如日志中的索引)。如果系统是分区的，我假设它是分区的，那么日志和服务节点将具有相同数量的分区，尽管它们可能具有非常不同的机器数量。![](../Images/7c94a50dfa22a9d1b911d638db91c4b4.png)

服务节点订阅日志，并按照日志存储它们的顺序尽快将写操作应用到其本地索引。

客户端可以通过提供写操作的时间戳作为其查询的一部分，从任何节点获得“读取您的写操作”语义—接收此类查询的服务节点将比较所需的时间戳与其自己的索引点，并且如果必要的话，延迟请求，直到它至少索引到该时间，以避免提供陈旧数据。

服务节点可能需要也可能不需要任何“主控权”或“领导者选举”的概念。对于许多简单的用例，服务节点可以完全没有领导者，因为日志是事实的来源。

分布式系统必须做的一件更棘手的事情是处理故障节点的恢复或在节点间移动分区。典型的方法是让日志只保留固定的数据窗口，并将其与存储在分区中的数据的快照相结合。同样可能的是，日志保留数据的完整副本，而[对日志本身进行垃圾收集](https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction)。这将大量的复杂性从特定于系统的服务层转移到了通用的日志层。

有了这个日志系统，您就获得了一个针对数据存储内容的完全开发的订阅 API，该 API 将 ETL 提供给其他系统。事实上，许多系统可以共享相同的日志，同时提供不同的索引，如下所示:

请注意，这样一个以日志为中心的系统本身是如何立即为其他系统中的处理和加载提供数据流的。同样，流处理器可以使用多个输入流，然后通过另一个索引输出的系统为它们提供服务。

我发现这种将系统分解到日志和查询 api 中的观点非常有启发性，因为它让您将查询特征与系统的可用性和一致性方面分开。实际上，我认为这甚至是一种有用的方式，可以从心理上考虑一个不是以这种方式构建的系统，以便更好地理解它。

值得注意的是，虽然 Kafka 和 Bookeeper 是一致的日志，但这不是一个要求。你可以很容易地将一个类似于[发电机](http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html)的数据库分解成一个最终一致的 [AP](http://en.wikipedia.org/wiki/CAP_theorem) 日志和一个键值服务层。处理这样的日志有点棘手，因为它会重新传递旧消息，并依赖订阅者来处理(很像 Dynamo 本身)。

在日志中有一个单独的数据副本的想法(特别是如果它是一个完整的副本)让许多人觉得是一种浪费。实际上，尽管有一些因素使得这不是一个问题。首先，日志可能是一种特别有效的存储机制。我们在生产 Kafka 服务器上的每个数据中心存储超过 75TB。同时，许多服务系统需要更多的内存来有效地提供数据(例如，文本搜索通常都在内存中)。服务系统也可以使用优化的硬件。例如，大多数实时数据系统要么内存不足，要么使用固态硬盘。相比之下，日志系统只做线性读写，所以使用大容量的多 TB 硬盘驱动器是相当不错的。最后，如上图所示，在数据由多个系统提供服务的情况下，日志的成本分摊到多个索引上。这种组合使得外部日志的开销非常小。

这正是 LinkedIn 用来构建自己的许多实时查询系统的模式。这些系统以数据库为基础(使用 Databus 作为日志抽象，或者使用 Kafka 的专用日志),并在数据流之上提供特定的分区、索引和查询功能。这就是我们实现搜索、社交图和 OLAP 查询系统的方式。事实上，将单个数据提要(无论是实时提要还是来自 Hadoop 的派生提要)复制到多个服务系统中进行实时服务是很常见的。这被证明是一个巨大的简化假设。这些系统都不需要外部可访问的写 api，Kafka 和数据库被用作记录系统，更改通过该日志流向适当的查询系统。写入由托管特定分区的节点在本地处理。这些节点盲目地将日志提供的提要转录到自己的存储中。可以通过重放上游日志来恢复故障节点。

这些系统对日志的依赖程度各不相同。一个完全可靠的系统可以利用日志进行数据分区、节点恢复、重新平衡以及一致性和数据传播的所有方面。在这种设置中，实际的服务层实际上就是一种“缓存”,其结构支持直接写入日志的特定类型的处理。

## 结束了

如果你已经做到这一步，你应该知道我所知道的关于日志的大部分内容。

这里有一些有趣的参考资料，你可能想看看。

每个人似乎对相同的事物使用不同的术语，因此将数据库文献与分布式系统、各种企业软件阵营以及开源世界联系起来有点令人困惑。尽管如此，这里还是有一些大体方向的提示。

学术论文、系统、演讲和博客:

*   很好地概述了[状态机](http://www.cs.cornell.edu/fbs/publications/smsurvey.pdf)和[主备份](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.5896)复制
*   PacificA 是一个通用框架，用于在微软实现基于日志的分布式存储系统。
*   不是每个人都喜欢逻辑时间来写日志。谷歌的新数据库试图使用物理时间，并将时间戳视为一个范围，直接对时钟漂移的不确定性进行建模。
*   [数据学](http://www.datomic.com/) : [解构数据库](https://www.youtube.com/watch?v=Cym4TZwTCNU)是 Clojure 的创始人里奇·希基(Rich Hickey)关于他的初创公司数据库产品的一次精彩演示。
*   消息传递系统中的回滚恢复协议综述。我发现这是对容错和日志在数据库外恢复的实际应用的非常有帮助的介绍。
*   反应式宣言(Reactive Manifesto)—实际上我不太确定反应式编程是什么意思，但我认为它的意思和“事件驱动”是一样的。这个链接没有太多信息，但是马丁·奥德斯基(Scala 名人)的[这个类](https://www.coursera.org/course/reactive)看起来很有趣。
*   Paxos!
    *   原文是[这里是](http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf)。莱斯利·兰波特有一段有趣的[历史](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos)，讲述了该算法是如何在 20 世纪 80 年代创建的，但直到 1998 年才发表，因为评论者不喜欢论文中的希腊寓言，他也不想改变它。
    *   即使最初的论文发表了，它也没有被很好地理解。Lamport [再次尝试](http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf)，这一次甚至包括一些如何使用这些新型自动计算机的“无趣细节”。它仍然没有被广泛理解。
    *   弗雷德·施耐德和[巴特勒·兰普森](http://research.microsoft.com/en-us/um/people/blampson/58-consensus/Abstract.html)分别给出了在真实系统中应用 Paxos 的更详细的概述。
    *   一些谷歌工程师总结了他们在 Chubby 中实现 Paxos 的经验。
    *   实际上，我发现所有的 Paxos 文件都很难理解，但我还是尽职尽责地努力读完了。但是你不需要因为[这个视频](https://www.youtube.com/watch?v=JEpsBg0AO6o)由[约翰推出](http://www.stanford.edu/~ouster/cgi-bin/papers/lfs.pdf)(日志结构文件系统的名气！)会让一切变得非常简单。不知何故，这些共识算法在交流过程中画出来会更好，而不是静态地写在纸上。具有讽刺意味的是，这个视频是为了表明 Paxos 很难理解而制作的。
    *   [使用 Paxos 构建可扩展的一致数据存储](http://arxiv.org/pdf/1103.2408.pdf):这是一篇关于使用日志构建数据存储的很酷的论文，作者是 Jun，合著者之一，也是 Kafka 最早的工程师之一。
*   Paxos 有竞争对手！实际上，其中的每一个都与日志的实现更为接近，并且可能更适合于实际的实现:
    *   Barbara Liskov 的 Viewstamped Replication 是一个早期的直接模拟日志复制的算法。
    *   [Zab](http://www.stanford.edu/class/cs347/reading/zab.pdf) 是 Zookeeper 使用的算法。
    *   RAFT 是一种更容易理解的共识算法的尝试。同样由约翰·奥斯特胡特制作的[视频演示](https://www.youtube.com/watch?v=YbZ3zDzDnrw)也很棒。
*   您可以在不同的实际分布式数据库中看到登录操作的作用。
    *   [PNUTS](http://www.mpi-sws.org/~druschel/courses/ds/papers/cooper-pnuts.pdf) 是一个尝试大规模应用于传统分布式数据库的以日志为中心的设计的系统。
    *   HBase 和 T2 Bigtable 都给出了现代数据库中日志的另一个例子。
    *   LinkedIn 自己的分布式数据库 [Espresso](http://www.slideshare.net/amywtang/espresso-20952131) 和 PNUTs 一样，使用日志进行复制，但采用了一种略有不同的方法，使用底层表本身作为日志的来源。
*   如果你发现自己在为一个复制算法进行比较，这篇文章可以帮助你。
*   [Replication:Theory and Practice](http://www.amazon.com/Replication-Practice-Lecture-Computer-Theoretical/dp/3642112935)是一本很棒的书，收集了一堆关于分布式系统中复制的总结论文。很多章节都是在线的(比如 [1](http://disi.unitn.it/~montreso/ds/papers/replication.pdf) 、 [4](http://research.microsoft.com/en-us/people/aguilera/stumbling-chapter.pdf) 、 [5](http://www.distributed-systems.net/papers/2010.verita.pdf) 、 [6](http://www.cs.cornell.edu/ken/history.pdf) 、 [7](http://www.pmg.csail.mit.edu/papers/vr-to-bft.pdf) 、 [8](http://engineering.linkedin.com/distributed-systems/www.cs.cornell.edu/fbs/publications/TrustSurveyTR.pdf) )。
*   流处理。这是一个有点太宽泛的总结，但这里有一些我喜欢的东西。

企业软件也有同样的问题，只是名称不同，规模较小，使用 XML。呵呵，开个玩笑。算是吧。

*   [事件源](http://martinfowler.com/eaaDev/EventSourcing.html)—据我所知，这基本上是企业软件工程师对“状态机复制”的说法。有趣的是，同样的想法会在如此不同的背景下再次被发明出来。事件源似乎专注于较小的内存用例。这种应用程序开发方法似乎将事件日志中发生的“流处理”与应用程序结合了起来。因为当处理规模大到需要数据分区时，这就变得相当重要了，所以我把流处理作为一个独立的基础设施原语来关注。
*   [改变数据捕获](http://en.wikipedia.org/wiki/Change_data_capture)—有一个围绕从数据库中获取数据的小行业，这是对日志最友好的数据提取方式。
*   [企业应用集成](http://en.wikipedia.org/wiki/Enterprise_application_integration)似乎是为了解决数据集成问题，如果你拥有的是一组现成的企业软件，如 CRM 或供应链管理软件。
*   [复杂事件处理(CEP)](http://en.wikipedia.org/wiki/Complex_event_processing) :很肯定没有人知道这意味着什么，或者它与流处理有什么不同。区别似乎在于关注的是无序的流和事件过滤和检测，而不是聚合，但在我看来，这是没有区别的区别。我认为任何一个系统在某一方面做得好，就应该在另一方面做得好。
*   [企业服务总线](http://en.wikipedia.org/wiki/Enterprise_service_bus)—我认为企业服务总线的概念非常类似于我所描述的关于数据集成的一些想法。这个想法似乎在企业软件社区中取得了一定的成功，但在 web 人员或分布式数据基础设施人群中却鲜为人知。

有趣的开源材料:

*   Kafka 是“日志即服务”项目，是这篇文章的基础。
*   Bookeeper 和 [Hedwig](https://cwiki.apache.org/confluence/display/BOOKKEEPER/HedWig) 组成了另一个开源的“日志即服务”。它们似乎更针对数据系统内部，而不是事件数据。
*   [Databus](https://github.com/linkedin/databus) 是一个为数据库表提供类似日志覆盖的系统。
*   Akka 是 Scala 的一个 actor 框架。它有一个插件 [eventsourced](https://github.com/eligosource/eventsourced) ，提供持久性和日志功能。
*   Samza 是我们在 LinkedIn 上开发的一个流处理框架。它使用了本文中的许多想法，并与 Kafka 集成作为底层日志。
*   Storm 是流行的流处理框架，与 Kafka 很好地集成在一起。
*   [Spark Streaming](http://spark.incubator.apache.org/docs/0.7.3/streaming-programming-guide.html) 是一个流处理框架，是 [Spark](http://spark.incubator.apache.org/) 的一部分。
*   Summingbird 是 Storm 或 Hadoop 之上的一层，提供了一个方便的计算抽象。

我试着跟上这个领域，所以如果你知道一些我遗漏的东西，让我知道。

我给你留下这个信息: