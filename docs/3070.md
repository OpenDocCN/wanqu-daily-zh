# 循环社会。算法社交编程|作者:伊亚德·拉万|麻省理工学院媒体实验室| Medium

> 原文:[https://medium . com/MIT-media-lab/society-in-the-loop-54 ffd 71 CD 802？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://medium.com/mit-media-lab/society-in-the-loop-54ffd71cd802?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

# 循环社会

## 编程算法社会契约

> **更新(2017 . 7 . 24):**本文的扩展版将出现在《伦理与信息技术》关于“AI &伦理”的专刊上。可以在[这里](https://arxiv.org/abs/1707.07232)找到免费的预印本。

麻省理工学院媒体实验室主任 T2 最近发表了一篇名为《[环中社会人工智能》的论文，并感谢我创造了这个术语。既然它已经出来了，我想详细说明一下我所说的“循环中的社会”是什么意思，并强调它在人文学科和计算之间架起了一座桥梁。](https://joi.ito.com/weblog/2016/06/23/society-in-the-.html)

## 人在回路中

我所说的“循环中的社会”是一个旧思想的放大版本，它将“[人类置于自动化系统的循环中](https://en.wikipedia.org/wiki/Human-in-the-loop)”(HITL)。在 HITL 系统中，操作员是控制系统的重要组成部分，处理监督、异常控制、优化和维护等具有挑战性的任务。整个研究领域都集中在如何最好地设计 HITL 系统以优化性能，最大限度地利用人类的判断，同时避免人类的局限性，例如对信息过载或系统认知偏差的敏感性。



最近，已经有许多文章讨论了将 HITL 思维应用于人工智能(AI)和机器学习系统的重要性(例如，参见[这篇奥莱利报告](http://radar.oreilly.com/2015/02/human-in-the-loop-machine-learning.html)中的几个例子)。HITL 人工智能已经持续了一段时间。例如，许多应用程序从你的行为中学习，以提高它们更好地为你服务的能力(例如，通过预测你将要键入的下一个单词)。同样，当你在 Gmail 中将一封电子邮件标记为“垃圾邮件”时，你是复杂的机器学习算法(具体来说是一个[主动学习](https://en.wikipedia.org/wiki/Active_learning_(machine_learning))系统)循环中的许多人之一，帮助它不断寻求改善电子邮件的垃圾邮件或非垃圾邮件分类。

当然，HITL 的重要性不仅限于改进垃圾邮件过滤或其他分类任务。各种形式的 HITL 可以在从医疗诊断到机器人战争等领域发挥重要作用。设计这些系统是一项重要的任务。

## 治理算法的回路中的社会

当一个人工智能系统不服务于一个狭窄的、定义明确的功能，而是一个具有广泛社会影响的广泛功能时，会发生什么？考虑一个控制数十亿辆自动驾驶汽车的人工智能算法；或者一套影响数十亿公民政治信仰和偏好的新闻过滤算法；或者调节整个经济中资源和劳动力分配的算法。这些*治理算法*的 HITL 等价物是什么？这就是我们从 HITL 到*循环社会* (SITL)的质变。

HITL 人工智能是将个人或群体的判断嵌入到狭义人工智能系统的优化中，而 SITL 则是将整个社会的判断嵌入到社会结果的算法治理中。换句话说，SITL 更类似于政府和被治理公民之间的互动。现代政府是被统治者和他们的统治者之间达成的隐性协议的产物，或者说是社会契约，目的是实现公民的 T2 公意。类似地，SITL 可以被视为将公众意志嵌入一个*算法社会契约*的尝试。

在以人为本的政府中，公民利用各种渠道——例如民主投票、民意调查、民间社会机构、社交媒体——向政府表达他们的期望。同时，政府通过其官僚机构和各种分支机构承担治理职能，并最终由公民进行评价。现代社会(理论上)是 SITL 以人为本的治理机器。其中一些机器的程序比其他的更好。

类似地，随着越来越多的治理功能被编码到人工智能算法中，我们需要在人类价值观和治理算法之间建立通道。



## 算法社会契约

为了实现 SITL，我们需要知道人们对人工智能的期望是什么类型的行为，并使政策制定者和公众能够向机器表达这些期望(目标、道德、规范、社会契约)。为了结束这个循环，我们还需要新的度量和方法来评估人工智能行为与可量化的人类价值的对比。换句话说:我们需要建立新的工具，使社会能够*编程、调试和监控人类和治理算法之间的算法社会契约*。

在治理算法中实现 SITL 控制有许多困难。首先，这些算法中的一些会产生经济学家所说的负[外部性](https://en.wikipedia.org/wiki/Externality)——由不参与决策的第三方产生的成本。例如，如果自动驾驶汽车算法优先考虑乘客的安全——乘客拥有它们或付费使用它们——它们可能会不成比例地增加行人承担的风险。量化这些类型的外部性并不总是直截了当的，特别是当它们作为一个长的、间接的因果链的结果出现时。

实现 SITL 的另一个困难是控制算法通常实现隐式的权衡。基于人类专家的治理已经实现了权衡。例如，降低道路上的速度限制降低了想要快速回家的司机的效用，同时增加了司机和行人的整体安全性。完全消除事故是可能的——通过将速度限制降至零并禁止汽车行驶——但这也将消除驾驶的效用，监管机构试图通过不断的学习过程达成一种社会满意的平衡。公民需要表达他们对治理算法的期望的方式，就像他们对人类监管者一样。

## SITL 公式

总而言之，构建 SITL 系统需要实现两个不同的过程。首先，就像 HITL 一样，它需要人类对算法和数据驱动系统做出的决策进行监督。第二，*与* HITL 不同，SITL 也需要在社会不同利益相关者的目标之间进行协商和强制权衡。那就是:

> 回路中的社会=回路中的人+社会契约

简而言之:

> SITL = HITL + SC

我发现这种高层次的描述很有吸引力，因为它从关于哪些价值观应该是普遍的，哪些应该是特定文化的讨论中抽象出来。它只是说，治理算法必须以我们管理与政府关系的方式来管理——这意味着来自社会契约理论和政治哲学的所有概念工具都可以派上用场。

## SITL 缺口

为什么我们还没有到那里？关于渗透和支配我们生活的不透明算法所带来的社会和法律挑战，已经有了一系列深思熟虑的条约。其中最著名的包括弗兰克·帕斯奎尔的《黑箱社会》和伊莱·帕里泽的《T2》和《过滤泡沫》。虽然这些著作有助于阐明许多挑战，但它们往往缺乏解决方案。这是因为我们仍然缺乏以机器可以理解的方式表达社会期望(如伦理、规范、法律原则)的机制。我们还缺乏一套全面的机制来审查控制算法的行为是否符合精确的预期。下图显示了这一差距。让社会参与进来需要我们在人文学科和计算机之间架起一座桥梁。



这幅图的一个重要组成部分是，人类价值观和人工智能都在持续不断地共同进化——这是丹尼·希利斯提醒我的。因此，技术能力的发展可以不可逆转地改变社会认为可以接受的东西——想想由于智能手机和互联网提供的效用，隐私规范发生了怎样的变化。

## 前进的道路

越来越多的来自人文科学和计算机科学的研究人员已经认识到 SITL 鸿沟，并采取一致的努力来弥合它。这些包括量化算法歧视的新方法、量化新闻过滤算法中的偏见的方法、从机器中引出公众道德期望的调查、指定可接受的隐私-效用权衡的方法等等。

启蒙时代标志着人类向现代 T2 社会契约的过渡。缩小 SITL 差距可能会让人类更接近实现一种新的算法社会契约。





