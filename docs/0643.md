# 就 Twitter 的基础设施采访 Raffi Krikorian

> 原文:[http://www.infoq.com/articles/twitter-infrastructure?UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](http://www.infoq.com/articles/twitter-infrastructure?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

Twitter 平台工程副总裁 Raffi Krikorian 就 Twitter 如何为意外的流量高峰做准备以及系统架构如何设计为支持故障提供了见解。

InfoQ:嗨，拉菲。请你向 InfoQ 的观众和读者介绍一下你自己好吗？

> <input type="hidden" name="" value="" id="cont_item_primary_topic">
> 
> #### 相关赞助内容
> 
> 拉菲:当然。我叫拉菲·克里科里安。我是 Twitter 平台工程的副总裁。我们是负责 Twitter 后端基础设施的团队。

InfoQ:在“空中楼阁”的帮助下，Twitter 创造了新的 TPS 峰值记录。Twitter 如何应对不可预测的高峰流量？

> 拉菲:当然。所以你所指的是空中楼阁事件，我们内部称之为空中楼阁事件，那是在东京播出的电视节目。在那次活动中，我们创造了每秒 34000 条推文的新纪录。通常情况下，Twitter 每秒会发出 5000 到 10000 条推文，所以这远远超出了我们的标准操作范围。我想这说明了我们的一些事情。我认为它反映了 twitter 对整个世界的反应，就像世界上发生的事情会在 Twitter 上得到反映。
> 
> 因此，我们最终为这样的事情做准备的方式实际上是提前数年的工作，就像这种类型的事件可能在没有真正通知的情况下随时发生。所以我们最终要做的是对 Twitter 基础设施进行负载测试。我们每个月都会运行这些程序——我不知道现在的确切时间表是怎样的——然后我们对 Twitter 的每一个系统进行分析。
> 
> 因此，当我们在 Twitter 上构建架构和系统时，我们每周都会查看所有这些系统的性能，以真正了解我们认为该系统目前每个服务的理论容量是什么样的，然后我们尝试了解整体的理论容量是什么样的。由此，我们可以决定:(1)在任何给定的时间，我们是否有合适数量的机器投入生产，或者我们是否需要购买更多的计算机？(2)我们可以就系统是否有效运行进行理性的对话。
> 
> 因此，如果我们有某些服务，例如，与其他服务相比，每秒只能处理一半的请求数，我们应该查看这些服务，并从体系结构上了解它们的性能是否正确，或者我们是否需要做出改变。
> 
> 所以对我们来说，像空中楼阁这样的建筑是一个缓慢的进化过程。我们做出改变，我们看到这种改变如何反应，这种改变在系统中如何表现，我们在缓慢滚动的基础上做出决定，这是否是我们可以接受的，我们做出权衡，比如我们是购买更多的机器还是编写新的软件来承受这种情况？
> 
> 因此，虽然我们以前从未经历过类似空中楼阁的事件，但我们的一些负载测试已经将我们推到了这些极限，所以我们很乐意知道它在现实生活中何时发生。我们就像“是的，它确实起作用了。”

InfoQ:Twitter 中有什么应急计划吗？你们会在平时做一些练习吗，比如关闭一些服务器或者交换机？

> 拉菲:对。所以我们做了两件不同的事情作为我们的应急计划，也许是三件，这取决于你如何看待它。每一个系统都被仔细地记录了如何开启它，如何关闭它，所以我们对每一个系统都有操作手册，这样我们就知道在紧急情况下应该怎么做。我们已经考虑过不同类型的失败。我们不认为我们考虑了所有的事情，但是至少我们认为我们已经记录了最常见的事情，并且我们知道我们需要做什么。
> 
> 第二，我们总是针对生产运行测试，所以我们很好地理解了当我们真正努力时系统会是什么样子，这样我们就可以练习了。所以，就像我们真的很努力，随叫随到的团队，他们可能会收到传呼或发生什么事情，或者传呼可能会响，所以我们可以尝试决定我们是否需要做一些不同的事情，以及如何对此做出反应。
> 
> 第三，我们从网飞获得了一些灵感。网飞有他们所谓的混沌猴子，可以主动杀死生产中的机器。我们在 Twitter 中有类似的东西，所以我们可以确保我们没有在某个地方意外引入单点故障。因此，我们可以随机杀死数据中心内的机器，并确保在此过程中服务看不到任何信号。
> 
> 所有这些都要求我们对所有不同系统的成功率有很好的透明度。所以我们有一个巨大的董事会。这是一面玻璃墙，上面有所有这些图表，所以我们可以真正看到 Twitter 内部发生了什么。当这些事件发生时，我们可以立即看到是否有变化，是否是 Twitter 的流量，或者是否是数据中心的故障，这样我们就可以尽快做出反应。

**InfoQ:如何隔离系统中的坏模块？出问题的时候，你第一时间的反应是什么？**

> 拉菲:对。因此，Twitter 目前的架构方式是，故障应该相对局限于发生故障的功能。当然，现在你越深入，问题就变得越大。因此，如果我们的存储机制突然出现问题，一系列不同的系统将会表现出出错的行为。例如，如果有人在网站上犯了一个错误，这几天不会影响 API。
> 
> 因此，我们知道再次出现问题的方法是能够看到系统的不同图表，然后我们针对不同的服务设置超过不同阈值的警报。因此，如果 API 的成功率低于某个数字，一堆寻呼机会立即响起，Twitter 上的每项服务都会有人随时待命，他们可以尽快做出反应。
> 
> 我们的运营团队和网络指挥中心也会看到这一点，他们可能会尝试一些非常基本的东西，就像我们应该关闭再打开，看看会发生什么，而第二条轨道上的实际软件开发人员则试图真正了解系统发生了什么问题。因此，运营部门正努力确保网站尽快恢复。软件开发试图理解到底哪里出了问题，我们是否有需要解决的 bug。
> 
> 这就是我们最终的反应。但是，就像我说的，Twitter 的架构很好地限制了失败。如果我们认为它会传播，或者我们认为，例如，社交图有问题，它只出现在这个特定的功能中，社交图团队将立即开始通知其他所有人，以防他们应该警惕出现问题。
> 
> 我喜欢开玩笑地说，这是我们这些天的优势之一，就是应急管理，就像在灾难发生的情况下你会做什么，因为它可能随时发生，我与世界的合同是 Twitter 将会打开，所以你不必担心它。

InfoQ:新的架构在稳定性和性能方面帮助很大。你能给我们简单介绍一下新建筑吗？

> 拉菲:当然。所以当我几年前加入 Twitter 的时候，我们在我们所谓的整体代码库上运行这个系统。所以你用 Twitter 软件做的一切都在一个代码库中，任何人都可以部署，任何人都可以接触，任何人都可以修改。听起来很棒。理论上，那实际上是极好的。这意味着 Twitter 的每个开发者都有权利做正确的事情。
> 
> 然而在实践中，有一个平衡的行为，开发者需要理解每件事实际上是如何工作的，以便做出改变。在现实中，我担心的是 Twitter 编写新代码的速度，人们没有深入思考正确的东西——只是在他们以前没有见过的地方。我认为这是开发人员编写软件的标准方式。就好像我不明白我完全需要做什么来做这个改变，但是如果我只改变这一行，它可能会得到我想要的效果。我不是说这是不好的行为。这是非常谨慎和权宜的行为。但这意味着当你这么做的时候，技术债务正在累积。
> 
> 所以我们所做的是，我们把这个庞大的代码库分成数百个不同的服务，组成 Twitter。这样，Twitter 的每一个业务逻辑和每一个功能都有了真正的主人。实际上有一个团队，他们的工作之一就是为 Twitter 管理照片。还有另一个团队，他们的工作之一是管理 Twitter 的 URL，因此现在整个公司都有主题专家，例如，当你想做一个改变 URL 工作的功能时，你可以咨询他们。
> 
> 既然我们已经用所有这些不同的方式把它分解了，我们现在有了主题专家，但是这也允许我们所说的事情:隔离失败，也隔离特性开发。如果你想改变推特的工作方式，你只需要改变一定数量的系统。你不必再改变 Twitter 中的一切，这样我们就可以很好地隔离失败和发展。

InfoQ:你们有一个系统叫做决策人，决策人在系统中的角色是什么？

> 拉菲:当然。所以 Decider 是我们在 Twitter 的运行时配置机制之一。我的意思是，我们可以在不进行部署的情况下关闭 Twitter 中的功能和软件。所以 Twitter 的每一项服务都在不断地关注决策系统，看看 Twitter 现在的运行时值是多少。这实际上是如何映射的，我可以说，例如，discover 主页有一个包装它的 Decider 值，该 Decider 值告诉 discover 它现在是打开还是关闭。
> 
> 因此，我可以将 discover 部署到 Twitter 中，并将其部署在 Decider 认为应该关闭的状态。所以这一点我们不会得到不一致的状态。例如，Twitter 的 discover 或任何功能都可以在许多机器上运行。它不能在一台机器上运行，所以你不希望陷入不一致的状态，有些机器有这个特性，有些没有。因此，我们可以使用 Decider 将其部署关闭，然后当它在我们希望它打开的所有计算机上时，我们可以通过扳动 Decider 开关在整个数据中心自动打开它。
> 
> 这也使我们能够进行基于百分比的控制。所以我现在可以说，它在所有的机器上，我只希望 50%的用户得到它。事实上，我可以做出这样的决定，而不是作为 Twitter 部署方式的副作用。因此，这使我们能够真正拥有对 Twitter 的运行时控制，而不必推送代码。推送代码实际上是一件危险的事情，就像在我们这样的系统中，不仅仅是 Twitter，而是任何大系统中，与失败最相关的是软件开发错误。这样我们就能以相对安全的方式部署软件，因为它是关闭的。慢慢地，有目的地打开它，确保它是好的，然后按照我想要的速度加速。

InfoQ:Twitter 如何在线推送代码？能否请您和我们分享一下部署过程？比如几个不同的阶段，你选择日推还是周推还是两者都选？

> 拉菲:当然。所以 Twitter 的部署，因为我们有这个服务架构，实际上是由每一个单独的团队控制的。因此，团队有责任确保当他们部署代码时，可能受其影响的每个人都应该知道你在做什么，网络控制中心也应该知道你在做什么，以便他们对系统有一个全局的看法。但这真的取决于每一个团队来决定他们何时以及是否想要推动。
> 
> 平均来说，我认为团队有两周或三周的部署时间表。一些团队每天都进行部署；有些团队一个月只部署一次。但是部署过程对每个人来说看起来都是一样的:您部署到一个开发环境中。这是为了让开发人员可以很快地破解它，做出改变，看看产品经理，看看设计师，确保它做正确的事情。然后，我们在 Twitter 中部署了我们称之为“金丝雀系统”的系统，这意味着它正在获得实时生产流量，但我们还不依赖它的结果。所以它只是基本上加载它，以确保它处理它的性能，我们可以查看返回的结果，比较它，并手动检查它，以确保它做了我们认为它会做的给定实时流量。
> 
> 我们的测试场景可能没有涵盖实时流量的所有不同边缘情况，因此这是我们了解真实测试场景应该是什么样子的一种方式。然后我们进入金丝雀，然后我们在黑暗中部署，然后我们慢慢地开始加速，以真正了解它在规模上是什么样子。事实上，这种增长可能需要一天到一周的时间，就像我们有不同的产品，我们在一两周内就增长到了 100 种。我们已经添加了不同的产品，在几分钟内我们已经增加到 100%。
> 
> 所以还是那句话，这真的取决于团队。而且每个团队都要对他们的功能负责，对他们的服务负责。因此，这是他们的要求，他们想怎么做，但这些发展阶段，金丝雀，黑暗阅读，由决策者斜升是每个人都遵循的模式。

InfoQ:Twitter 中有大量的数据。您必须有一些特殊的基础设施(如 Gizzard 和 Snowflake)和方法来存储数据，甚至实时处理它们。

> 拉菲:对。所以我认为这是两个不同的问题。因此，我们如何吸收所有这些进入 Twitter 的数据，因为 Twitter 是一个实时系统，就像我测量一条推文在毫秒内传递给 Twitter 用户的延迟。然后是第二个问题，你有很多数据。我们如何处理这些数据？
> 
> 所以第一个你是对的。我们有像雪花，鸡胗之类的系统来处理推文摄取。显然，推文只是进入 Twitter 的数据的一部分。我们有喜欢的东西。我们有转发。我们有人发送直接信息。人们会改变他们的头像，背景图片等等。所以所有人都点击网址；人们加载网页。这些都是进入 Twitter 的事件。
> 
> 因此，我们开始吸收所有这些信息并记录下来，以便进行分析。这是一件非常困难的事情。实际上，根据输入的数据类型，我们有不同的 SLA。所以推特，我们用毫秒来衡量。例如，为了绕过数据库锁定，我们开发了 Snowflake，它可以以令人难以置信的速度为我们生成唯一的 id，并且以分散的方式进行，这样我们在为我们生成 id 时就不会出现单点故障。
> 
> 我们有 Gizzard，它处理流入的数据并尽可能快地对其进行分片，这样我们就不会在系统中的不同集群上出现热点，就像它实际上试图以概率方式分散负载，这样数据库就不会因流入的数据量而过载。同样，推特在一个系统中传播速度非常快。
> 
> 例如，日志，比如人们点击东西，人们查看推文，它们的 SLA 以分钟而不是毫秒来衡量。所以这些进入完全不同的管道。现在大部分都是基于 Scribe 的。所以这些只是慢慢地渗透，聚集，收集，然后跳到 HDFS，这样我们就可以对它们进行后续分析。
> 
> 对于长期保留，所有数据，无论是否是实时的，都将在 HDFS 结束，我们在那里运行大量的 MapReduce 作业和 Hadoop 作业，以真正了解系统中发生了什么。
> 
> 因此，我们试图在目前需要处理的事情之间取得平衡，特别是考虑到我们拥有的大量数据，以及我们应该将数据放在哪里，因为这些未记录的数据积累得非常快。比如，如果我每天生成 4 亿条推文，而 Twitter 已经运行了几年，你可以想象我们的语料库有多大。因此，HDFS 为我们处理所有这些，然后我们可以运行这些大量的 MapReduce 工作。

InfoQ: Twitter 对工程师来说是一个神奇的地方，一名工程师在 Twitter 的成长道路是什么？尤其是如何成为像你这样成功的极客，请大家给点建议？

> Raffi:嗯，我不能说我是一个成功的工程师，因为我现在不再写软件了。我在 Twitter 开始时是一名工程师，如今我已经升到了负责大量工程的职位。
> 
> Twitter 有几个不同的哲学和心态，但我们有一条工程师的职业道路，基本上包括解决越来越难的问题。我们想说的是，你构建的特性有多好并不重要。在某些情况下，确实如此。但是真正喜欢的是你在你工作的项目中投入的技术思想和技术价值的水平。
> 
> 因此，通过 Twitter 的增长在很大程度上是在一种基于同行的机制下完成的。例如，非常具体地谈论促销。在 Twitter 从一个级别晋升到下一个级别需要共识——不是共识，而是需要更高级别的一群工程师同意，是的，你已经做了在 Twitter 达到这个级别所需的工作。
> 
> 为了帮助实现这一点，经理们要确保项目交给寻求巨大挑战的工程师。工程师可以在团队之间流动。例如，他们不局限于 tweet 团队，也不局限于时间轴团队。如果一个工程师说，“我想在移动团队工作，因为那很有趣。我认为对我来说有职业发展。事实上，作为一个管理大量此类事务的人，我的工作就是让这一切成为可能。”所以你可以在 Twitter 上做任何你想做的事情。我会告诉你，在运营工程方面，我优先考虑的是什么，公司优先考虑的是用户增长、资金或你想打造的功能。然后工程师们应该流向他们认为他们能产生最大影响的项目。
> 
> 除此之外，我在 Twitter 内部经营一所小型大学，我们称之为 Twitter 大学。这是一群人，他们的全部工作就是训练。例如，如果一名工程师想加入移动团队，但他们是后端 Java 开发人员，我们会说“太好了。我们已经创建了一个培训课程，这样你就可以学习 Android 工程或 iOS 工程，你可以参加为期一周的课程，这将带你到达你致力于代码库的地方，然后你就可以真正加入这个团队。”所以这给了你一种在 Twitter 上扩展你的视野的方法，一种安全地决定你是否想去尝试新事物的方法。
> 
> 所以我们投资我们的工程师，因为老实说，他们是公司的支柱。工程师们创造了我们在 Twitter 和全世界都在使用的东西，所以我尽可能多地给他们机会，让他们尝试不同的东西，用许多不同的方式极客。

## 关于面试官

Raffi Krikorian 是 Twitter 平台工程副总裁。他的团队管理 Twitter 应用的业务逻辑、可伸缩交付、API 和认证。他的团队帮助创建了 iOS 5 Twitter 集成以及“The X Factor”+Twitter 投票机制。