# 如何设计更好的社交媒体

> 原文:[https://medium . com/s/story/how-to-fix-what-social-media-has-broken-cb0b 2737128？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://medium.com/s/story/how-to-fix-what-social-media-has-broken-cb0b2737128?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

# 有利于吸引人类注意力的东西往往对人类不利

我们的饲料可以让我们生气是有原因的。

想象你走在街上，听到有人打架。声音很大，咄咄逼人，人们在喊着什么。你可能会暂时停下来看看发生了什么——这是你的天性。

如果你个人认识打架的人，你可能会立即选择一方——你甚至可能会卷入其中。至少，你会注意到。

这就是社交媒体经常对我们做的:**它鼓励我们观察冲突，并在我们没有什么意见的话题上选择立场。**

从本质上来说，它是一个意见服务机器。在社交媒体上，并不是所有的观点都能得到平等的对待。



我们的大多数内容提要和时间线不再按时间顺序排序。相反，决定向我们展示哪些内容是基于我们参与其中的可能性。



Instagram, Facebook, Twitter, Youtube and others have moved away from chronological sorting to help users process more information and to keep them on-site



愤怒等情绪反应是参与度的强有力指标。使用最基本的算法对我们的订阅源进行排序，这种有分歧的内容将首先显示，因为*它比其他类型的内容更能吸引注意力。*



A hypothetical distribution based on interviews with platform engineers



这些内容会引发我们自己的情绪反应。当我们回应时，我们会有规律地将我们的情绪推向世界的其他地方。

这是我们如何在社交媒体上分享的简化模型:



Middle step skipped when users simply Retweet or Share content



愤怒的内容让我们可以称之为 ***的愤怒瀑布*——**道德判断和厌恶的病毒式爆发。这些已经主导了我们的订阅和对话，并成为文化时代精神的重要组成部分。



Outrage triggers are often shared, triggering others and creating outrage cascades.



# 道德愤怒=病毒式传播

NYU 大学的研究员威廉·j·布雷迪最近在病毒性社交媒体帖子中发现了一种模式。他研究了成千上万条推文的大规模数据集，发现使用道德和情感语言的[帖子每使用一个道德和情感关键词就会获得 20%的提升。](https://drive.google.com/file/d/0B3NfwG0qz3qOaUl5bVZpQWRWc2c/view?usp=sharing)

> ***保守派*** ***举例推文:***
> “同性婚姻是一个**毒辣的**，**邪恶的谎言**旨在毁灭我们的民族
> *-@ overpasses 4 America*
> 
> ***自由派举例推特:*** 摩门教新政策禁止同性父母的孩子——本教会要**惩罚孩子**？你在开玩笑吗？！？**羞耻"** - @ *玛蒂娜*

这些推文中的每一条都包含了道德指责和 T2 对他人的谴责。他们激起了深刻的情感反应，更有可能被认同他们的人看到&，为病毒式传播和参与提供了可衡量的推动力。

这是人们在网上分享分裂的、令人愤慨的和情绪化的内容的一个隐藏的基础。

这不仅仅适用于我们的个人帖子。它可能适用于我们在社交媒体上分享的任何内容——评论、模因、视频、文章。它孕育了一个道德愤怒的生态系统，被任何地方的内容创造者利用，[包括新闻机构](/the-mission/the-enemy-in-our-feeds-e86511488de)，仅仅因为它有效。

脸书、Twitter、YouTube 和其他网站优先考虑这类内容，因为这是我们点击、悬停和回复的内容。这是吸引观众的隐性途径。当试图吸引注意力时，我们的愤怒、恐惧和厌恶是噪音中的一个信号。

通过这些工具的主导地位——在我们的媒体、我们的谈话和生活中——我们已经看到我们共同的话语变得丑陋、分裂和越来越两极分化。

# 我们能做些什么呢？

这是我们可能会考虑的四个设计变化，以改善我们在线分享的方式:



# 给出人性化的提示

*在人们发帖前用具体的提示将他们推向正确的方向*



耶鲁大学克罗克特实验室 [的莫莉·克罗克特提出](https://static1.squarespace.com/static/538ca3ade4b090f9ef331978/t/5a53c0d49140b7212c35b20e/1515438295247/Crockett_2017_NHB_Outrage.pdf)我们无法从身体上看到他人的情绪反应，这可能会助长社交媒体上的负面行为。在网上，我们看不到他人的痛苦，这让我们更愿意变得不友善。



尼古拉斯·克里斯塔基斯 (也在耶鲁)已经表明，现实生活中的社交网络可以通过简单的人工智能来影响，以帮助改善群体行为和结果。我们可以考虑在这里加入这样的提示。

这是几个我们可以在人们发帖后立即实施的干预措施——让我们能够促使用户变得善良和有更好的行为。



关于感知去人性化的研究表明，在数字环境中，我们可能会对人们更加具有惩罚性。增加像这样的移情反应可能有助于增加我们的数字化身作为人类的感知。(这项研究很有意思:像我们在个人资料照片中面部的垂直/水平方向这样简单的事情就能决定其他人是否认为我们应该受到惩罚。)



***你的愤怒不太可能被对方听到。*** 更好的思考方式是:你的推文越离谱，对方越不可能看到。[这是基于布雷迪的研究](https://drive.google.com/file/d/0B3NfwG0qz3qOaUl5bVZpQWRWc2c/view?usp=sharing)，该研究表明，带有情感/道德语言的推文在意识形态网络中的传播方式是有限的。

对于真正想与其他观众交流的人来说，这可能会让他们停下来，帮助他们重新思考。虽然这可能不会阻止大多数发布煽动性内容的人，但有些人可能会重新考虑他们的语言。这可以用如何让其他受众更容易获得的基本信息来框定。



我们会做一些让自己事后后悔的愤怒的事情。 花点时间暂停、回顾和撤销被标记为有害的内容可能会降低我们在最糟糕的时刻分享这些内容的可能性。

*注意:只要内容被正确地索引和标记，所有上述提示也可以用于类似链接内容的文章。*



***与人意见相左很难。在公共场合与人意见相左就更难了。*** 当我们在公共场合对别人的帖子发表评论时，会有巨大的社会压力在起作用:我们被展示，在一群人的注视下为一个想法而争论。私下回复——默认为直接信息——可能会鼓励人们打开侧边栏进行对话，减少外部压力。

这一切都很棒，但是我们如何首先找到这些帖子呢？这不是很棘手吗？是的，它是。让我们进入杂草中一会儿。



# 用更好的指标挑出不健康的内容

*标记和过滤特定内容类型我们希望看到更少的*

这比听起来难多了。我们需要度量来训练算法和人类找到我们不想要的东西。如果我们用不正确的度量标准进行干预，我们很容易陷入一些真正黑暗的地方。

算法是人类智慧的代表——就像任何人类创造一样，它们可以继承和放大我们的观点和缺陷。这就是所谓的 [**算法偏差**](https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/)**，它可以表现在股票市场失灵、[系统性贷款歧视、不公平的教师评估，甚至是种族不公正的监禁判决](https://www.propublica.org/series/machine-bias)。**

**脸书已经围绕对用户“有意义”的指标训练了他们的新闻订阅算法。这个标准的问题是许多强烈的人类反应——如道德义愤、厌恶和愤怒——被广泛认为是有意义的。**

**为了做到这一点，我们需要更好的度量标准。**

**通过正确地衡量用户不希望看到的内容类型，我们可以开始给用户一个菜单，准确地代表他们的偏好，而不仅仅是他们会点击什么。**



**Social media is built for present self**



**这是三个可能的候选内容，我们可能希望少一些 *:* **【愤怒的】****【有毒的】****遗憾的**。这些是起点。**

****重要提示:** 标记引发特定负面反应的内容会立即进入言论自由领域。谁说平台应该操纵其他人的声音，如果他们说的东西让某些人觉得冒犯的话？那不是审查吗？这是一个巨大而关键的问题，也是我将要解决的问题。请继续阅读。**



**These metrics don’t include the important work that is being done in information fidelity: false news, disinformation, and state-sponsored propaganda, though they could help with reducing its spread.**



**一旦这些内容类型被识别出来，我们就可以训练受监督的 AI 在未来对其进行标记。**

## **令人遗憾的内容**

**后悔是我们在社交媒体上花费大量时间后最常见的情绪之一。这在很大程度上是由于我们点击的内容类型，尽管我们知道我们不应该这样做。**

**这是基于 ***当前偏差*** 的概念:当考虑两个未来时刻之间的权衡时，人们自然倾向于给予更接近当前时间的收益更大的权重。**

**衡量目前的偏见是棘手的，但是一个开始可能是在用户消费一组内容后，向他们呈现一个不显眼的、简单的反馈提示。这将需要一些深思熟虑的设计选择，以平衡频率与流量。**

## **有毒成分**

**毒性可以通过要求人们对帖子进行评级来确定，从*到 ***健康，*** 有毒被定义为“一个粗鲁、不尊重或不合理的帖子，可能会让你离开讨论。”这是 [Perspective](https://www.perspectiveapi.com/#/) API 的模型 Jigsaw 的一个项目(Google 的一部分，该工具的最早版本有相当大的缺陷 [](https://arxiv.org/pdf/1702.08138.pdf) [](https://www.engadget.com/2017/09/01/google-perspective-comment-ranking-system/))。实现这一点的一个关键部分是确保这些术语由不同的、有代表性的用户样本来定义。***

## ***愤怒的内容***

***这是使用道德、情感和谴责他人的语言的内容。我们可以通过索引最初由[乔纳森·海特](https://twitter.com/JonHaidt)和[杰西·格拉汉姆](http://www-bcf.usc.edu/~jessegra/)开发的[道德基础词典](http://www.moralfoundations.org/othermaterials)并结合其他词典来定义它。这一定义可以扩大和完善，以包括“谴责他人”和极端化的语言。***



# ***默认情况下过滤不健康内容***

****用户发布负面帖子后，过滤该内容的提供方式****



***我们在社交媒体上分享的所有内容都已经通过了互动过滤器。脸书、推特和其他网站根据内容吸引注意力的可能性(或者你付给他们的推广费用)来提升或降低内容。问题是某些类型的内容——愤怒的、耸人听闻的、点击诱饵的等等。—以不健康的方式自然地侵入我们的注意力。***



***不健康的内容可以按比例降低权重，以考虑其自然的病毒式传播。当然，社交媒体公司已经定期这样做，以解决滥用内容、盗版、垃圾邮件和数以千计的其他变量。他们也可以对其他类型的触发内容做同样的事情。这将按比例降低引起愤怒、有毒和令人遗憾的帖子的显著性，并给予它们与其他类型内容更平等的地位。***

***注意:这进一步进入了言论自由的领域，这个领域带来了过滤泡沫和回音室。我将在下面讨论这两个问题。***



# ***给予用户订阅控制***

****为用户提供对自己算法内容的控制****



***在 2016 年大选期间，当政治愤怒和尖酸刻薄成为我们社交媒体的主要产品时，这似乎是一个坏主意。用户难道不会把自己困在充满错误信息的现实中吗？这难道不会增加政治孤立吗？但随着识别虚假新闻、错误信息和宣传的努力越来越多，支撑我们依赖黑盒算法的问题依然存在。缺少的是我们为什么会在社交媒体上看到我们所看到的东西的透明度。***

***解决这个问题的方法是让用户访问决定我们看到哪些内容的编辑流程。从最基本的**倒序**到**高度** **始乱终弃的供稿，**这一过程可以打通。***

## ***如何做到这一点:仪表板***



***Gobo’s dashboard with several initial metrics.***



***麻省理工学院媒体实验室的一个名为 Gobo 的项目开始了这一过程，开发了一个开放的社交信息聚合器，展示了一个仪表板的样子。这使得用户可以通过诸如*政治、粗鲁、*和*病毒之类的东西来过滤他们的内容。*这些指标是思考开放提要可能是什么样子的良好起点。***

## ***另一个类比:积木，或食谱***

***这可能看起来像是一套探索新观点和深度的新工具。这些提要可以单独讨论、重新配置和共享。***



***这个为用户设计饲料控制的过程将开启一个关于什么是健康信息饮食的关键对话——这是目前被他们的专有性质所掩盖的东西。强迫用户做出决定也可能鼓励用户更多地了解他们经常遇到的不健康的触发因素。***



# ***最大的问题***

***这些干预是另一种形式的审查吗？如果我们降低网上说我们不同意的话的人的能见度，那不是压制言论吗？***

***这是一个*巨大的*问题。***

***在这个时候，很难夸大这些平台对社会的重要性和影响力。近年来，一些最大、最重要的社会变革来自通过社交媒体分享的道德愤怒所催化的行动主义。没有他们，这些文化和政治运动中的许多都是不可能的:#ArabSpring，#TeaParty，#BlackLivesMatter，#MeToo。如果这些声音被压制会发生什么？***

***但这是一个关键问题:社交媒体已经压制了我们的声音。***

***我们目前获得内容的方式并不是一个中立、公正的过程。它本身并不民主，没有公平分配，也没有宪法保护。这些工具已经用一种我们没有发言权的专有算法来提升或埋没内容。***

***他们不会因为政治党派而受到审查。他们对我们的参与进行审查——以保持我们对这些产品的重视和联系，并为我们提供广告。我们的时序提要成为专有排序机制的那一天，就是这些平台不再中立的那一天。***

***这些工具对公共话语、言论自由和民主越重要，这些算法就越难以为我们所知。***

***这是最终的要点——通过接受这些工具的重要性，但不要求任何控制，我们放弃了决定我们作为一个社会的对话类型的能力。***

# ***最后三件事:***

1.  ****这些都是命中注定的***干政* ***。*T11】******
2.  *****他们本来就被设计成* ***无党派*** *。*****
3.  *****它们都可能导致短期* ***参与度和广告收入*** *的下降，但长期来看，感知、健康和幸福感可能会增加。*****

****这些都不是完美的解决方案。它们是可以更彻底探索的起点。这项工作的部分目标是推进基于可测试结果的关于这些工具的对话。考虑到这一点，当有更多的证据和研究可用时，我会不时地更新这篇文章。****

****我希望通过深思熟虑地挑选、测试和评论这些设计，它可能会引导我们走向一个更好的选择，最终取代我们共同居住的分裂、有毒和非常不健康的数字领域。****































