# 瓶装水:PostgreSQL 和 Kafka 的实时集成

> 原文:[http://blog . confluent . io/2015/04/23/瓶装水-实时整合-of-postgresql-and-kafka/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](http://blog.confluent.io/2015/04/23/bottled-water-real-time-integration-of-postgresql-and-kafka/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

*总结:[汇合](/)开始探索数据库与事件流的整合。作为探索的第一步， [Martin Kleppmann](http://martin.kleppmann.com/) 开发了一个新的开源工具叫做[瓶装水](https://github.com/confluentinc/bottledwater-pg)。它允许您将 PostgreSQL 数据库转换成结构化 Kafka 事件流。这对数据集成非常有用。*

写入数据库很容易，但是再次获取数据却异常困难。

当然，如果你只是想查询数据库，得到一些结果，那也可以。但是，如果您想在其他系统中获得数据库内容的副本，例如，使其在 Elasticsearch 中可搜索，或者预填充缓存以使其又好又快，或者将其加载到数据仓库中进行分析，或者如果您想迁移到不同的数据库技术，该怎么办呢？

如果你的数据从未改变，那就简单了。您可以拍摄数据库的快照(完全转储，例如备份)，复制它，并将其加载到另一个系统中。问题是数据库中的数据不断变化，所以当您加载快照时，它已经过时了。即使您每天拍摄一次快照，下游系统中仍然有一天前的数据，并且在大型数据库中，这些快照和批量加载可能会变得非常昂贵。不太好。

那么，如果您想在几个不同的系统中复制您的数据，该怎么做呢？

一种选择是让您的应用程序进行所谓的“双重写入”。也就是说，每次您的应用程序代码写入数据库时，它也会更新/失效相应的缓存条目，在您的搜索引擎中重新索引数据，将其发送到您的分析系统，等等:![Application-managed dual writes](../Images/016b679a3964f3d3f0fd7e68c8bbc9bd.png)然而，正如我在[我的一次演讲](http://martin.kleppmann.com/2014/10/28/staying-agile-at-span.html)中解释的那样，双重写入方法确实存在问题。它受到竞争条件和可靠性问题的困扰。如果将略有不同的数据写入两个不同的数据存储库(可能是由于错误或竞争情况)，数据存储库的内容将逐渐分离，随着时间的推移，它们将变得越来越不一致。从这种逐渐的数据损坏中恢复是困难的。

如果您从数据库的快照中重建缓存或索引，这样做的好处是，当您从新的数据库转储中重建时，任何不一致都会被[消除掉](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)。然而，在大型数据库中，一天一次(或更频繁地)处理整个数据库转储既慢又低效。我们怎样才能让它变得更快？

通常，在一个快照和下一个快照之间，只有一小部分数据库会发生变化。如果您只能处理自上次快照以来数据库中发生变化的“差异”,会怎么样？这也将是一个较小的数据量，所以你可以更频繁地采取这样的差异。如果你能每分钟都有这样的“差异”会怎么样？每秒钟？一秒钟 100 次？

当您将它发挥到极致时，对数据库的更改就变成了一个流。每当有人向数据库写入数据时，这就是数据流中的一条消息。如果您按照与原始数据库提交消息的顺序完全相同的顺序将这些消息应用到数据库，您最终会得到数据库的精确副本。仔细想想，这正是数据库复制的工作方式。

数据同步的复制方法比双重写入效果好得多。首先，将所有数据写入一个数据库(这可能是您已经在做的事情)。接下来，从数据库中提取两件事:

1.  一个时间点的**一致快照**，以及
2.  从该点开始的**实时变化流**。

您可以将快照加载到其他系统中(例如您的搜索索引或缓存)，然后持续应用实时更改。如果这个管道调得很好，您可能会得到不到一秒钟的延迟，因此您的下游系统几乎保持最新。由于更改流提供了写操作的顺序，因此竞争条件[不再是问题](https://martin.kleppmann.com/2015/04/24/logs-for-data-infrastructure-at-craft.html)。

这种构建系统的方法有时被称为[变更数据捕获](http://en.wikipedia.org/wiki/Change_data_capture) (CDC)，尽管目前这样做的工具还不是很好。然而，在一些公司，CDC 已经成为应用程序的关键组成部分——例如，LinkedIn 建立了[数据总线](http://www.socc2012.org/s18-das.pdf)，脸书为此建立了[虫洞](https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/)。

我对变更捕获感到兴奋，因为它允许您释放您已经拥有的数据中的价值。您可以将数据输入到一个[数据流](http://blog.confluent.io/2015/02/25/stream-data-platform-1/)中心，在这里，数据可以很容易地与事件流和来自其他数据库的数据实时结合。这种方法使得试验新的分析或数据格式更加容易，它允许以最小的风险从一个系统逐渐迁移到另一个系统，并且它对数据损坏更加健壮:如果出现问题，您总是可以从快照和流重建数据存储。

![Using change capture to drive derived data stores](../Images/2a238ecda0297d1e786b6b5b04b2a257.png)

**获取实时变更流**

获得数据库的一致快照是一个常见的特性，因为您需要它来进行备份。但是，获取实时的变化流一直是数据库的一个被忽视的特性。Oracle [GoldenGate](http://www.oracle.com/us/products/middleware/data-integration/goldengate/overview/index.html) 、 [MySQL binlog](https://dev.mysql.com/doc/refman/5.7/en/binary-log.html) 、 [MongoDB oplog](http://www.manuel-schoebel.com/blog/meteorjs-and-mongodb-replica-set-for-oplog-tailing) 或 [CouchDB changes feed](http://guide.couchdb.org/draft/notifications.html) 做了类似的事情，但它们并不容易正确使用。最近，一些数据库如 [RethinkDB](http://rethinkdb.com/blog/realtime-web/) 或 [Firebase](https://www.firebase.com/docs/web/guide/retrieving-data.html) 已经面向实时变化流。

不过，今天我们就来说说 **PostgreSQL** 。这是一个老派的数据库，但它很好。它很稳定，性能很好，而且是[出奇全功能](https://vimeo.com/61044807)。

直到最近，如果你想从 Postgres 中获得一系列的变化，你必须使用触发器。这是可能的(见下文)，但是很复杂，需要改变模式，并且执行得不是很好。然而，Postgres 9.4(2014 年 12 月发布)引入了一个改变一切的新特性:[逻辑解码](http://www.postgresql.org/docs/9.4/static/logicaldecoding.html)(下面我会更详细地解释)。

有了逻辑解码，Postgres 的变更数据捕获突然变得更有吸引力了。因此，当这个特性发布时，我着手为 Postgres 构建一个变更数据捕获工具，它将利用新的功能。[汇合](/)赞助我做的(谢谢汇合！)，今天我们将发布该工具的 alpha 版本作为开源。它叫做[瓶装水](https://github.com/confluentinc/bottledwater-pg)。![Bottled Water: Data streams freshly bottled at source](../Images/b4e5d519345ed2a0aacafa86d272fa94.png)T5】引进瓶装水

逻辑解码获取数据库的预写日志(WAL ),并允许我们访问行级的更改事件:每次插入、更新或删除表中的一行，都是一个事件。这些事件按事务分组，并按提交到数据库的顺序显示。中止/回滚的事务不会出现在流中。因此，如果您以相同的顺序应用更改事件，您最终会得到一个精确的、事务一致的数据库副本。

Postgres 逻辑解码设计得很好:它甚至创建了与变更流相协调的一致快照。您可以使用此快照制作整个数据库的时间点副本(无需锁定，您可以在制作副本的同时继续写入数据库)，然后使用更改流获取自快照以来发生的所有写入。

瓶装水使用这些功能来复制数据库中的所有数据，并将其编码为高效的二进制格式 [Avro 格式](http://avro.apache.org/)。编码后的数据被发送到 [Kafka](https://kafka.apache.org/) —数据库中的每一张表都成为 Kafka 的一个主题，数据库中的每一行都成为 Kafka 中的一条消息。

一旦数据在 Kafka 中，你可以很容易地编写一个 Kafka 消费者来做你需要的任何事情:将它发送到 Elasticsearch，或者填充一个缓存，或者在一个 [Samza](http://samza.apache.org/) 作业中处理它，或者用[加缪](/docs/current/camus/docs/intro.html)将它加载到 HDFS……可能性是无限的。

**为什么是卡夫卡？**

Kafka 是一个消息传递系统，以传输大量活动事件而闻名，如 web 服务器日志和用户点击事件。在卡夫卡那里，这样的事件通常会保留一段时间，然后被丢弃。Kafka 真的很适合数据库变更事件吗？我们不希望数据库数据被丢弃！

事实上，Kafka 是一个完美的组合——关键是 Kafka 的[日志压缩功能](https://kafka.apache.org/documentation.html#compaction)，它正是为此目的而设计的。如果启用日志压缩，则没有基于时间的数据过期时间。相反，每个消息都有一个密钥，Kafka 无限期地保留给定密钥的最新消息。给定键的早期消息最终会被垃圾收集。这非常类似于在键值存储中用新值覆盖旧值。

瓶装水识别 Postgres 中每个表的主键(或[副本标识](http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/)),并将其用作发送给 Kafka 的消息的密钥。消息的价值取决于事件的种类:

*   对于插入和更新，消息值包含行的所有字段，编码为 Avro。
*   对于删除，消息值设置为 null。这将导致 Kafka 在日志压缩期间删除消息，从而释放其磁盘空间。

使用日志压缩，您不需要一个系统存储整个数据库的快照，而另一个系统存储实时消息——它们可以在同一个系统中很好地共存。瓶装水通过将数据库中的每一行转换成一条消息，以主键为键，并将它们全部发送给 Kafka 经纪人，从而将初始快照写入 Kafka。当快照完成时，插入、更新或删除的每一行都类似地变成一条消息。

如果一行被频繁更新，将会有许多具有相同键的消息(因为每次更新都会变成一条消息)。幸运的是，Kafka 的日志压缩将解决这个问题，并对旧值进行垃圾收集，这样我们就不会浪费磁盘空间。另一方面，如果一行从未被更新或删除，它在 Kafka 中永远保持不变——它永远不会被垃圾收集。

在同一个系统中拥有完整的数据库转储和实时流是非常强大的。如果您想从头开始重建下游数据库，您可以从一个空数据库开始，从头开始使用 Kafka 主题，然后浏览整个主题，将每条消息写入您的数据库。当您到达末尾时，您就拥有了整个数据库的最新副本。更重要的是，您可以通过简单地继续使用流来保持它的最新状态。在您的数据上构建替代视图从未如此简单！

在 Kafka 中维护数据库副本的想法让更熟悉传统企业消息传递及其局限性的人感到惊讶。实际上，这个用例正是 Kafka 围绕一个[复制日志抽象](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)构建的原因:它使这种大规模数据保留和分发成为可能。下游系统可以随意重新加载和重新处理数据，而不会影响提供低延迟查询的上游数据库的性能。

**为什么是 Avro？**

从 Postgres 中提取的数据可以编码为 JSON、Protobuf、Thrift 或任何格式。但是，我相信 Avro 是最好的选择。Gwen Shapira 已经写了 Avro 在模式管理方面的[优势，我也有一篇](http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html)[博客文章](http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)将它与 Protobuf 和 Thrift 进行了比较。[合流流数据平台指南](http://blog.confluent.io/2015/02/25/stream-data-platform-2/)给出了 Avro 有利于数据集成的更多理由。

瓶装水检查数据库表的模式，并自动为每个表生成一个 Avro 模式。模式被自动注册到 [Confluent 的模式注册表](/docs/current/schema-registry/docs/index.html)，模式版本被嵌入到发送给 Kafka 的消息中。这意味着它与流数据平台的[序列化器](/docs/current/schema-registry/docs/serializer-formatter.html)一起“工作”:您可以将来自 Postgres 的数据作为有意义的应用程序对象和丰富的数据类型，而无需编写大量冗长的解析代码。

Postgres 数据类型到 Avro 的转换已经相当全面，涵盖了所有常见的数据类型，并提供了无损和合理的类型转换。我打算扩展它以支持 Postgres 的所有内置数据类型(有很多！)—这需要一些努力，但是值得，因为好的数据模式非常重要。![Inside the bottle factory](../Images/c1a6779f7d3dec4c01c6ef74db2b5b6d.png)

**逻辑解码输出插件**

Postgres 的逻辑解码特性的一个有趣的特性是，它没有定义通过网络将变更数据发送给消费者的网络格式。相反，它定义了一个[输出插件 API](http://www.postgresql.org/docs/9.4/static/logicaldecoding-output-plugin.html) ，它接收每次插入、更新或删除的函数调用。瓶装水使用这个 API 读取数据库内部格式的数据，并将其序列化为 Avro。

输出插件必须使用 Postgres 扩展机制用 C 编写，并作为共享库加载到数据库服务器中。这需要数据库服务器上的超级用户权限和文件系统访问权限，因此这不是一件轻而易举的事情。我理解许多数据库管理员会对在数据库服务器中运行定制代码的前景感到害怕。不幸的是，这是目前唯一可以使用的逻辑解码方式。

目前，必须在 leader 数据库上安装逻辑解码插件。原则上，可以让它运行在一个单独的 follower 上，这样它就不会影响其他客户端，但是目前 Postgres 中的实现不允许这样做。这一限制有望在 Postgres 的未来版本中被取消。![Bottled water client daemon](../Images/8759010954867b0f40972ee144436e7a.png)

**客户端守护进程**

除了插件(在数据库服务器内部运行)，瓶装水包含一个可以在任何地方运行的客户端程序。它连接到 Postgres 服务器和 Kafka 代理，从数据库接收 Avro 编码的数据，并将其转发给 Kafka。

客户端也是用 C 编写的，因为这样使用 Postgres 客户端库最容易，而且一些代码在插件和客户端之间共享。它相当轻量级，不需要写入磁盘。

如果客户端崩溃，或者与 Postgres 或 Kafka 断开连接，会发生什么？没问题。它会跟踪哪些消息已经被 Kafka 经纪人发布和确认。当客户端在出错后重新启动时，它会重放所有未被确认的消息。因此，一些信息可能会在 Kafka 中出现两次，但不会丢失任何数据。

**相关工作**

许多其他人也在研究类似的问题:

*   [Decoderbufs](https://github.com/xstevens/decoderbufs) 是由 [Xavier Stevens](https://twitter.com/xstevens) 开发的实验性 Postgres 插件，它将更改流解码为协议缓冲格式。它只提供了故事的逻辑解码插件部分——它没有一致的快照或客户端部分(Xavier 提到他已经编写了一个从 Postgres 读取并写入 Kafka 的客户端，但它不是开源的)。
*   pg_kafka (也来自 Xavier)是 Postgres 函数中的 kafka 生产者客户端，因此您可以从触发器向 Kafka 生产。
*   [PGQ](https://wiki.postgresql.org/wiki/PGQ_Tutorial) 是基于 Postgres 的队列实现， [Skytools Londiste](https://wiki.postgresql.org/wiki/SkyTools) (由 Skype 开发)使用它来提供基于触发器的复制。[布卡多](https://bucardo.org/wiki/Bucardo)是另一个基于触发的复制器。我得到的印象是，基于触发器的复制有点像黑客，需要模式更改和复杂的配置，并且会导致巨大的开销。此外，这些项目似乎都没有得到 PostgreSQL 核心团队的认可，而逻辑解码却得到了完全的支持。
*   最近，Sqoop 增加了对[写给卡夫卡](https://issues.apache.org/jira/browse/SQOOP-1852)的支持。据我所知，Sqoop 只能拍摄数据库的完整快照，而不能捕获正在进行的更改流。此外，我不确定其快照的事务一致性。
*   对于那些使用 MySQL 的人来说，James Cheng 整理了一份从 MySQL 获取数据到 Kafka 的[变更捕获项目](https://github.com/wushujames/mysql-cdc-projects/wiki)的列表。AFAIK，他们都专注于 binlog 解析部分，不做一致性快照部分。

**瓶装水的状况**

目前，瓶装水是阿尔法质量的软件。这不仅仅是一个概念验证——它的设计和实现相当谨慎——但它还没有在任何真实世界的场景中进行测试。它现在肯定还没有准备好投入生产，但是经过一些测试和调整，它有望在未来投入生产。

我们现在以开源的方式发布它，希望能得到社区的反馈。另外，一些听说我正在做这个的人一直烦着我发布它🙂

[自述文件](https://github.com/confluentinc/bottledwater-pg/blob/master/README.md)有更多关于如何开始的信息。请让我们知道你进展如何！此外，我将在六月的[柏林流行语](http://berlinbuzzwords.de/session/change-data-capture-magic-wand-we-forgot)上谈论更多关于瓶装水的话题——希望能在那里见到你。