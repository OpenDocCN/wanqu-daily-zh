# 机器学习的规则:|谷歌开发者

> 原文：<https://developers.google.com/machine-learning/rules-of-ml/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

# ML 工程的最佳实践

马丁·津克维奇

本文档旨在帮助那些具有机器学习基础知识的人从谷歌的机器学习最佳实践中受益。它为机器学习提供了一种风格，类似于 Google C++风格指南和其他流行的实用编程指南。如果你参加过机器学习的课程，或者建立或研究过机器学习模型，那么你就有阅读本文的必要背景。

*Martin Zinkevich introduces 10 of his favorite rules of machine learning. Read on to learn all 43 rules!*

## 术语

在我们讨论有效的机器学习时，以下术语会反复出现:

*   **实例**:你要预测的事物。例如，实例可能是您想要分类为“关于猫”或“不关于猫”的网页。
*   **标签**:预测任务的答案，可以是机器学习系统产生的答案，也可以是训练数据中提供的正确答案。例如，网页的标签可能是“关于猫”。
*   **特征**:预测任务中使用的实例的属性。例如，网页可能具有“包含单词‘猫’”的特征。
*   **特征列**:一组相关的特征，例如用户可能居住的所有可能国家的集合。一个示例可能在特征列中有一个或多个特征。“功能栏”是谷歌特有的术语。一个特性列在大众系统中被称为“名称空间”(在雅虎/微软)，或者一个[字段](https://www.csie.ntu.edu.tw/%7Ecjlin/libffm/)。
*   **示例**:一个实例(及其特性)和一个标签。
*   **模型**:预测任务的统计表示。您根据示例训练模型，然后使用该模型进行预测。
*   **度量**:你关心的一个数字。可以直接优化也可以不直接优化。
*   **目标**:您的算法试图优化的指标。
*   **流水线**:围绕机器学习算法的基础设施。包括从前端收集数据，将其放入训练数据文件中，训练一个或多个模型，以及将模型导出到生产中。
*   点击通过率点击广告链接的网页访问者的百分比。

## 概观

要制造优秀的产品:

像你是伟大的工程师一样进行机器学习，而不是像你不是伟大的机器学习专家一样。

事实上，你将面临的大多数问题都是工程问题。即使拥有一个伟大的机器学习专家的所有资源，大部分收获还是来自伟大的功能，而不是伟大的机器学习算法。所以，基本的方法是:

1.  确保你的管道端到端是稳固的。
2.  从一个合理的目标开始。
3.  以简单的方式添加常识性的功能。
4.  确保你的管道保持稳固。

这种方法将长期有效。只有当没有更简单的技巧可以让你走得更远时，才偏离这种方法。增加复杂性会降低未来版本的速度。

一旦你用尽了简单的技巧，尖端的机器学习可能真的就在你的未来。参见[第三阶段](#ml_phase_iii_slowed_growth_optimization_refinement_and_complex_models)机器学习项目部分。

本文件安排如下:

1.  [第一部分](#before_machine_learning)应该可以帮助你理解构建机器学习系统的时机是否成熟。
2.  第二部分是关于部署你的第一个管道。
3.  [第三部分](#ml_phase_ii_feature_engineering)是关于向你的管道添加新功能时的启动和迭代，如何评估模型和训练服务偏差。
4.  最后一部分是关于当你到达平台期时该做什么。
5.  之后，有一个与[相关的工作](#related_work)的列表和一个[附录](#appendix)，其中有一些关于本文档中常用系统的背景示例。

## 在机器学习之前

#### 规则 1:不要害怕推出没有机器学习的产品。

机器学习很酷，但是需要数据。理论上，你可以从一个不同的问题中获取数据，然后为一个新产品调整模型，但这可能会不如基本的 [**启发式**](/machine-learning/glossary#heuristic) 。如果你认为机器学习会给你 100%的提升，那么启发式会给你 50%的提升。

例如，如果你在一个应用程序市场中对应用程序进行排名，你可以使用安装率或安装数量作为启发。如果您正在检测垃圾邮件，请过滤掉之前发送过垃圾邮件的发布者。也不要害怕使用人工编辑。如果你需要给联系人排名，把最近使用的排到最高(或者甚至按字母顺序排列)。如果机器学习不是你的产品绝对需要的，那么在你有数据之前不要使用它。

#### 规则#2:首先，设计并实现度量标准。

在正式确定你的机器学习系统将做什么之前，尽可能多地跟踪你当前的系统。这样做的原因如下:

1.  更容易在早期获得系统用户的许可。
2.  如果你认为某件事将来可能会引起关注，最好现在就获取历史数据。
3.  如果您在设计系统时考虑了公制仪器，那么将来事情会变得更好。具体来说，您不希望在日志中寻找字符串来测量您的指标！
4.  你会注意到什么东西变了，什么东西没变。例如，假设您想直接优化一天的活跃用户。然而，在您对系统的早期操作中，您可能会注意到用户体验的巨大变化并没有显著地改变这个指标。

[Google Plus](#google_plus_overview) 团队测量每次阅读的扩展、每次阅读的重新分享、每次阅读的增加、评论/阅读、每个用户的评论、每个用户的重新分享等。他们用它来计算一个职位在任职期间的表现。**此外，请注意，实验框架非常重要，在这个框架中，您可以将用户分组到不同的组中，并通过实验汇总统计数据。**参见[规则#12](#rule_12_don_t_overthink_which_objective_you_choose_to_directly_optimize) 。

通过更自由地收集度量，您可以获得系统的更广泛的图像。注意到问题了吗？添加一个指标来跟踪它！对上一个版本的一些数量变化感到兴奋吗？添加一个指标来跟踪它！

#### 规则 3:选择机器学习而不是复杂的启发式方法。

一个简单的启发可以让你的产品走出大门。复杂的启发式算法是不可维护的。一旦你有了数据和你想要完成的事情的基本想法，继续机器学习。正如在大多数软件工程任务中一样，你会希望不断更新你的方法，无论是启发式的还是机器学习的模型，你会发现机器学习的模型更容易更新和维护(见[规则#16](#rule_16_plan_to_launch_and_iterate) )。

## ML 第一阶段:您的第一条管道

关注第一条管道的系统基础设施。虽然想想你将要做的所有富有想象力的机器学习很有趣，但如果你不首先信任你的管道，就很难弄清楚会发生什么。

#### 规则#4:保持第一个模型简单，并获得正确的基础设施。

第一个模型为你的产品提供了最大的推动力，所以它不需要太花哨。但是您将会遇到比预期更多的基础设施问题。在任何人可以使用你的新机器学习系统之前，你必须确定:

*   如何给你的学习算法举例？
*   首先说明“好”和“坏”对你的系统意味着什么。
*   如何将您的模型集成到您的应用程序中。您可以实时应用模型，也可以在离线时根据示例预先计算模型，并将结果存储在一个表中。例如，您可能希望对网页进行预分类并将结果存储在一个表中，但是您可能希望对实时聊天消息进行分类。

选择简单特征可以更容易地确保:

*   特征正确地到达你的学习算法。
*   模型学习合理的权重。
*   特征正确地到达服务器中的模型。

一旦你有了一个可靠地做这三件事的系统，你就完成了大部分工作。您的简单模型为您提供了基线度量和基线行为，您可以使用它们来测试更复杂的模型。一些团队的目标是“中立”的首次发布:首次发布明确地取消了机器学习收益的优先级，以避免分心。

#### 规则 5:独立于机器学习测试基础设施。

确保基础设施是可测试的，并且系统的学习部分是封装的，以便您可以测试它周围的一切。具体来说:

1.  测试将数据输入算法。检查应填充的要素列是否已填充。在隐私允许的情况下，手动检查训练算法的输入。如果可能，将管道中的统计数据与其他地方处理的相同数据的统计数据进行比较。
2.  测试从训练算法中获取模型。确保您培训环境中的模型与您服务环境中的模型给出相同的分数(参见[规则#37](#rule_37_measure_training_serving_skew) )。

机器学习有一个不可预测的元素，所以请确保您对用于在培训和服务中创建示例的代码进行了测试，并且您可以在服务期间加载和使用固定的模型。此外，理解您的数据也很重要:参见[大型复杂数据集分析的实用建议](http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html)。

#### 规则 6:复制管道时，要小心丢失的数据。

通常，我们通过复制现有管道(即 [cargo cult programming](https://wikipedia.org/wiki/Cargo_cult_programming) )来创建管道，旧管道会丢弃新管道所需的数据。例如， [Google Plus](#google_plus_overview) 的管道会删除旧帖子(因为它试图对新帖子进行排名)。这条管道被复制用于 [Google Plus](#google_plus_overview) 流，旧的帖子仍然有意义，但是管道仍然在丢弃旧的帖子。另一种常见的模式是只记录用户看到的数据。因此，如果我们想要模拟为什么一个特定的帖子没有被用户看到，这些数据是没有用的，因为所有的负面例子都已经被删除了。类似的问题也发生在 Play 上。在开发 Play Apps Home 时，创建了一个新的管道，其中也包含来自 Play Games 登录页面的示例，但没有任何功能来区分每个示例的来源。

#### 规则 7:把启发式变成特性，或者在外部处理它们。

通常机器学习试图解决的问题并不完全是新的。有一个现存的系统来进行排名，或者分类，或者你试图解决的任何问题。这意味着有一堆规则和启发。当与机器学习相结合时，这些同样的启发法可以给你一个提升。出于两个原因，无论你的启发中有什么信息，都应该挖掘出来。首先，向机器学习系统的过渡将更加平稳。第二，通常这些规则包含了很多你不想丢掉的关于系统的直觉。有四种方法可以使用现有的启发式算法:

*   使用试探法进行预处理。如果该功能非常棒，那么这是一个选项。例如，如果在垃圾邮件过滤器中，发件人已经被列入黑名单，不要试图重新学习“列入黑名单”是什么意思。阻止消息。这种方法在二元分类任务中最有意义。
*   创建一个特征。直接从试探法创建一个特征是很棒的。例如，如果使用试探法来计算查询结果的相关性分数，则可以将该分数作为特性的值包括在内。稍后，您可能希望使用机器学习技术来处理该值(例如，将该值转换为有限的一组离散值中的一个，或者将它与其他特征相结合)，但首先要使用启发式方法产生的原始值。
*   挖掘启发式的原始输入。如果有一个结合了安装数量、文本中的字符数量和星期几的启发式应用程序，那么考虑将这些部分分开，并将这些输入分别输入到学习中。一些适用于合奏的技巧在这里也适用(见[规则#40](#rule_40_keep_ensembles_simple) )。
*   修改标签。当您认为启发式搜索捕获了标签中当前不包含的信息时，这是一个选项。例如，如果你试图最大化下载次数，但你也想要高质量的内容，那么也许解决方案是将标签乘以应用程序获得的平均星级数。这里有很大的回旋余地。参见[“你的第一个目标”](#your_first_objective)。

在 ML 系统中使用试探法时，一定要注意增加的复杂性。在你的新机器学习算法中使用旧的启发式方法可以帮助创建一个平滑的过渡，但想想是否有更简单的方法来实现同样的效果。

### 监视

一般来说，实践良好的警报卫生，例如使警报可操作和有一个仪表板页面。

#### 规则 8:了解系统的新鲜度要求。

如果您的模型使用了一天，性能会下降多少？一周大？四分之一岁？这些信息可以帮助您了解监控的优先级。如果模型一天不更新，你的产品质量会下降，那么让工程师持续关注它是有意义的。大多数广告服务系统每天都有新的广告要处理，并且必须每天更新。例如，如果 Google Play 搜索的 ML 模型不更新，不到一个月就会产生负面影响。在 [Google Plus](#google_plus_overview) 的一些热门模型中没有帖子标识符，所以他们可以偶尔导出这些模型。其他具有 post 标识符的模型更新得更频繁。另请注意，新鲜度会随时间而变化，尤其是在模型中添加或删除要素列时。

#### 规则 9:在导出模型之前检测问题。

很多机器学习系统都有一个阶段，你把模型导出来服务。如果导出的模型有问题，这是一个面向用户的问题。

在导出模型之前进行健全性检查。具体来说，要确保模型的表现在数据上是合理的。或者，如果您对数据有挥之不去的担忧，不要导出模型。很多持续部署模型的团队在导出之前会检查 [ROC 曲线](https://wikipedia.org/wiki/Receiver_operating_characteristic)(或 AUC)下的面积。**关于尚未导出的模型的问题需要一个电子邮件提醒，但是关于面向用户的模型的问题可能需要一个页面。**因此，最好在影响用户之前等待并确定。

#### 规则 10:留意无声的失败。

这是机器学习系统比其他类型的系统更容易出现的问题。假设一个被连接的特定表不再被更新。机器学习系统将进行调整，行为将继续保持合理的良好状态，逐渐衰退。有时您会发现表已经过期几个月了，一次简单的刷新比该季度的任何其他发布都更能提高性能！一个特性的覆盖范围可能由于实现的变化而变化:例如，一个特性列可能在 90%的例子中被填充，而突然下降到 60%的例子中。Play 曾经有一个表过期了 6 个月，仅刷新该表就提高了 2%的安装率。如果您跟踪数据的统计信息，并偶尔手动检查数据，就可以减少这类故障。

#### 规则 11:给专栏作者和文档。

如果系统很大，并且有许多特性列，那么要知道是谁创建或维护了每个特性列。如果你发现了解一个专栏的人要离开，确保有人知道这个信息。尽管许多特性专栏都有描述性的名称，但是最好有一个更详细的描述，说明特性是什么，它来自哪里，以及它将如何提供帮助。

### 你的第一个目标

你有许多关于你关心的系统的度量或测量，但你的机器学习算法通常需要一个单一的**目标，一个你的算法正在“试图”优化的数字。**我在这里区分了目标和指标:指标是您的系统报告的任何数字，可能重要，也可能不重要。参见[规则 2](#rule_2_first_design_and_implement_metrics) 。

#### 规则#12:不要过多考虑你选择直接优化的目标。

你想赚钱，让你的用户开心，让世界变得更美好。有很多你关心的指标，你应该衡量它们(见[规则#2](#rule_2_first_design_and_implement_metrics) )。然而，在机器学习过程的早期，你会注意到它们都在上升，甚至那些你没有直接优化的。例如，假设您关心网站的点击次数和花费的时间。如果你对点击次数进行优化，你可能会看到花费的时间增加。

因此，保持简单，当您仍然可以轻松增加所有指标时，不要过多考虑平衡不同的指标。但是不要把这条规则看得太远:不要把你的目标和系统的最终健康混淆起来(见[规则#39](#rule_39_launch_decisions_are_a_proxy_for_long_term_product_goals) )。并且，**如果你发现自己增加了直接优化的指标，但是决定不启动，可能需要一些客观的修正。**

#### 规则 13:为你的第一个目标选择一个简单的、可观察的和可归因的度量标准。

通常你不知道真正的目标是什么。你认为你可以，但是当你盯着数据和新旧 ML 系统的对比分析时，你意识到你想要调整目标。此外，不同的团队成员经常不能就真正的目标达成一致。ML 目标应该是易于衡量的，是“真实”目标的代表。事实上，通常没有“真正的”目标(见[规则 39](#rule_39_launch_decisions_are_a_proxy_for_long_term_product_goals) )。因此，训练简单的 ML 目标，并考虑在顶部有一个“策略层”，允许您添加额外的逻辑(希望非常简单的逻辑)来进行最终排名。

最容易建模的是直接观察到的用户行为，它可归因于系统的动作:

*   这个排名链接被点击了吗？
*   这个排名对象被下载了吗？
*   此排名对象是否被转发/回复/通过电子邮件发送？
*   是否对该排名对象进行了评级？
*   此展示对象是否被标记为垃圾邮件/色情/攻击性内容？

首先避免模拟间接影响:

*   用户第二天访问了吗？
*   用户访问网站多长时间？
*   每日活跃用户有多少？

间接影响是很好的衡量标准，可以在 A/B 测试和发布决策中使用。

最后，不要试图让机器学习弄明白:

*   用户使用产品开心吗？
*   用户对体验满意吗？
*   产品是否提高了用户的整体福利？
*   这将如何影响公司的整体健康？

这些都很重要，但也难以衡量。相反，使用代理:如果用户满意，他们会在网站上停留更长时间。如果用户满意，他们明天会再次访问。就福祉和公司健康而言，人类的判断需要将任何机器学习的目标与你正在销售的产品的性质和你的商业计划联系起来。

#### 规则#14:从一个可解释的模型开始使调试更容易。

线性回归、逻辑回归和泊松回归直接由概率模型驱动。每个预测都可以解释为概率或期望值。这使得它们比使用目标(零-1 损失、各种铰链损失等)的模型更容易调试，这些目标试图直接优化分类精度或排名性能。例如，如果训练中的概率偏离了并排预测或通过检查生产系统预测的概率，这种偏离就可能揭示出问题。

例如，在线性、逻辑或泊松回归中，**存在数据的子集，其中平均预测期望等于平均标签(1-矩校准，或刚刚校准)**。假设你没有正则化，并且你的算法已经收敛，这是正确的，一般来说这是正确的。如果每个示例都有一个为 1 或 0 的特征，则校准该特征为 1 的 3 个示例的集合。此外，如果每个示例都有一个为 1 的特征，那么所有示例的集合都将被校准。

对于简单的模型，更容易处理反馈循环(见[规则#36](#rule_36_avoid_feedback_loops_with_positional_features) )。通常，我们使用这些概率预测来做出决定:例如，按照降低的期望值(即，点击/下载/等的概率)来排列帖子。).**然而，记住当选择使用哪种模型时，决策比给定模型的数据的可能性更重要(见[规则#27](#rule_27_try_to_quantify_observed_undesirable_behavior) )。**

#### 规则#15:在策略层分离垃圾邮件过滤和质量排名。

质量排名是一门艺术，但垃圾邮件过滤是一场战争。你用来决定高质量帖子的信号对那些使用你的系统的人来说将变得显而易见，他们将调整他们的帖子来拥有这些属性。因此，你的质量排名应该把重点放在善意发布的内容上。你不应该因为垃圾邮件排名高而轻视质量排名学习者。类似地,“色情”内容应该与质量排名分开处理。垃圾邮件过滤是另一回事。你必须预料到你需要生成的特征会不断变化。通常，你会把一些明显的规则放入系统中(如果一个帖子有三个以上的垃圾投票，就不要检索它，等等)。任何学习到的模型都必须每天更新，如果不是更快的话。内容创作者的口碑会起到很大的作用。

在某种程度上，这两个系统的输出必须整合。请记住，在搜索结果中过滤垃圾邮件可能比在电子邮件中过滤垃圾邮件更积极。这是真的，假设你没有正则化，你的算法已经收敛。这大体上是正确的。此外，从质量分类器的训练数据中移除垃圾邮件也是一种标准做法。

## ML 第二阶段:特征工程

在机器学习系统生命周期的第一阶段，重要的问题是将训练数据放入学习系统，获取任何感兴趣的指标，并创建服务基础设施。当你有了一个端到端的工作系统，并进行单元和系统测试之后，第二阶段就开始了。

在第二阶段，有很多低挂的水果。有各种明显的特征可以被引入到系统中。因此，机器学习的第二阶段包括引入尽可能多的特征，并以直观的方式组合它们。在此阶段，所有指标都应该还在上升。将会有大量的发布，这是一个吸引大量工程师的好时机，他们可以将你需要的所有数据结合起来，创建一个真正令人敬畏的学习系统。

#### 规则#16:计划启动和迭代。

不要期望你现在正在开发的模型会是你将要发布的最后一个模型，甚至不要期望你会停止发布模型。因此，请考虑您在此次发布中增加的复杂性是否会减缓未来的发布。多年来，许多团队每个季度都会推出一个模型。推出新车型有三个基本原因:

*   您正在开发新功能。
*   您正在调整正则化，并以新的方式组合旧的功能。
*   你在调整目标。

无论如何，给一个模型一点爱可能是好的:查看输入到例子中的数据可以帮助找到新的信号以及旧的、坏的信号。因此，在构建模型时，想想添加、移除或重组特征有多容易。想想创建管道的新副本并验证其正确性是多么容易。想想有没有可能有两个或三个副本并行运行。最后，不要担心 35 个特性中的第 16 个是否会进入这个版本的管道。下个季度你会拿到的。

#### 规则 17:从直接观察和报告的特征开始，而不是从习得的特征开始。

这可能是一个有争议的点，但它避免了很多陷阱。首先，我们来描述一下什么是习得特征。已学习的特征是由外部系统(例如无监督的聚类系统)或由学习者自身(例如，通过分解模型或深度学习)生成的特征。这两种方法都很有用，但是会有很多问题，所以不应该放在第一个模型中。

如果使用外部系统创建特征，请记住外部系统有自己的目标。外部系统的目标可能与你当前的目标只有微弱的关联。如果您获取外部系统的快照，那么它可能会过时。如果您从外部系统更新特征，那么含义可能会改变。如果您使用外部系统来提供一个特性，请注意这种方法需要非常小心。

分解模型和深度模型的主要问题是它们是非凸的。因此，不能保证可以近似或找到最优解，并且在每次迭代中找到的局部最小值可能不同。这种变化使得很难判断一个变化对系统的影响是有意义的还是随机的。通过创建一个没有深度特征的模型，你可以得到一个优秀的基线性能。达到这个基线后，你可以尝试更深奥的方法。

#### 规则 18:探索内容的特性，这些特性可以在不同的上下文中推广。

通常，机器学习系统是一个更大的图景的一小部分。例如，如果你想象一个帖子可能会出现在《热门话题》中，那么在这个帖子出现在《热门话题》之前，很多人会加一个，重新分享或者评论这个帖子。如果你向学习者提供这些统计数据，它可以推广那些在它优化的上下文中没有数据的新帖子。 [YouTube](#youtube_overview) Watch Next 可以使用来自 [YouTube](#youtube_overview) 搜索的观看次数或共同观看次数(一个视频被观看后另一个视频被观看的次数)。您也可以使用明确的用户评级。最后，如果您有一个用户操作用作标签，那么在不同的上下文中查看文档上的操作可能是一个很好的特性。所有这些特性都允许您在上下文中引入新内容。注意，这不是关于个性化:先弄清楚某人是否喜欢这个上下文中的内容，然后弄清楚谁更喜欢或更不喜欢它。

#### 规则 19:尽可能使用非常具体的功能。

在大量数据的情况下，学习数百万个简单特征比学习几个复杂特征更简单。被检索的文档的标识符和规范化的查询不能提供太多的概括，但是可以根据标题查询上的标签调整您的排名。因此，不要害怕一组特性，其中每个特性只适用于数据的很小一部分，但总体覆盖率超过 90%。您可以使用正则化来消除应用于太少示例的特性。

#### 规则 20:组合和修改现有的特征，以人类可以理解的方式创造新的特征。

有多种方法可以组合和修改特征。TensorFlow 等机器学习系统允许你通过[变换](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)对数据进行预处理。两种最标准的方法是“离散化”和“交叉”。

离散化包括获取一个连续特征并从中创建许多离散特征。考虑一个连续的特征，例如年龄。当年龄小于 18 岁时，您可以创建一个为 1 的要素，当年龄在 18 到 35 岁之间时，您可以创建另一个为 1 的要素，依此类推。不要过多考虑这些直方图的界限:基本分位数会给你带来最大的影响。

十字组合两个或多个特征列。在 TensorFlow 的术语中，特征列是一组同质特征，(例如{男性、女性}、{美国、加拿大、墨西哥}等)。十字是一个新的特征栏，例如，{男性，女性}×{美国，加拿大，墨西哥}中的特征。这个新的特性列将包含特性(男性，加拿大)。如果您正在使用 TensorFlow，并且您告诉 TensorFlow 为您创建此十字，此(男性，加拿大)特征将出现在代表男性加拿大人的示例中。注意，学习具有三个、四个或更多基本特征列的交叉的模型需要大量数据。

生成非常大的特征列的十字可能会过度拟合。例如，假设您正在进行某种搜索，在查询中有一个包含单词的特征列，在文档中有一个包含单词的特征列。你可以用一个十字来组合这些，但是你最终会得到很多特性(见[规则#21](#rule_21_the_number_of_feature_weights_you_can_learn_in_a_linear_model_is_roughly_proportional_to_the_amount_of_data_you_have) )。

处理文本时，有两种选择。最严厉的是点产品。点积最简单的形式就是计算查询和文档之间共有的字数。然后可以将该特征离散化。另一种方法是交集:因此，当且仅当单词“pony”同时出现在文档和查询中时，我们将拥有一个特征；当且仅当单词“the”同时出现在文档和查询中时，我们将拥有另一个特征。

#### 规则 21:在线性模型中可以学习的特征权重的数量大致与您拥有的数据量成比例。

关于模型的适当复杂程度，有一些有趣的统计学习理论结果，但是你只需要知道这条规则。在我的谈话中，人们怀疑从一千个例子中能学到什么，或者你需要一百万个以上的例子，因为他们陷入了某种学习方法。关键是根据数据的规模来扩展您的学习:

1.  如果您正在开发一个搜索排名系统，文档和查询中有数百万个不同的单词，并且您有 1000 个带标签的示例，那么您应该在文档和查询特征之间使用点积， [TF-IDF](https://wikipedia.org/wiki/Tf%E2%80%93idf) ，以及半打其他高度人性化的特征。1000 个例子，一打特征。
2.  如果您有一百万个示例，那么使用正则化和可能的特征选择使文档和查询特征列相交。这会给你带来数百万的特性，但是随着正规化，你会拥有更少的特性。一千万个例子，也许十万个特征。
3.  如果您有数十亿或数千亿个示例，您可以使用特征选择和正则化，将特征列与文档和查询标记交叉。你会有十亿个例子和一千万个特征。统计学习理论很少给出严格的界限，但是对于一个起点给出了很大的指导。

最后，使用[规则#28](#rule_28_be_aware_that_identical_short_term_behavior_does_not_imply_identical_long_term_behavior) 来决定使用什么特性。

#### 规则#22:清理你不再使用的功能。

未使用的功能会产生技术债务。如果您发现您没有使用某个功能，并且将它与其他功能结合起来也不起作用，那么就将它从您的基础架构中删除。您希望保持基础设施的整洁，以便尽可能快地尝试最有前途的功能。如果有必要，有人可以随时添加回你的功能。

在考虑添加或保留什么功能时，请记住覆盖率。该功能涵盖了多少个示例？例如，如果你有一些个性化功能，但只有 8%的用户有个性化功能，这就不会很有效。

同时，一些功能可能会超出其重量。例如，如果你有一个特性，它只覆盖了 1%的数据，但是 90%的例子都是正面的，那么这将是一个很好的特性。

### 系统的人为分析

在进入机器学习的第三阶段之前，重要的是要关注任何机器学习课程都没有教授的东西:如何看待现有的模型，并对其进行改进。这与其说是一门科学，不如说是一门艺术，然而它有助于避免一些反模式。

#### 规则 23:你不是典型的最终用户。

这可能是团队陷入困境的最简单的方式。虽然鱼目混珠(在你的团队中使用原型)和狗目混珠(在你的公司中使用原型)有很多好处，但员工应该看看表现是否正确。虽然不应该使用明显不好的变化，但任何看起来合理的接近生产的东西都应该进一步测试，要么通过在众包平台上付钱给外行人来回答问题，要么通过在真实用户身上进行现场实验。

这有两个原因。首先是你离代码太近了。你可能在寻找帖子的某个特定方面，或者你只是过于情绪化(例如确认偏见)。第二是你的时间太宝贵了。想想九个工程师参加一个小时的会议的成本，想想有多少签约的人类标签在众包平台上购买。

如果你真的想要用户反馈，**使用用户体验方法论**。在流程的早期创建用户角色(比尔·巴克斯顿的[的《勾画用户体验](https://play.google.com/store/books/details/Bill_Buxton_Sketching_User_Experiences_Getting_the?id=2vfPxocmLh0C)》中有一个描述)，然后进行可用性测试(史蒂夫·克鲁格的[的《不要让我思考](https://play.google.com/store/books/details/Steve_Krug_Don_t_Make_Me_Think_Revisited?id=QlduAgAAQBAJ)》中有一个描述)。用户角色包括创建一个假想的用户。例如，如果您的团队都是男性，那么设计一个 35 岁的女性用户角色(包括用户特征)并查看它所产生的结果，而不是 25-40 岁男性的 10 个结果，可能会有所帮助。在可用性测试中，让真实的人来观察他们对你的网站的反应(本地或远程)也能让你有一个新的视角。

#### 规则 24:测量模型之间的差异。

在任何用户看到你的新模型之前，你能做的最简单的，有时也是最有用的测量之一就是计算出新的结果与产品有多大的不同。例如，如果您有一个排名问题，在整个系统的查询样本上运行这两个模型，并查看结果的对称差的大小(按排名位置加权)。如果差别很小，那么你不用运行实验就能知道变化很小。如果差异非常大，那么你要确保改变是好的。查看对称差异高的查询可以帮助您定性地理解变化是什么样的。但是，要确保系统是稳定的。确保一个模型在与自身比较时具有较低的(理想情况下为零)对称差异。

#### 规则 25:在选择模型时，实用性能胜过预测能力。

你的模型可能试图预测点击率。然而，最终，关键问题是你如何处理这个预测。如果您使用它来对文档进行排名，那么最终排名的质量比预测本身更重要。如果您预测了某个文档是垃圾邮件的概率，然后确定了被阻止的内容，那么允许通过的内容的精确度就更重要了。大多数时候，这两件事应该是一致的:当他们不一致时，可能会有小的收获。因此，如果有一些改进了日志丢失但降低了系统性能的变化，请寻找另一个特性。当这种情况发生得越来越频繁时，是时候重新审视你的模型的目标了。

#### 规则 26:在测量的误差中寻找模式，创造新的特征。

假设您看到一个模型“出错”的训练示例。在分类任务中，这种错误可能是假阳性或假阴性。在排序任务中，错误可能是一对，其中肯定的排序低于否定的。最重要的一点是，这是一个机器学习系统知道它出错了并愿意在有机会的情况下修复的例子。如果你给模型一个允许它修复错误的特性，模型会尝试使用它。

另一方面，如果你试图根据系统不认为是错误的例子创建一个特征，该特征将被忽略。例如，假设在 Play Apps 搜索中，有人搜索“免费游戏”。假设一个排名靠前的结果是一个不太相关的 gag 应用。所以你创建了一个“插科打诨应用”的功能。然而，如果你最大化安装数量，人们在搜索免费游戏时安装 gag 应用程序,“gag 应用程序”功能不会有你想要的效果。

一旦有了模型出错的例子，就要寻找当前特性集之外的趋势。例如，如果系统似乎在降级较长的帖子，则添加帖子长度。不要对你添加的功能过于具体。如果你打算增加文章的长度，不要试图猜测长是什么意思，只需添加十几个特征，然后让模型知道如何处理它们(参见[规则#21](#rule_21_the_number_of_feature_weights_you_can_learn_in_a_linear_model_is_roughly_proportional_to_the_amount_of_data_you_have) )。这是得到你想要的东西的最简单的方法。

#### 尝试量化观察到的不良行为。

你的团队中的一些成员将开始对他们不喜欢的系统属性感到沮丧，这些属性没有被现有的损失函数捕获。在这一点上，他们应该尽一切努力将他们的抱怨转化为实实在在的数字。例如，如果他们认为在 Play Search 中显示了太多的“gag 应用程序”，他们可以让人类评分员识别 gag 应用程序。(在这种情况下，您可以可行地使用人工标记的数据，因为相对小部分的查询占了大部分的流量。)如果你的问题是可度量的，那么你可以开始把它们作为特性、目标或度量标准来使用。总的规则是“**先度量，再优化**”。

#### 规则 28:要知道相同的短期行为并不意味着相同的长期行为。

假设您有一个新系统，它查看每个 doc_id 和 exact_query，然后为每个查询计算每个文档的点击概率。您发现它的行为在并排和 A/B 测试中与您当前的系统几乎相同，因此考虑到它的简单性，您启动它。但是，您会注意到没有显示新的应用程序。为什么？因为您的系统只显示了一个基于它自己的历史的文档，所以没有办法知道应该显示一个新的文档。

了解这样一个系统如何长期工作的唯一方法是，只根据模型运行时获得的数据对其进行训练。这个很难。

### 训练发球偏斜

训练-发球偏斜是训练期间的表现和发球期间的表现之间的差异。这种偏斜可能是由以下原因造成的:

*   培训管道和服务管道中数据处理方式的差异。
*   训练和服役期间的数据变化。
*   你的模型和算法之间的反馈回路。

我们已经观察到谷歌的生产机器学习系统具有对性能产生负面影响的训练服务偏斜。最好的解决方案是明确地对其进行监控，这样系统和数据的变化就不会引入未被注意到的偏差。

#### 规则 29:确保你像发球一样训练的最好方法是保存发球时使用的一组特征，然后将这些特征传输到日志中，以便在训练时使用。

即使你不能对每个例子都这样做，也要对一小部分这样做，这样你就可以验证发球和训练之间的一致性(见 [Rule #37](#rule_37_measure_training_serving_skew) )。在谷歌进行这种测量的团队有时会对结果感到惊讶。 [YouTube](#youtube_overview) 主页在服务时间切换到日志功能，质量显著提高，代码复杂度降低，许多团队正在切换他们的基础设施。

#### 规则 30:重要性-加权采样数据，不要随意丢弃它！

当你有太多的数据时，有一种诱惑，即取文件 1-12，而忽略文件 13-99。这是一个错误。尽管从未向用户显示的数据可能会被删除，但重要性加权最适用于其他数据。重要性加权意味着，如果你决定以 30%的概率对示例 X 进行采样，那么给它 10/3 的权重。**通过重要性加权，[规则#14](#rule_14_starting_with_an_interpretable_model_makes_debugging_easier) 中讨论的所有校准特性仍然有效。**

#### 规则 31:注意，如果你在培训和服务时间连接一个表中的数据，表中的数据可能会改变。

假设您将文档 id 与一个包含这些文档的特性(比如评论数或点击数)的表连接起来。在训练和服务时间之间，表中的特征可能会改变。您的模型对同一文档的预测在培训和服务之间可能会有所不同。避免这类问题的最简单方法是在上菜时记录特征(见[规则#32](#rule_32_re_use_code_between_your_training_pipeline_and_your_serving_pipeline_whenever_possible) )。如果表变化很慢，您还可以每小时或每天对表进行快照，以获得合理的接近数据。请注意，这仍然不能完全解决问题。

#### 规则 32:尽可能在你的培训管道和服务管道之间重用代码。

批处理不同于在线处理。在联机处理中，您必须在每个请求到达时对其进行处理(例如，您必须对每个查询进行单独的查找)，而在批处理中，您可以组合任务(例如，进行连接)。在服务时间，您正在进行在线处理，而培训是一个批处理任务。但是，您可以做一些事情来重用代码。例如，您可以创建一个特定于您的系统的对象，其中任何查询或连接的结果都可以以非常容易理解的方式存储，并且可以很容易地测试错误。然后，一旦您收集了所有信息，在服务或培训期间，您可以运行一个通用方法，在特定于您的系统的人类可读对象和机器学习系统期望的任何格式之间建立桥梁。**这消除了培训服务偏差的来源**。作为推论，尽量不要在培训和服务之间使用两种不同的编程语言。这个决定将使你几乎不可能共享代码。

#### 规则 33:如果你基于 1 月 5 日之前的数据建立了一个模型，那么就用 1 月 6 日以及之后的数据来测试这个模型。

一般来说，在您训练模型的数据之后，在收集的数据上测量模型的性能，因为这可以更好地反映您的系统在生产中的表现。如果您基于 1 月 5 日之前的数据生成一个模型，请使用 1 月 6 日之后的数据测试该模型。您可能认为新数据的性能不会那么好，但也不应该差得太多。由于可能存在日常影响，您可能无法预测平均点击率或转换率，但曲线下的面积(代表正面例子得分高于负面例子的可能性)应该相当接近。

#### 规则 34:在过滤的二进制分类中(例如垃圾邮件检测或确定感兴趣的电子邮件)，为了非常干净的数据，在性能上做出小的短期牺牲。

在过滤任务中，标记为否定的例子不向用户显示。假设你有一个过滤器，可以在上菜时阻挡 75%的负面例子。您可能会想从向用户显示的实例中提取额外的训练数据。例如，如果用户将一封电子邮件标记为垃圾邮件，而您的过滤器却让其通过，您可能需要从中吸取教训。

但是这种方法引入了采样偏差。如果在服务期间，您将所有流量的 1%标记为“被拒绝”,并将所有被拒绝的示例发送给用户，则您可以收集更干净的数据。现在你的过滤器阻挡了至少 74%的负面例子。这些举出来的例子可以成为你的训练数据。

请注意，如果您的过滤器阻止了 95%或更多的负面示例，这种方法就变得不太可行。即便如此，如果你想衡量服务表现，你可以做一个更小的样本(比如 0.1%或 0.001%)。一万个例子足以相当准确地估计性能。

#### 规则 35:小心排名问题中固有的偏差。

当你彻底改变你的排名算法，以至于不同的结果出现时，你已经有效地改变了你的算法将来会看到的数据。这种偏斜会出现，你应该围绕它来设计你的模型。有多种不同的方法。这些方法都是支持模型已经看到的数据的方式。

1.  覆盖更多查询的特征具有更高的正则化，而不是仅针对一个查询的那些特征。通过这种方式，该模型将优先考虑特定于一个或几个查询的功能，而不是适用于所有查询的功能。这种方法有助于防止非常流行的结果泄漏到不相关的查询中。请注意，这与更传统的建议相反，更传统的建议是对具有更多唯一值的特性列进行更多的正则化。
2.  仅允许要素具有正权重。因此，任何好的特性都比“未知”的特性要好。
3.  没有文档专用功能。这是#1 的极端版本。例如，即使给定的应用程序是一个受欢迎的下载，不管查询是什么，你也不希望在任何地方都显示它。没有文档专用的特性可以保持简单。你不想在任何地方显示一个特定的流行应用程序的原因与让所有想要的应用程序都可用的重要性有关。例如，如果有人搜索“观鸟应用”，他们可能会下载“愤怒的小鸟”，但这肯定不是他们的意图。展示这样的应用程序可能会提高下载速度，但最终会让用户的需求得不到满足。

#### 规则 36:避免带有位置特征的反馈循环。

内容的位置极大地影响了用户与它交互的可能性。如果你把一个应用程序放在第一个位置，它会被更频繁地点击，你会确信它更有可能被点击。处理这个问题的一种方法是添加位置特征，即关于内容在页面中的位置的特征。您使用位置特征训练您的模型，它会学习加权，例如，特征“第一位置”很重。因此，您的模型对其他因素给予的权重较小，例如“第一位置=真”。然后在发球时，你不要给任何实例位置特征，或者你给它们所有相同的默认特征，因为你在决定显示它们的顺序之前就在给候选人打分。

请注意，由于训练和测试之间的这种不对称性，将任何位置特征与模型的其余部分保持一定程度的分离是很重要的。让模型是位置特征的函数和其余特征的函数的总和是理想的。例如，不要将位置特征与任何文档特征交叉。

#### 规则 37:衡量训练/服务的偏差。

从最普遍的意义上来说，有几种情况会导致偏斜。而且，你可以把它分成几个部分:

*   定型数据和维持数据的性能差异。总的来说，这种情况会一直存在，也不总是不好的。
*   维持数据和“第二天”数据的性能差异。还是那句话，这个会一直存在。你应该调整你的正规化，以最大限度地提高第二天的表现。但是，维持数据和第二天数据之间的性能大幅下降可能表明某些功能对时间敏感，可能会降低模型性能。
*   “第二天”数据和实时数据的性能差异。如果你将一个模型应用于训练数据中的一个例子和发球时的同一个例子，它会给你完全相同的结果(见[规则#5](/machine-learning/guides/rules-of-ml/rule_5_test_the_infrastructure_independently_from_the_machine_learning) )。因此，这里的差异可能表明工程错误。

## ML 第三阶段:缓慢增长、优化改进和复杂模型

有一些迹象表明，第二阶段即将结束。首先，你每月的收益会开始减少。您将开始在度量标准之间进行权衡:在一些实验中，您将看到一些上升，另一些下降。这就是有趣的地方。由于收益更难实现，机器学习必须变得更加复杂。警告:这一部分比前面的部分有更多的蓝天规则。我们已经看到许多团队经历了机器学习第一阶段和第二阶段的快乐时光。一旦进入第三阶段，团队必须找到自己的道路。

#### 规则 38:如果目标不一致已经成为问题，不要在新功能上浪费时间。

随着您的测量趋于平稳，您的团队将开始关注当前机器学习系统的目标范围之外的问题。如前所述，如果产品目标没有被现有的算法目标覆盖，你需要改变你的目标或者你的产品目标。例如，你可以优化点击量、增加量或下载量，但是部分基于人类评分者做出发布决定。

#### 规则 39:发布决定是长期产品目标的代理。

Alice 有一个关于减少预测安装的物流损失的想法。她增加了一个功能。物流损耗下降。当她进行现场实验时，她发现安装率增加了。然而，当她参加一个发布审查会议时，有人指出，日活跃用户数下降了 5%。团队决定不推出该模型。Alice 很失望，但是现在意识到发布决策依赖于多个标准，其中只有一些可以使用 ML 直接优化。

事实是，现实世界不是龙与地下城:没有“命中点”来确定你的产品的健康状况。团队必须使用收集到的统计数据来有效地预测系统在未来会有多好。他们需要关心参与度、1 天活跃用户(DAU)、30 DAU、收入和广告商的投资回报。这些在 A/B 测试中可测量的指标本身只是更长期目标的代表:满足用户、增加用户、满足合作伙伴和利润，即使这样，你也可以考虑从现在起五年内拥有一个有用的、高质量的产品和一个繁荣的公司。

只有当所有指标都变得更好(或者至少不会变得更差)时，才容易做出发布决策。如果团队可以在复杂的机器学习算法和简单的启发式算法之间进行选择，如果简单的启发式算法在所有这些指标上都做得更好，那么它应该选择启发式算法。此外，没有对所有可能的度量值的明确排序。具体来说，考虑以下两种情况:

| 实验 | 每日活跃用户 | 收入/日 |
| --- | --- | --- |
| A | 一百万 | 四百万美元 |
| B | 两百万 | 两百万美元 |

如果当前系统是 A，那么团队不太可能切换到 B。如果当前系统是 B，那么团队不太可能切换到 A。这似乎与理性行为相冲突；然而，对不断变化的指标的预测可能会成功，也可能不会成功，因此这两种变化都存在很大的风险。每个度量都包含了团队所关心的一些风险。

此外，没有度量标准涵盖团队的最终关注点，“五年后我的产品会在哪里”？

另一方面，个人倾向于支持一个他们可以直接优化的目标。大多数机器学习工具都青睐这样的环境。在这样的环境下，开发新功能的工程师可以获得稳定的发布流。有一种机器学习，多目标学习，开始解决这个问题。例如，可以制定一个约束满足问题，它对每个度量都有较低的限制，并优化一些度量的线性组合。然而，即使这样，也不是所有的指标都容易被框定为机器学习的目标:如果一个文档被点击或一个应用程序被安装，那是因为内容被显示了。但是很难弄清楚为什么用户会访问你的网站。如何从整体上预测一个网站未来的成功是 [AI-complete](https://wikipedia.org/wiki/AI-complete) :和计算机视觉或者自然语言处理一样难。

#### 规则 40:保持合奏简单。

接受原始特性并直接对内容进行排名的统一模型是最容易调试和理解的模型。然而，模型的集合(一个“模型”结合了其他模型的分数)可以工作得更好。为了简单起见，每个模型要么是一个只接受其他模型输入的集合，要么是一个接受许多特征的基础模型，但不能两者都是。如果你的模型在其他单独训练的模型之上，那么组合它们会导致不好的行为。

使用一个简单的模型进行集成，只将“基础”模型的输出作为输入。您还想在这些集合模型上实施属性。例如，由基础模型产生的分数的增加不应该降低集合的分数。此外，如果引入的模型是语义可解释的(例如，校准的),那么底层模型的变化不会混淆整体模型，这是最好的。此外，**强制底层分类器的预测概率的增加不降低集成的预测概率**。

#### 规则 41:当性能停滞不前时，寻找质量上新的信息来源，而不是提炼现有的信号。

您添加了一些关于用户的人口统计信息。您在文档中添加了一些关于单词的信息。您已经完成了模板探索，并调整了正则化。几个季度以来，您还没有看到一款产品的关键指标提高了 1%以上。现在怎么办？

是时候开始为完全不同的特性构建基础设施了，比如这个用户在最近一天、一周或一年访问过的文档的历史，或者来自不同属性的数据。使用[维基数据](https://wikipedia.org/wiki/Wikidata)实体或者公司内部的东西(比如谷歌的[知识图谱](https://wikipedia.org/wiki/Knowledge_Graph))。使用深度学习。开始调整你对投资回报的预期，并相应地扩大你的努力。正如在任何工程项目中一样，您必须权衡增加新功能的好处和增加复杂性的成本。

一组内容的多样性可能意味着许多事情，内容来源的多样性是最常见的一种。个性化意味着每个用户得到他们自己的结果。相关性意味着特定查询的结果比任何其他查询更适合该查询。因此，所有这三个属性都被定义为不同于普通属性。

问题是普通人往往很难被打败。

请注意，如果你的系统在衡量点击量、花费时间、观看次数、+1、转发次数等等，你就是在衡量内容的**受欢迎程度**。团队有时试图学习多样化的个人模型。为了个性化，他们添加了允许系统个性化(一些代表用户兴趣的特征)或多样化(指示该文档是否与返回的其他文档有任何共同特征的特征，如作者或内容)的特征，并且发现这些特征得到的权重比他们预期的要低(或者有时是不同的符号)。

这并不意味着多样性、个性化或相关性没有价值。正如前面的规则所指出的，您可以进行后期处理来增加多样性或相关性。如果你看到长期目标增加了，那么你可以宣称除了受欢迎之外，多样性/相关性也是有价值的。然后，您可以继续使用您的后期处理，或者根据多样性或相关性直接修改目标。

#### 规则 43:你的朋友在不同的产品上往往是一样的。你的兴趣不是。

谷歌的团队通过采用一种模型来预测一种产品的联系紧密程度，并使其在另一种产品上很好地工作，从而获得了很大的吸引力。你的朋友就是他们。另一方面，我看到几个团队在不同产品部门的个性化特性上苦苦挣扎。是的，它似乎应该工作。目前看来，似乎没有。有时奏效的方法是利用一处房产的原始数据来预测另一处房产的行为。此外，请记住，即使知道用户在另一个属性上有历史记录也会有所帮助。例如，两个产品上的用户活动的存在本身就可以是指示性的。

谷歌和外部都有很多关于机器学习的文档。

## 承认

感谢 David Westbrook，Peter Brandt，Samuel Ieong，Chenyu Zhao，Li Wei，Michalis Potamias，Evan Rosen，Barry Rosenberg，Christine Robson，James Pine，Tal Shaked，Tushar Chandra，Mustafa Ispir，Jeremiah Harmsen，Konstantinos Katsiapis，Glen Anderson，Dan，shishishir Birmiwal，Gal Elidan，Wu，Jaihui Liu，Fernando Pereira 和 Hrishikesh Aradhye 为本文件提供了许多更正，建议和有用示例。此外，感谢克里斯汀·勒费夫尔、苏达·巴苏和克里斯·伯格帮助完成了早期版本。任何错误，遗漏，或(喘息！)冷门观点都是我自己的。

## 附录

本文档中有多处提及 Google 产品。为了提供更多的背景信息，我在下面给出了最常见例子的简短描述。

### YouTube 概述

YouTube 是一种流媒体视频服务。YouTube Watch Next 和 YouTube 主页团队都使用 ML 模型对视频推荐进行排名。观看下一个推荐在当前播放的视频之后观看的视频，而主页向浏览主页的用户推荐视频。

### Google Play 概述

Google Play 有许多解决各种问题的模型。玩搜索，玩首页个性化推荐，‘用户也装’app 都是用机器学习。

### Google Plus 概述

Google Plus 在各种情况下使用机器学习:对用户看到的帖子“流”中的帖子进行排名，对“热门”帖子(现在非常受欢迎的帖子)进行排名，对你认识的人进行排名，等等。Google Plus 在 2019 年关闭了所有个人账户，2020 年 7 月 6 日由 Google Currents 取代商业账户。