# 为什么我们从 Heroku 转向 Google Kubernetes 引擎|雨林问答

> 原文:[https://www . rain forest QA . com/blog/2019-04-02-why-we-moved-from-heroku-to-Google-kubernetes-engine/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://www.rainforestqa.com/blog/2019-04-02-why-we-moved-from-heroku-to-google-kubernetes-engine/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

直到去年年底，雨林在 Heroku 上运行我们的大部分生产应用程序。Heroku 在很多方面都是一个非常棒的雨林平台:它允许我们在不雇佣大型运营团队的情况下扩展并保持敏捷，并且整体开发人员体验是无与伦比的。但在 2018 年，很明显我们开始超越 Heroku。我们最终转移到谷歌云平台(GCP)，我们的大部分应用程序运行在谷歌 Kubernetes 引擎(GKE)；以下是我们如何做出决定并选择新的 DevOps 技术堆栈的。

## 理由:转变背后的 3 个主要驱动因素

### 1.可量测性

我们是 Rainforest 的大量 Postgres 用户，我们的大部分客户数据都在一个大型数据库中(最终我们可能会将我们的数据分成更小的独立服务，但这是一项工程工作，我们还不想深入其中)。2018 年，我们很快接近了 Heroku 数据库计划的大小限制(当时为 1 TB ),我们不想在仍与 Heroku 签订合同的情况下冒险触及任何限制。

我们在 Heroku 的计算方面也遇到了限制:我们一些较新的基于自动化的功能涉及运行大量短期批处理作业，这在 Heroku 上不太好(由于计算资源成本相对较高)。作为权宜之计，我们已经在 [AWS Batch](https://aws.amazon.com/batch/) 上运行了一些服务，但我们从未对该解决方案感到特别满意，因为我们有多个从运营角度来看非常不同的计算环境(团队中很少有工程师对 Batch 有深刻的理解)。

### 2.安全性

随着雨林的增长，应用程序安全性变得同等重要，而“标准”Heroku 产品由于缺乏安全性方面的灵活性(例如，无法在私有网络中设置 Postgres 实例)而变得有问题。我们一度试图迁移到 Heroku Shield 来解决这些问题，但是我们发现它并不适合我们的应用。

### 3.费用

也许令人惊讶的是，成本并不是决定离开 Heroku 的最初驱动因素。Heroku 以极其昂贵而闻名，但这并不是我们的普遍经验:当考虑到保持一个精简的运营团队所节省的成本时，Heroku 与主要的云提供商相比是非常划算的。这一点尤其正确，因为我们的大部分托管成本都用在了数据库上，而托管 Postgres 服务的成本在不同的云提供商(包括 Heroku)之间是相似的。

然而，Heroku 的成本成为一个问题，有几个原因:

1.  Heroku 的默认运行时不包括一些主要云提供商“开箱即用”的安全相关功能，如[虚拟私有云](https://cloud.google.com/vpc/)。一旦这些特性成为一种需求(对我们来说是这样)，Heroku 就成了一个不太划算的选择。
2.  GCP 和 AWS 的原始计算资源都比 Heroku 便宜，正如前面提到的，我们还不能在 Heroku 上运行所有的计算密集型服务。在规划未来增长时，我们希望有一个平台能够通过一套通用工具来处理我们的 web 服务和计算密集型工作负载。

## 我们的 Heroku 设置

Heroku 对于如何期望你在它的环境中运行应用程序非常固执:所有的应用程序都必须遵循 [12 因素指南](https://12factor.net)才能运行良好，并且应用程序总是在 [Heroku 的 dynos](https://www.heroku.com/dynos) 中运行，这不是非常灵活。不过，这些限制带来了巨大的好处。12 因素应用易于横向扩展，对其环境的依赖性很小，因此易于在本地开发环境中运行并移植到新的生产环境中。对于我们的应用程序，我们非常严格地遵循了 12 因素准则，持久数据被专门存储在 Postgres 或第三方服务中，如 [S3](https://aws.amazon.com/s3/) 。

对于自动缩放，我们在大多数 Heroku 应用程序中使用了 [HireFire](https://hirefire.io) 。Web workers 通常根据负载进行调整，但是后台 worker 根据各种队列大小进行调整。(事实证明，在大多数其他 PaaS 产品中模仿这一特性是一件棘手的事情。)

## 为什么是 Kubernetes？

鉴于我们正在远离 Heroku，我们需要一个新的平台来运行我们的应用程序，而不需要太多的移植工作。我们可以完全跳过容器化的解决方案，直接在虚拟机上运行我们的代码(使用像 [capistrano](https://capistranorb.com) 这样的工具来执行实际部署)，但是我们很快就放弃了这个选项，原因有很多:

*   我们的环境是异构的:我们最大的应用程序使用 Rails，但是我们也有用 Go、Elixir、Python 和 Crystal 编写的较小的服务。为每种语言维护单独的部署管道将是一个主要的难点。
*   设置自动伸缩、高可用性、监控和日志聚合等基本功能需要大量的开发时间，而且几乎不可能以独立于供应商的方式实现这些功能。
*   Heroku 表现得像一个容器化的环境(与 Docker 有相似的技术)，这是我们的开发人员所习惯的。我们必须看到显著的优势才能转向替代模式。

总的来说，正是因为这些原因，行业正在向容器化部署发展，我们没有看到任何令人信服的理由来反对这一趋势。

## 考虑到这一点，我们评估了四个主要的基于 Docker 的平台:

AWS 将 Elastic Beanstalk 作为他们运行容器化应用程序的“易于使用”的方式进行营销。虽然这在理论上看起来是一个有趣的选择，但最初的实验表明它在实践中很难使用。Elastic Beanstalk 在相当长的一段时间内也没有太多重大更新，因此 AWS 对该产品的承诺尚不明确。这是一个很容易拒绝的选择。

我们更认真考虑的一个选择是 Convox，它自称是 Heroku 的开源替代方案(使用 AWS 作为其底层基础设施提供商)。假设我们符合他们的客户特征，迁移可能会相当简单。

然而，经过一番评估后，我们担心，与主要的云提供商相比，我们依赖的平台在行业中的影响力相对较小。Convox 让其客户直接访问底层 AWS 资源，这很好，但 Convox 的业务变化仍可能使我们依赖于不受支持的产品——对于这样一个关键的供应商，这不是我们愿意冒的风险。Convox 还缺少一些与自动缩放相关的关键特性，这是棺材上的最后一颗钉子。

ECS 或多或少是 Kubernetes 的直接竞争对手，它提供了一种非常灵活地运行容器化应用程序的方法(以复杂性为代价)。我们已经通过 AWS Batch(它是 ECS 之上的一层)接触了一些 ECS，我们对用户体验没有特别的印象。我们也不热衷于通过使用 ECS 来接受供应商锁定的数量(例如，不可能在开发人员的笔记本电脑上设置类似生产的环境)，或者对设置自定义自动伸缩和类似功能所需的开发工作量感到高兴。

如果没有更好的选择，我们可能会选择 ECS，但谢天谢地，事实并非如此。

### 库伯内特斯

Kubernetes 在我们考虑的选项中脱颖而出，原因如下:

1.  Kubernetes 在 DevOps 领域有着巨大的吸引力，拥有来自所有主要云供应商的托管实现，以及几乎无穷无尽的培训材料和补充技术。
2.  Kubernetes 是开源的，这是一个重要的优势:这意味着我们可以避免供应商锁定，并实现模拟生产的本地开发环境。
3.  Kubernetes 有一个很大的特性集，非常适合我们的需求，包括我们更奇特的需求，比如基于自定义指标的自动缩放。

Kubernetes 的批评者经常说，它的复杂性对于许多情况来说是过度的。虽然 Kubernetes 确实是一个非常庞大和复杂的软件，但基本的抽象大多是直观的和经过深思熟虑的，我们能够避开很多复杂性有几个原因:

*   Kubernetes 是 12 因素应用程序的天然平台，不需要数据持久性、状态性和其他棘手的问题。
*   使用托管的 Kubernetes 服务作为客户端比实际运行 Kubernetes 集群要容易得多。

## 为什么选择谷歌云平台？

我们已经决定使用 Kubernetes，所以问题仍然是:哪个 Kubernetes？在原始虚拟机上运行具有生产价值的 Kubernetes 集群对我们来说并不是一个可行的选择(因为我们的运营团队仍然相对较小)，所以我们在三个最著名的云提供商上评估了托管的 Kubernetes 服务:AWS、GCP 和 Azure。

Kubernetes 不是我们唯一的需求:我们还需要托管的 Postgres 和 Redis 服务。这排除了 Azure 作为一个选项，因为它的托管 Postgres 服务与 AWS 和 GCP 相比相对不成熟(数据大小限制与 Heroku 相当)。这就剩下了 AWS 和 GCP，它们在大多数方面都是同样好的选择:成本预测非常相似，而且两个平台都提供大量的托管服务。

然而，在 GCP 上管理的 Kubernetes 服务 GKE 和 AWS 的同类服务 EKS 之间有着巨大的差异。GKE 是一个成熟得多的产品，具有许多 EKS 所缺乏的基本特征:

*   GKE 管理 Kubernetes 主节点和节点，而 EKS 只管理主节点。对于 EKS，我们必须完全维护 Kubernetes 节点，包括维护安全更新。
*   GKE 在集群级别管理自动缩放，并且在应用程序级别对[水平 pod 自动缩放](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)提供极好的支持，包括对自定义指标的[自动缩放的支持。在评估时，EKS 不支持集群级自动伸缩，对任何类型的水平 pod 自动伸缩的支持也极其有限。](https://cloud.google.com/kubernetes-engine/docs/tutorials/custom-metrics-autoscaling)

这些差异仅仅触及了 GKE 和 EKS 之间差异的表面，但它们足以排除 EKS 作为一个可行的选择。

## 技术堆栈

做出重大决定后，我们必须选择新的技术组合！在选择技术时，我们有一些指导原则:

*   **主要受管理:**考虑到我们的运营团队的职责范围，他们仍然很小，所以我们希望尽量减少他们负责运行复杂软件堆栈的情况。我们强烈偏好可用的托管服务。
*   **最小化变更:**对于工程团队来说，迁移不可避免地会是一个很大的变更，但是我们希望尽可能地使过渡不那么痛苦。在可行的情况下，我们希望保留现有的提供商和实践。
*   **尽可能枯燥:**“云原生”开发运维环境正处于一个激动人心的快速发展阶段，新技术似乎在一夜之间涌现。然而，雨林的托管需求通常非常简单:我们的大多数服务都是“传统的”Postgres 支持的 web 应用程序，它们通过 REST APIs 或消息队列进行通信。虽然我们欣赏 Kubernetes 带来的架构灵活性(特别是与 Heroku 相比)，但对于初始迁移，我们决定不要太深入使用“尖端”辅助技术，这些技术对我们的用例来说并不是绝对必要的。

根据这些指导原则，我们选定了以下技术:

*   [**Terraform**](https://www.terraform.io/) :我们早期做出的一个更重要的决定是尽可能地转向基础设施即代码。Terraform 并不完美，但它是迄今为止最流行、最完整的代码式管理基础设施的选择，尤其是在 GCP。(我们以过渡为“借口”,将基础设施的许多其他方面交由 Terraform 管理。)
*   谷歌 Kubernetes 引擎 :考虑到我们决定使用 Kubernetes，GKE 是一个不用动脑筋的人——它是完全管理的，有非常丰富的功能集。
*   [**云 SQL for PostgreSQL**](https://cloud.google.com/sql/docs/postgres/) :我们的 Postgres 数据库可能是我们基础设施中最重要的部分，因此找到一个支持我们想要的功能(如高可用性、自动备份和内部网络连接)的托管 Postgres 服务非常重要。云 SQL 符合这个要求。
*   [**Cloud Memorystore**](https://cloud.google.com/memorystore/) :我们相对来说是 Redis 的轻度用户，但是我们确实把它作为一些应用的缓存层。Cloud Memorystore 是一个相对简单的 Redis 实现，但对于我们的需求来说已经足够好了。
*   [**Helm**](https://helm.sh/) : Helm 填补了部署到 Kubernetes 的一些“缺失部分”(例如，模板和发布管理)。我们选择了它，因为它的社区很大，而且相对简单。对于实际的部署过程，我们使用[云构建](https://cloud.google.com/cloud-build/)来构建应用程序的 Docker 映像，使用 [CircleCI](https://circleci.com/) 来启动发布。
*   [**stack driver**](https://cloud.google.com/stackdriver/):stack driver 或多或少是 GKE 上的“默认”日志记录和监控解决方案，它有一些我们的实现所必需的集成。

我们能够保留大部分其他现有的基础设施工具(如 [Cloudflare](https://www.cloudflare.com) 和 [Statuspage](https://www.statuspage.io/) ，只做最小的改动。

我们也考虑了一些技术，但没有在最初的过渡中考虑:

*   Istio :当我们开始过渡时，安装和管理 Istio 是一个手动过程，似乎太复杂了，无法满足我们的需求。GKE 后来增加了内置的 Istio 支持，我们可能会考虑在未来使用，但在我们的规模下，我们还没有看到服务网格的需求。
*   [**Vault**](https://www.vaultproject.io/) : Vault 有许多引人注目的秘密管理功能，但我们必须自己将它作为基础设施的关键部分来运行，这是一个主要缺点。但是，我们可能会考虑将其作为未来基础架构升级的一部分。
*   [**Spinnaker**](https://www.spinnaker.io/) **，**[**weaver works**](https://www.weave.works/)**以及类似的** : Kubernetes 允许大量的部署灵活性，并且有许多强大的 CI/CD 选项可以与 Kubernetes 集成，以实现定制部署策略之类的东西。但是我们对已经存在的 CI/CD 管道(使用 [CircleCI](https://circleci.com/) )非常满意，所以我们决定实现与 Kubernetes 集成所需的最小变化，而不是尝试实现一些“更好的”东西。

在以后的文章中，我们将讨论迁移过程本身。