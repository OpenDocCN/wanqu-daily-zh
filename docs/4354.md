# HBO 的硅谷如何用移动 TensorFlow、Keras & React Native 打造“非热狗”|

> 原文：<https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

# HBO 的硅谷如何用移动 TensorFlow、Keras & React Native 打造“不是热狗”



> —2018 年 7 月更新—
> 
> **未热狗已被** [**提名为黄金时段艾美奖**](http://www.emmys.com/awards/nominees-winners/2018/outstanding-creative-achievement-in-interactive-media-within-a-scripted-program) **。感谢所有粉丝的热情接待，感谢 HBO 布朗希尔制作公司的所有人让这一切成为可能。**



T2】

HBO 电视台[节目*硅谷*](http://www.hbo.com/silicon-valley) 发布了一个真正的人工智能应用程序，它可以识别热狗——而不是热狗——就像第四季第四集中显示的一样(该应用程序是[，现在在 Android 和 iOS 上可用！](https://www.seefoodtechnologies.com/nothotdog/))



为了实现这一点，我们设计了一个直接在手机上运行的定制神经架构，并用 Tensorflow，Keras & Nvidia GPUs 对其进行了训练。

虽然用例很滑稽，但这款应用是深度学习和边缘计算的一个平易近人的例子。所有人工智能工作 100%由用户的设备驱动，图像无需离开手机即可处理。这为用户提供了更快的体验(没有到云的往返)、离线可用性和更好的隐私。这也使我们能够以 0 美元的成本运行应用程序，即使在 100 万用户的负载下，与传统的基于云的人工智能方法相比，也能节省大量成本。



The author’s development setup with the attached eGPU used to train Not Hotdog’s AI.



该应用由展会内部开发，由一名开发人员开发，在一台笔记本电脑上运行，连接 GPU，使用手动整理的数据。在这方面，它可能提供了一种感觉，即在有限的时间和资源下，非技术公司、个人开发人员和爱好者可以实现什么。本着这种精神，本文试图给出一个详细的步骤概述，以帮助其他人建立自己的应用程序。



T2】

1.  **app**[**☛**](#88fa)
2.  **从原型到生产**[**☛**](#249c)v 0:原型[☚](#7b0a)
    v1:tensor flow，Inception &转移学习[☚](#7abc)
    v2:keras&squeeze net[☚](#d4ca)
3.  **deep dog 架构**[**☛**](#61c5)训练[☚](#bf7e)
    在手机上运行神经网络[☚](#12de)
    通过动态注入神经网络来改变应用程序行为[☚](#2f6b)
    我们会有哪些不同的做法[☚](#919a)
4.  **UX，DX，bias&艾未未** [**☛**](#01cf)

T2】

# 1.该应用程序



如果你没看过节目或者没试过 [app](https://www.seefoodtechnologies.com/nothotdog/) (你应该！)，该应用程序让你拍一张照片，然后告诉你它是否认为那张照片是热狗。这是一个简单的用例，向最近的人工智能研究和应用致敬，特别是 ImageNet。

虽然我们可能比任何人都投入了更多的工程资源来识别热狗，但该应用仍然以可怕和/或微妙的方式失败。



相反，它有时也能在复杂的情况下识别热狗……[据 Engadget](https://www.engadget.com/2017/05/15/not-hotdog-app-hbo-silicon-valley/) 、*说，这太不可思议了。我用这款应用在 20 分钟内识别食物的成功次数，比过去两年用 Shazam 标记和识别歌曲的成功次数还要多*。





T2】

# 2.从原型到生产

你有没有发现自己在阅读黑客新闻时，想着*“他们为此筹集了 1000 万美元的 A 轮融资？我可以在一个周末内完成！”*这个应用程序可能感觉很像，最初的原型确实是在一个周末使用谷歌云平台的 Vision API 和 React Native 构建的。但是，我们最终在 app store 上发布的最终应用程序需要几个月的额外(兼职)工作，才能提供外人难以欣赏的有意义的改进。我们花了几周时间优化整体准确性、训练时间、推理时间，迭代我们的设置&工具，以便我们可以有更快的开发迭代，花了整个周末优化围绕 iOS & Android 权限的用户体验(甚至不要让我开始那个)。

技术博客帖子或学术论文经常跳过这一部分，更倾向于给出最终选择的解决方案。为了帮助其他人从我们的错误和选择中吸取教训，在我们在下一节中描述我们最终发布的最终架构之前，我们将呈现对我们无效的方法的简略视图。

## V0:原型



Example image & corresponding API output from Google Cloud Vision’s documentation



我们选择 React Native 来构建原型，因为它将为我们提供一个简单的沙箱来进行实验，并帮助我们快速支持许多设备。体验最终很好，我们在项目的剩余部分保持了 React Native:它并不总是让事情变得容易，应用程序的设计有目的地受到限制，但最终 React Native 完成了工作。

我们用于原型的另一个主要组件— [Google Cloud 的 Vision API](https://cloud.google.com/vision/) 很快就被放弃了。有 3 个主要因素:

1.  首先，它识别热狗的准确率一般。虽然它在识别大量事物方面很棒，但在识别特定的一件事物方面却不那么棒，在我们 2016 年的实验中，有各种非常常见的例子会失败。
2.  由于其作为云服务的性质，它必然比在设备上运行慢(网络延迟是痛苦的！)，并且离线不可用。图像离开设备的想法也可能引发隐私和法律问题。
3.  最后，如果这款应用成功了，在谷歌云上运行的成本可能会高得令人望而却步。

出于这些原因，我们开始尝试所谓的“边缘计算”，就我们的目的而言，这意味着在我们的笔记本电脑上训练我们的神经网络后，我们将导出它并将其直接嵌入到我们的移动应用程序中，这样神经网络执行阶段(或推理)将直接在用户的手机中运行。

## V1:张量流、认知和迁移学习



通过与 TensorFlow 团队的 [Pete Warden](https://petewarden.com/) 的偶遇，我们已经意识到它能够直接在 iOS 设备上运行 TensorFlow，并开始探索这条道路。React Native 之后，TensorFlow 成为我们栈的第二个固定部分。

在我们的 React 原生 shell 中集成 TensorFlow 的 Objective-C++ camera 示例只花了一天的时间。使用他们的迁移学习脚本花费了稍长的时间，这有助于您重新训练 Inception 架构来处理更具体的映像问题。Inception 是谷歌为处理图像识别问题而建立的一系列神经架构的名称。Inception 是“预训练”的，这意味着训练阶段已经完成，权重已经设置。对于图像识别网络来说，最常见的是在 ImageNet 上对它们进行训练，这是一个包含 20，000 多种不同类型对象的数据集(热狗就是其中之一)。然而，就像谷歌云的视觉 API 一样，ImageNet 培训在这里奖励广度和深度，并且在 20，000 多个类别中的单个类别上缺乏开箱即用的准确性。因此，再训练(也称为“迁移学习”)旨在获得一个经过全面训练的神经网络，并对其进行再训练，以更好地处理您想要处理的特定问题。这通常涉及某种程度的“遗忘”，要么从堆栈中删除整个层，要么慢慢消除网络区分一种对象(如椅子)的能力，以利于更准确地识别你所关心的对象(如热狗)。

虽然网络(在这种情况下为 Inception)可能已经在 ImageNet 中包含的 1400 万张图像上进行了训练，但我们能够在仅仅几千张热狗图像上对其进行重新训练，以获得大幅增强的热狗识别。

迁移学习的最大优势是，与从头开始训练相比，你会更快地获得更好的结果，并且使用更少的数据。一次完整的训练可能需要在多个 GPU 上花费几个月的时间，并需要数百万张图像，而再训练可以在几个小时内在一台拥有几千张图像的笔记本电脑上完成。

我们遇到的最大挑战之一是准确理解什么应该算作热狗，什么不应该。定义什么是“热狗”最终出奇地困难(切好的香肠算吗，如果算，是哪种？)并受制于文化解读。

类似地，我们问题的“开放世界”性质意味着我们必须处理几乎无限数量的输入。虽然某些计算机视觉问题的输入相对有限(比如说，有或没有机械故障的螺栓的 x 射线)，但我们必须准备好应用程序，以获得自拍、自然照片和任何数量的食物。

可以说，这种方法是有希望的，并且确实导致了一些改进的结果，但是，由于几个原因，它不得不被放弃。

首先，我们问题的本质意味着训练数据的严重不平衡:非热狗的例子比热狗的例子多得多。在实践中，这意味着如果你在 3 张热狗图片和 97 张非热狗图片上训练你的算法，它识别前者的 0%但识别后者的 100%，默认情况下它仍将获得 97%的准确率！使用 TensorFlow 的再训练工具解决这个问题并不简单，基本上需要从头开始建立深度学习模型，导入权重，并以更可控的方式进行训练。

在这一点上，我们决定咬紧牙关，从 Keras 开始，这是一个深度学习库，在 TensorFlow 的基础上提供了更好、更易于使用的抽象，包括非常棒的训练工具和 class_weights 选项，这是处理我们正在处理的这种数据集不平衡的理想选择。

我们利用这个机会尝试了其他流行的神经架构，比如 VGG，但是还有一个问题。他们中没有一个人可以轻松地安装在 iPhone 上。它们消耗了太多的内存，导致应用程序崩溃，有时需要 10 秒钟来计算，从 UX 的角度来看，这并不理想。人们尝试了许多方法来缓解这一问题，但最终这些架构太大，无法在移动设备上高效运行。

## V2: Keras & SqueezeNet



SqueezeNet vs. AlexNet, the grand-daddy of computer vision architectures. Source: [SqueezeNet paper](https://arxiv.org/abs/1602.07360).



给你一个时间上的背景，这大概是项目的中点。到那时，UI 已经完成了 90%以上，几乎没有什么要改变的了。但事后看来，神经网络最多完成了 20%。我们有很好的挑战意识&一个很好的数据集，但最终的神经架构已经写了 0 行，我们的神经代码没有一个能可靠地在移动设备上运行，甚至我们的准确性也将在未来几周内大幅提高。

摆在我们面前的问题很简单:如果《盗梦空间》和《VGG》太大，有没有一个更简单的、预先训练好的神经网络我们可以重新训练？在一贯优秀的杰里米·p·霍华德的建议下(那家伙这辈子都去哪了？)，我们探索了 Xception，Enet，SqueezeNet。由于其作为嵌入式深度学习解决方案的明确定位，以及 GitHub (yay 开源)上预先训练的 Keras 模型的可用性，我们很快就选定了 [SqueezeNet](https://arxiv.org/abs/1602.07360) 。

那么这有多大的区别呢？像 VGG 这样的架构使用了大约 1.38 亿个参数(本质上是对神经元和它们之间的值进行建模所需的数量)。《盗梦空间》已经是一个巨大的改进，只需要 2300 万个参数。相比之下，SqueezeNet 只需要 125 万英镑。

这有两个优点:

1.  在训练阶段，训练一个较小的网络要快得多。内存中要映射的参数更少，这意味着您可以将训练并行化得更好(批量更大)，网络将更快地收敛(即逼近理想化的数学函数)。
2.  在生产中，模型要小得多，速度也快得多。SqueezeNet 需要不到 10MB 的内存，而像 Inception 需要 100MB 或更多。增量是巨大的，并且当在移动设备上运行时特别重要，移动设备可能只有不到 100MB 的内存来运行你的应用。小型网络计算结果的速度也比大型网络快得多。

当然也有权衡:

1.  较小的神经架构具有较少的可用“记忆”:它在处理复杂情况(例如识别 20，000 个不同的对象)或甚至处理复杂的子情况(例如，欣赏纽约风格的热狗和芝加哥风格的热狗之间的差异)时不会那么有效。当试图识别 ImageNet 的 20，000 个不同的对象时，SqueezeNet 只能获得大约 58%的分数，而 Vgg 在 72%的时候都是准确的。
2.  在小网络上用迁移学习更难。从技术上来说，没有什么可以阻止我们使用与 Inception & Vgg 相同的方法，让 SqueezeNet“忘记”一点，并专门为热狗而不是热狗重新训练它。在实践中，我们发现很难调整学习速度，结果总是比从头开始训练 SqueezeNet 更令人失望。这也可能是由我们问题的开放世界性质造成或恶化的。
3.  据说，较小的网络很少会过度适应，但这种情况发生在我们几个“小”架构上。过度适应意味着你的网络专门化了太多，它不是学习如何识别一般的热狗，而是学习准确地识别&只识别你用来训练的特定热狗图像。一个人类的类似物将会在视觉上准确地记忆呈现给你的图像中哪一个是“热狗”,而不会抽象出热狗通常是由小圆面包中的香肠组成的，可能还有调味品等等。如果给你一个全新的热狗图像，而不是你记忆中的图像，你会倾向于说这不是热狗。因为较小的网络通常“内存”较少，所以很容易理解为什么它们更难专门化。但在一些情况下，我们的小型网络的准确率跃升至 99%，突然变得无法识别它在训练中没有见过的图像。一旦我们添加了足够的数据扩充(半随机地拉伸/扭曲输入图像，因此网络不是在 1，000 个图像中的每一个上被训练 1，000 次，而是在 1，000 个图像的有意义的变化上被训练，这使得网络不太可能准确地记住 1，000 个图像，而是必须学会识别热狗的“特征”(面包、香肠、调味品等)。)同时保持足够的流动性/概括性，不会过度依赖于训练集中特定图像的特定像素值。



Data Augmentation example from the [Keras Blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).



在这个阶段，我们开始尝试调整神经网络架构。特别是，我们开始使用批量规范化，尝试不同的激活函数。

*   批处理规范化通过“平滑”堆栈中不同阶段的值来帮助您的网络更快地学习。这种做法的确切原因似乎还没有得到很好的理解，但它有助于您的网络更快地收敛，这意味着它可以用更少的训练获得更高的准确性，或者在相同数量的训练后获得更高的准确性，通常是惊人的。
*   激活函数是决定你的“神经元”是否激活的内部数学函数。许多论文仍然使用 ReLU，校正的线性单位，但是我们使用 eLU 得到了最好的结果。

在 SqueezeNet 中添加批处理标准化和 ELU 后，我们能够训练神经网络，从头开始训练时达到 90%以上的准确性，但是，它们相对脆弱，这意味着同一网络在某些情况下会过拟合，或者在面对现实测试时会欠拟合。即使向数据集添加更多的例子，并对数据进行扩充，也无法提供一个符合预期的网络。

因此，虽然这个阶段很有希望，并且第一次给了我们一个完全可以在 iPhone 上工作的功能应用程序，但在不到一秒钟的时间内，我们最终转向了第四个也是最后一个架构。



T2】

# 3.DeepDog 架构



## 设计

我们最终的架构在很大程度上受到了谷歌 4 月 17 日发表的 [MobileNets 论文](https://arxiv.org/abs/1704.04861)的激励，该论文承诺了一种新的神经架构，在像我们这样的简单问题上具有类似盗梦空间的准确性，只有大约 400 万个参数。这意味着它位于一个有趣的甜蜜点之间，对于我们的目的来说，SqueezeNet 可能过于简单，而在移动设备上使用 Inception 或 VGG 可能会过度紧张。该论文引入了一些容量来调整网络的规模&复杂性，特别是在内存/CPU 消耗与准确性之间进行权衡，这在当时是我们最关心的问题。

离应用程序发布还有不到一个月的时间，我们努力重现论文的结果。这完全是虎头蛇尾，因为在论文发表的一天内，伊斯坦布尔理工大学的学生 Refik Can Malli 已经在 GitHub 上公开提供了一个 [Keras 实现](https://github.com/rcmalli/keras-mobilenet),当我们从他出色的 Keras SqueezeNet 实现中获得灵感时，他的工作已经让我们受益匪浅。深度学习社区的深度&开放性，以及像 R.C .这样的天才头脑的存在，是让深度学习在今天的应用中变得可行的原因——但它们也让这个领域的工作比我们参与的任何技术趋势都更令人激动。

我们最终的架构与 MobileNets 架构或惯例有了重大的不同，特别是:

*   我们不在深度方向和点方向卷积之间使用批量标准化和激活，因为异常论文(详细讨论了深度方向卷积)似乎表明它实际上会导致这种类型架构的准确性降低(正如 Reddit 上 QuickNet 论文的作者[有益地指出的](https://www.reddit.com/r/MachineLearning/comments/663m43/r_170404861_mobilenets_efficient_convolutional/dgfaoz1/))。这也有利于减小网络规模。
*   我们用 ELU 而不是瑞鲁。就像我们的 SqueezeNet 实验一样，与 ReLU 相比，它提供了更好的收敛速度和最终精度
*   我们没有使用 PELU。虽然很有希望，但每当我们试图使用它时，这个激活函数似乎就会陷入二元状态。我们的网络精度不是逐渐提高，而是从一批到下一批在 0%到 100%之间变化。不清楚为什么会发生这种情况，可能只是因为实现错误或用户错误。融合图像的宽/高轴没有任何效果。
*   我们没有使用 SELU。iOS 和 Android 版本之间的一个简短调查导致了与 PELU 非常相似的结果。我们怀疑 SELU 不应该作为一种激活功能的银弹被孤立使用，而是——正如论文标题所暗示的——作为狭义 SNN 架构的一部分。
*   我们保持对 ELU 的批处理规范化的使用。有许多迹象表明，这应该是不必要的，但是，我们运行的每一个实验没有批量标准化完全无法收敛。这可能是由于我们的架构规模较小。
*   我们在激活之前使用了批处理规范化*。虽然这是最近一些争论的[主题](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)，我们在小型网络上激活后放置 BN 的实验也没有收敛。*
*   为了优化网络，我们使用了[循环学习率](https://arxiv.org/abs/1506.01186)和(fast.ai 学生伙伴)布拉德·肯斯特勒的优秀 [Keras 实现](https://github.com/bckenstler/CLR)。clr 消除了为你的训练寻找最佳学习率的猜谜游戏。更重要的是，通过在整个训练过程中上下调整学习速度&，它们有助于实现最终的准确性，根据我们的经验，这比传统的优化器要好。出于这两个原因，我们不能设想在未来使用除 CLRs 之外的任何东西来训练神经网络。
*   值得一提的是，我们认为没有必要调整 MobileNets 架构中的 *α* 或 *ρ* 值。在 *α* = 1 时，我们的模型足够小，在 *ρ* = 1 时，计算足够快，我们更喜欢专注于实现最大精度。然而，当试图在旧的移动设备或嵌入式平台上运行时，这可能是有帮助的。

那么这个堆栈到底是如何工作的呢？深度学习经常因为是“黑箱”而受到指责，虽然它的许多组件可能是神秘的，但我们使用的网络经常泄露关于它们的一些魔法如何工作的信息。我们可以观察这个堆栈的各层，以及它们如何在特定的输入图像上激活，让我们了解每一层识别香肠、面包或其他特别突出的热狗特征的能力。



## 培养

数据质量至关重要。神经网络只能和训练它的数据一样好，提高训练集的质量可能是我们在这个项目中花费时间的前三件事之一。我们为改善这一状况所做的主要工作是:

*   获取更多的图像，以及更多不同的图像(高度/宽度、背景、照明条件、文化差异、视角、构图等)。)
*   将图像类型与预期的生产输入相匹配。我们的猜测是，人们通常会尝试拍摄真实的热狗和其他食物，或者有时会尝试用随机对象来欺骗系统，所以我们的数据集反映了这一点。
*   举很多类似的例子，这些例子可能会让你的关系网出错。一些看起来与热狗最相似的东西是其他食物(如汉堡、三明治，或者裸热狗、小胡萝卜甚至煮熟的樱桃番茄)。我们的数据集反映了这一点。
*   预计失真:在移动情况下，大多数照片会比用 DLSR 或在完美的光线条件下拍摄的“普通”照片差。手机照片昏暗，嘈杂，拍摄角度。积极的数据扩充是解决这一问题的关键。
*   此外，我们还发现用户可能无法访问真正的热狗，因此可能会尝试从谷歌搜索结果中为热狗拍照，这导致了热狗本身的失真(如果以某个角度拍摄，则会出现扭曲，使用移动相机在 LCD 屏幕上拍摄会导致屏幕上的闪光反射以及可见的莫尔效应)。这些特定的失真拥有一种几乎不可思议的能力来欺骗我们的网络，这和最近发表的关于回旋网络抗噪声能力的论文没有什么不同。使用 Keras 的渠道转移功能解决了其中的大部分问题。



Example distortion introduced by moiré and a flash. Original photo: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:NCI_Visuals_Food_Hot_Dog.jpg).



*   一些边缘病例很难被发现。特别是，用柔焦或背景中大量散景拍摄的热狗图像有时会欺骗我们的神经网络。这很难避免，因为 a)柔焦的热狗照片并不多(光是想想我们就饿了)，b)花太多的网络容量来训练柔焦可能是有害的，而实际上大多数用手机拍摄的图像都没有这个功能。因此，我们选择不去解决这个问题。

我们的数据集的最终组成是 150，000 张图片，其中只有 3，000 张是热狗:你可以看的热狗就这么多，但还有很多不是热狗的。49:1 的不平衡被认为是有利于热狗的 49:1 的 Keras 等级权重。在剩余的 147k 张照片中，大多数是食物，只有 3k 张非食物的照片，以帮助网络进行更多的概括，而不是被骗去看一个穿着红色衣服的人的照片。

我们的数据扩充规则如下:

*   我们应用了 135 度以内的旋转——明显高于平均值，因为我们对应用程序进行了编码，忽略了手机的方向。
*   高度和宽度偏移 20%
*   剪切范围为 30%
*   10%的缩放范围
*   20%的频道偏移
*   随机水平翻转有助于网络泛化

这些数字是根据实验和我们对应用程序实际使用情况的理解直观得出的，而不是仔细的实验。

我们数据管道的最后一个关键是为 Keras 使用 Patrick Rodriguez 的[多进程图像数据生成器](https://github.com/stratospark/keras-multiprocess-image-data-generator)。虽然 Keras 确实有一个内置的多线程和多进程实现，但我们发现 Patrick 的库在我们的实验中一直更快，原因我们没有时间研究。这个图书馆把我们的训练时间减少到原来的三分之一。

该网络使用 2015 MacBook Pro 和附加的外部 GPU (eGPU)进行训练，特别是英伟达 GTX 980 Ti(如果我们今天开始，我们可能会购买 1080 Ti)。我们能够一次用 128 幅图像来训练网络。该网络总共训练了 240 个时期，这意味着我们通过网络运行了所有 150，000 个图像 240 次。这花了大约 80 个小时。

我们分三个阶段训练网络:

*   阶段 1 运行了 112 个时期(7 个完整的 CLR 周期，步长为 8 个时期)，学习率在 0.005 和 0.03 之间，采用三角 2 策略(意味着最大学习率每 16 个时期减半)。
*   阶段 2 运行了 64 个以上的时期(4 个 CLR 周期，步长为 8 个时期)，学习率在 0.0004 和 0.0045 之间，基于三角形 2 策略。
*   阶段 3 运行了 64 个以上的时期(4 个 CLR 周期，步长为 8 个时期)，学习率在 0.000015 和 0.0002 之间，基于三角形 2 策略。



UPDATED: a previous version of this chart contained inaccurate learning rates.



虽然学习率是通过运行 CLR 论文建议的线性实验来确定的，但它们似乎直观上有意义，因为每个阶段的最大值都在之前最小值的 2 倍以内，这与行业标准建议一致，即如果您的准确度在训练期间处于平稳状态，您的学习率将减半。

为了节省时间，我们在运行 Ubuntu 的 [Paperspace](https://www.paperspace.com/ml) P5000 实例上进行了一些训练。在这些情况下，我们能够将批量增加一倍，并发现每个阶段的最佳学习率也大致增加一倍。

## 在手机上运行神经网络

即使已经设计了一个相对紧凑的神经架构，并训练它处理移动环境中可能出现的情况，我们还有很多工作要做，才能使它正常运行。试图运行开箱即用的顶级神经网络架构可能会快速消耗数百兆字节的 RAM，而今天很少有移动设备能够节省这些 RAM。除了网络优化之外，事实证明，您处理图像甚至加载 TensorFlow 本身的方式都会对您的网络运行速度、使用的内存量以及用户的无崩溃体验产生巨大影响。

这可能是这个项目中最神秘的部分。可以找到的关于它的信息相对较少，这可能是因为目前在移动设备上运行的深度学习应用程序很少。然而，我们必须赞扬 Tensorflow 团队，特别是 Pete Warden、Andrew Harp 和 Chad Whipkey 提供的现有文档以及他们在回答我们的询问时表现出的善意。

*   将我们的网络的权重四舍五入有助于将网络压缩到其大小的 25%。本质上，这种优化不是使用从训练中获得的任意股票值，而是选取 N 个最常见的值，并将网络中的所有参数设置为这些值，这样可以在压缩时极大地减小网络的大小。然而，这对未压缩的应用程序大小或内存使用没有影响。我们没有将这一改进应用到生产中，因为网络对于我们的目的来说已经足够小了，我们也没有时间来量化舍入对应用准确性的影响。
*   通过使用-Os 编译 TensorFlow 库以用于生产来优化它
*   从 TensorFlow lib 中删除不必要的操作:TensorFlow 在某种程度上是一个虚拟机，能够解释许多或任意的 TensorFlow 操作:加法、乘法、连接等。通过从为 ios 编译的 TensorFlow 库中删除不必要的 op，您可以[显著节省重量(和内存)](https://github.com/tensorflow/tensorflow/issues/9073#issuecomment-294384481)。
*   其他改进也是可能的。例如，作者的无关工作通过一个[相对简单的技巧](https://github.com/tensorflow/tensorflow/pull/7832)在 Android 二进制大小方面产生了 1MB 的改进，因此 TensorFlow 的 iOS 代码可能有更多区域可以根据您的目的进行优化。

我们没有在 iOS 上使用 TensorFlow，而是考虑使用苹果内置的深度学习库(BNNS、MPSCNN 以及后来的 CoreML)。我们将在 Keras 中设计网络，用 TensorFlow 训练它，导出所有权重值，用 BNNS 或 MPSCNN 重新实现网络(或[通过 CoreML](https://developer.apple.com/documentation/coreml/converting_trained_models_to_core_ml) 导入它)，并将参数加载到新的实现中。然而，最大的障碍是这些新的苹果库只能在 iOS 10+上使用，我们希望支持旧版本的 iOS。随着 iOS 10+的采用和这些框架的不断改进，在不久的将来可能不会有在设备上使用 TensorFlow 的情况。

## 通过动态注入神经网络来改变应用程序的行为

如果你认为动态地将 JavaScript 注入到你的应用程序中很酷，试着将神经网络注入到你的应用程序中！我们使用的最后一个制作技巧是利用 [CodePush](https://microsoft.github.io/code-push/) 和苹果相对宽松的服务条款，在提交到应用商店后，实时注入我们神经网络的新版本。虽然这主要是为了帮助我们在发布后快速向用户提供准确性改进，但可以想象，您可以使用这种方法来大幅扩展或更改您的应用程序的功能集，而无需再次通过 app store 审查。



## 我们会有什么不同

有很多事情没有成功，或者我们没有时间去做，这些是我们将来要研究的想法:

*   更仔细地调整我们的数据扩充参数。
*   端到端测量准确性，即应用程序抽象出的最终决定，如我们的应用程序是否有 2 个或更多类别，热狗识别的最终阈值是多少(如果识别高于 0.90，而不是默认的 0.5，我们最终让应用程序说“热狗”)，在权重四舍五入后，等等。
*   在应用中建立反馈机制——如果结果错误，让用户发泄沮丧，或者积极改善神经网络。
*   使用比 224 x 224 像素更大的分辨率进行图像识别，本质上是使用 MobileNets *ρ* 值> 1.0







T2】

# UX/DX，偏见和人工智能的恐怖谷

最后，我们不能不提用户体验、开发者体验和开发人工智能应用的固有偏见的明显而重要的影响。每个人可能都应该有自己的帖子(或自己的书)，但在我们的经验中，这三件事有非常具体的影响。

可以说，UX(用户体验)在人工智能应用开发的每个阶段都比传统应用**更为关键。**目前没有深度学习算法可以给你完美的结果，但在许多情况下，深度学习+ UX 的正确组合将导致难以区分的完美结果。当谈到让开发人员走上正确的道路来设计他们的神经网络，为用户使用应用程序设定适当的期望，以及优雅地处理不可避免的人工智能故障时，适当的 UX 期望是不可替代的。构建没有 UX 优先思维模式的人工智能应用程序就像训练没有随机梯度下降的神经网络:在构建完美人工智能用例的过程中，你最终会陷入[恐怖谷](https://en.wikipedia.org/wiki/Uncanny_valley)的局部最小值。



Source: [New Scientist](https://www.newscientist.com/article/dn28432-into-the-uncanny-valley-80-robot-faces-ranked-by-creepiness/).



**DX(开发者体验)**也是极其重要的，因为深度学习训练时间是等待你的程序编译时新的胡闹。我们建议您优先选择 DX(因此选择 Keras)，因为它总是可以为以后的运行优化运行时(手动 GPU 并行化、多进程数据增强、TensorFlow 管道，甚至为 caffe2 / pyTorch 重新实现)。



即使是像 TensorFlow 这样的 API 和文档相对迟钝的项目，也可以通过为训练和运行神经网络提供一个高度测试、高度使用、维护良好的环境来极大地改进 DX。

出于同样的原因，拥有自己的本地 GPU 进行开发的成本和灵活性都是难以超越的。能够在本地查看/编辑图像，用你喜欢的工具毫无延迟地编辑代码，极大地提高了开发质量和构建人工智能项目的速度。

大多数人工智能应用程序会遇到比我们更严重的文化偏见，但作为一个例子，即使是我们简单的用例，也会因为我们初始数据集中的固有偏见而让我们措手不及，这使得应用程序无法识别法式热狗、亚洲热狗和更多我们没有直接亲身体验的奇怪食物。重要的是要记住，人工智能并不比人类做出“更好”的决定——通过人类提供的训练集，它们会受到与我们一样的人类偏见的影响。



T2】

***感谢:*** *迈克·詹郅、亚力克·博格、克雷·塔沃、托德·银色啤酒杯乐队、乔纳森·多坦、丽莎·肖马斯、艾米·索罗门、多萝西·斯特里特&里奇·托扬，以及这部剧的所有编剧——没有他们，这款应用根本就不会存在。
梅根、黛娜、大卫、杰伊以及 HBO 的所有人。规模创业伙伴& GitLab。雷切尔·托马斯和杰瑞米·霍华德&感谢他们教给我的一切，感谢他们善意地审阅了这篇文章的草稿。看看他们的* [*免费在线深度学习课程*](http://course.fast.ai/) *，太牛逼了！JP Simard 在 iOS 上的帮助。最后，TensorFlow 团队&*[*r/machine learning*](https://www.reddit.com/r/MachineLearning/)*为他们提供帮助&灵感。*

*…也感谢所有使用过&的人分享* [*这个 app*](https://www.seefoodtechnologies.com/nothotdog/) *！这让连续几个月盯着热狗的照片看完全值得😅*