# 用机器写作

> 原文:[https://www.robinsloan.com/notes/writing-with-the-machine/?UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://www.robinsloan.com/notes/writing-with-the-machine/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

<main>

# 用机器写作

我做了一些新东西:一个由人工智能语言模型提供内联文本提示的 plu gin。

![rnn-writer example](../Images/a3a4b5b1da833c41e05a6f539a1c3b55.png)

建造这个感觉就像在玩乐高，除了不是塑料砖块，我是在把方便包装的人类智慧和努力的砖块拼在一起。

一个模块:一个递归神经网络，深度学习热潮的成果，能够建模和生成具有怪异逼真度的字符序列。啪！

另一个障碍是:强大的完全可扩展的文本编辑器。啪！

Together:响应迅速、内嵌的“自动完成”功能，由一个熟悉老科幻故事的 RNN 驱动。

如果我不得不提供一个非常精彩的分析(我确实这么做了)，我会说这就像是在你的肩膀上写着一个疯狂但却非常好读的故事。任何时候你觉得有足够的勇气提出建议，你就按下`tab`，然后…

![rnn-writer example](../Images/6b96816e9fa1d0314c1e2bf01ac76e63.png)

如果您想亲自尝试，代码现在可以使用，分为两部分:

*   [`torch-rnn-server`](https://github.com/robinsloan/torch-rnn-server) 是运行神经网络的服务器，接受文本片段，并返回该文本的“完成”。事实上，它只是贾斯廷·约翰逊不可或缺的`torch-rnn`项目下的几个小垫片。
*   [`rnn-writer`](https://github.com/robinsloan/rnn-writer) 是 Atom 文本编辑器的一个包，它知道如何与`torch-rnn-server`对话，并向用户呈现它的完成。我还提供了一个 API 给那些想尝试这个，但是不想运行本地服务器的人。

您可以在各自的 Github 页面上找到这两个工具的说明，如果您对其中任何一个有异议，请随时提出问题或给我写信。

主要是，我想分享这些链接，但只要我在这里，我会增加一些东西:首先是动机的说明，然后是对深度学习场景的观察，最后是科幻小说的链接。

## 愿景

从我第一次使用 [`torch-rnn`](https://github.com/jcjohnson/torch-rnn) 项目开始，在命令行上进行傻乎乎的/怪异的文本模仿时，我就被一个在文本编辑器中正常打字的画面——<wbr>几乎——<wbr>击垮了，然后通过一个按键获得 RNN 的帮助。(当我说“帮助”的时候，我的意思是:少一些俏皮话，多一些安静。)

在闲逛了几个星期并学习了两种新的程序语言的百分之五之后，我把积木搭在了一起；RNN 训练有素；愿景实现了。然后我玩它的第一个小时就完全泄气了。*嗯。没有我想象的那么酷。*

在任何项目中，这都是一个不可避免的情感状态，而且很可能是一个关键的状态。

随着我和`rnn-writer`相处的时间越来越长，我的观点已经— <wbr> 呃—  <wbr>有些膨胀了。我只是被文本编辑器的概念搞得晕头转向，它拥有一个深刻的、细致入微的模型……什么？你写的所有东西？你最喜欢的作家写的？你的仇人？《纽约客》的所有员工，现在的和过去的？网上的每一个？无论怎么看都是挑衅。

我应该明确地说:我绝对 100%不是在谈论一个“为你写作”的编辑，无论那是什么意思。这个世界不需要更多没有视力的机器人短信了。

这里最重要的想法是增强；伙伴关系；呼叫和响应。

目标不是让写作“更容易”；这是为了增加难度。

目标不是使文本的结果“更好”；这是为了让它变得*不同*——更奇怪，用其他方法可能无法达到的效果。

我在这里分享的工具不能实现这个目标；它们的效果还不足以抵消使用它们所需要的努力。但是！我想他们能到达那里！如果这个项目有任何超越怪异乐趣的条件，我认为它可能是一个简单的技巧，让安婷·RNN 离开命令行，进入文本编辑器，在那里它的输出变成一些你可以真正使用的东西。

## Deep scenius

像任何与技术相关的人一样，多年来我一直在阅读关于深度学习的书籍，但直到今年早些时候与一位老朋友(他对这些技术非常兴奋)的一次长时间交谈，我才感到有动力挖掘自己。我不得不报告:在一个值得注意的时刻，这真是一个非凡的社区。跟踪 Arxiv 上的论文、Github 上的项目和 Twitter 上的帖子，你会感觉到一群人几乎绊倒在自己身上去做下一件事——推动艺术的发展。

这都是由一个强大的(最近？)文化的清晰解释。我兴奋的朋友声称，这对于深度学习的崛起来说，就像(更普遍讨论的)快速 GPU 和大型数据集的可用性一样重要。我自己也从那种文化中得到了启发，对我来说，这似乎是一个合理的论点，也是认识到的一件重要的事情。

以下是我发现特别有用的一些资源:

## 149，326，361 个字符

深度学习领域的大部分精力都集中在我称之为“一般性”的问题上，这些问题的解决方案对很多人都非常有用:图像识别、语音识别、语义翻译……你明白了吧。这些问题中的许多都有相关的基准挑战，如果你的模型得到了比在位冠军更好的分数，你知道你做了一些有价值的事情。这些挑战都依赖于标准数据集。而这些— <wbr> 数据集—  <wbr> 则—  <wbr> *极其*枯燥。

因此，将深度学习场景的来之不易的技术技巧应用于怪异/有趣的对象的很大一部分工作(和乐趣)是跟踪非标准、非无聊的数据集。对我来说，关于文本内容的收集和处理的决定比关于 RNN 的设计和后续训练的决定更有把握。

我用过最多的语料库来自互联网档案馆的[纸浆杂志档案馆](https://archive.org/details/pulpmagazinearchive) : 150MB 的*银河*和 *IF Mag a zine* 。它非常嘈杂，有大量的 OCR 错误和大量的广告夹杂在科幻故事中，但*哇*有很多文本，RNN 似乎在这方面茁壮成长。我对它进行了简单的处理和规范化，合并后的语料库— <wbr>现在只是一个巨大的文本文件，没有一个单独的换行—<wbr>[可以在互联网上查阅。](https://archive.org/details/scifi-corpus)

所以，总结一下:

![](../Images/1520b2726613c3f97ccfa717ff997606.png)

啪。啪。啪！

2016 年 5 月，奥克兰

</main>