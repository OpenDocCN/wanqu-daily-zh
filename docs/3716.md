# 机器如何学习偏见——科学美国人

> 原文:[https://www . scientific American . com/article/how-a-machine-learning-prediction/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

如果人工智能接管我们的生活，它可能不会涉及人类与机器人军队的战斗，这些机器人无情地应用斯波克式的逻辑，因为它们在物理上奴役我们。相反，已经让人工智能程序推荐你喜欢的电影或在照片中识别你朋友的脸的机器学习算法，可能会在某一天拒绝你的贷款，带领警察到你的社区或告诉你的医生你需要节食。由于人类创造了这些算法，它们同样容易产生可能导致错误决策和更糟糕结果的偏见。

这些偏见造成了一些对我们越来越依赖人工智能技术的直接担忧，因为任何由人类设计的绝对“中立”的人工智能系统仍然可能加强人类的偏见思维，而不是看穿它。例如，执法官员已经受到了批评，因为他们使用了[计算机算法，据称这些算法将黑人被告标记为更有可能在未来犯罪](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，尽管该程序并没有明确考虑种族因素。

主要问题有两个方面:首先，用于校准机器学习算法的数据有时不足，其次，算法本身可能设计得很差。机器学习是软件开发人员使用与手头任务相关的大量数据来训练人工智能算法的过程。最终，该算法在最初提供的数据中发现模式，使其能够识别新数据中的类似模式。但是这并不总是按计划进行，结果可能是可怕的。例如，2015 年 6 月，谷歌的照片分类系统将两名非洲裔美国人识别为“大猩猩”。该公司很快修复了这个问题，但微软人工智能研究员 Kate Crawford 在《纽约时报》的专栏文章中指出，这个错误反映了人工智能中更大的“白人问题”。也就是说，用于训练软件的数据过于依赖白人的照片，削弱了它准确识别不同特征的人的图像的能力。

最近淹没脸书用户新闻源的大量假新闻也凸显了人工智能偏见问题。脸书的趋势新闻算法是根据参与度——用户点击或分享的频率——对新闻进行优先排序。完全不考虑真实性。11 月初，几家新闻媒体透露，一群马其顿青少年[愚弄了脸书的新闻馈送](https://www.buzzfeed.com/craigsilverman/how-macedonia-became-a-global-hub-for-pro-trump-misinfo)算法，在美国大选期间宣传吸引右翼选民的明显虚假的故事。脸书表示，自那以后它已经修改了算法，并宣布了计划，让 [Snopes](http://www.snopes.com/) 、[Factcheck.org](http://factcheck.org/)、 [ABC News](http://abcnews.go.com/) 和 [PolitiFact](http://www.politifact.com/) 帮助剔除明显虚假的文章。

“这有点像‘俄罗斯坦克问题’，”马里兰大学计算机科学副教授 Hal Daumé III 说。这个传说是虚构的，但很有说明性，并且经常被计算机科学教师提起，可以追溯到 20 世纪 80 年代机器学习的早期。这篇报道说，美国军方试图训练一台计算机来区分照片中的俄罗斯和美国坦克。“他们获得了超高的分类精度——但俄罗斯坦克的所有照片都很模糊，而美国坦克是高清的，”多姆解释道。该算法没有识别坦克，而是学会了区分粒状和高质量的照片。

尽管存在这些已知的局限性，一组研究人员最近发布了一项研究，声称一种算法可以通过评估面部特征来推断某人是否是一名被定罪的罪犯。中国上海交通大学的研究人员吴晓林和张熙在 1856 张人脸照片的数据集上训练了一个机器学习算法，其中包括 730 名被定罪的罪犯和 1126 名非罪犯。在查看了 90%的图片后，人工智能能够正确地识别出剩余 10%中的哪些是被定罪的罪犯。

根据研究，该算法将特定面部特征与犯罪行为[相关联。吴说，例如，犯罪分子更有可能在眼角、嘴唇弯曲和鼻尖的位置之间存在某种空间关系——尽管他指出，拥有其中任何一种关系都不一定表明一个人更有可能是犯罪分子。吴还发现，罪犯的脸彼此差异更大，而非罪犯的脸往往具有相似的特征。](https://arxiv.org/abs/1611.04135)

吴继续使用一组以前没有见过的不同照片来测试该算法，并发现它可以正确地识别罪犯。研究人员试图通过只使用没有胡须或疤痕的年轻或中年中国男性的面部来训练和测试他们的算法，以避免偏见。

“我开始证明相面术是错误的，”吴说，他指的是基于面部特征评估性格的百年伪科学。"我们对结果感到惊讶。"虽然这项研究似乎证实了面相的某些方面，但吴承认，使用这种技术从警察队列中挑出一个人是“疯狂的”，并表示没有任何执法应用的计划。

其他科学家说，吴和张的发现可能只是加强了现有的偏见。谷歌研究机器学习的首席科学家 Blaise Agüera y Arcas 指出，受试者的犯罪行为是由人类做出(可能是下意识的)偏见决定的当地司法系统决定的。这篇论文的核心问题是，它依赖这个系统“作为给罪犯贴标签的基本事实，然后得出结论，认为由此产生的(机器学习)在人类判断方面是无偏见的，”Agü era y Arcas 补充道。

吴和他的同事“马上得出结论，他们发现了自然界的一个潜在模式——面部结构预示着犯罪。华盛顿学院研究计算机视觉的数学助理教授凯尔·威尔逊说:“这真是一个鲁莽的结论。威尔逊还表示，这种算法可能只是反映了一个特定司法系统中人类的偏见，可能会在任何其他国家做同样的事情。“同样的数据和工具可以用来更好地理解刑事司法系统中基于外表的(人类)偏见，”威尔逊说。"取而代之的是，他们教会了一台计算机重现人类同样的偏见."

还有人说，这项技术可以通过计算计算机学习模式中的错误来加以改进，以试图排除人类的偏见。瑞士人工智能实验室 Dalle Molle Institute for Artificial Intelligence 的科学主任 Jürgen Schmidhuber 说，人工智能系统在学习时会犯错误——事实上它必须犯错误，这就是为什么它被称为“学习”。他指出，计算机只会在数据允许的情况下学习。“你不能消除所有这些偏见的来源，就像你不能为人类消除这些来源一样，”他说。但他补充说，承认这一点，然后确保使用好的数据并很好地设计任务是可能的；问正确的问题至关重要。或者，记住一个老程序员的话:“垃圾进，垃圾出。”