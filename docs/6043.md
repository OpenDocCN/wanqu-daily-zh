# 为了削弱人工智能，黑客们正在利用数据对抗自己

> 原文:[https://www . wired . co . uk/article/artificial-intelligence-hacking-machine-learning-adversarial？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://www.wired.co.uk/article/artificial-intelligence-hacking-machine-learning-adversarial?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

加州大学伯克利分校计算机科学教授 Dawn Song 说:“对立的例子只是说明我们对深度学习如何工作及其局限性的理解仍然非常有限。宋是四所大学的几名研究人员之一，他们开发了停车标志贴纸，以迷惑无人驾驶汽车。

“根据攻击者处于机器学习模型生成管道的哪个阶段，会有一系列的攻击，”华盛顿大学的计算机安全研究员 Earlence Fernandes 说，他从事停止标志的研究。费尔南德斯说，例如，训练时间攻击发生在建立机器学习模型时，恶意数据被用来训练系统。“在面部检测算法中，攻击者可以毒化模型，使其将攻击者的面部识别为授权人，”他说。

另一方面，推理时间攻击使用一系列算法向模型显示特制的输入——快速梯度符号方法或 Carlini 和 Wagner 攻击是两种流行的方法——巧妙地改变图像以混淆神经网络。

*阅读更多: [DeepMind 的穆斯塔法·苏莱曼:2018 年，人工智能将获得一个道德指南针](https://www.wired.co.uk/article/mustafa-suleyman-deepmind-ai-morals-ethics)*

随着人工智能渗透到我们生活的方方面面——驾驶汽车、分析闭路电视系统、通过面部识别识别身份——对这些系统的攻击变得更加可能和危险。黑客修改路边的家具可能会导致车祸和伤害。机器学习系统所学习的数据的细微变化也可能导致偏见被主动添加到人工智能系统所做的决策中。

但是我们不应该担心。还没有。“据我们所知，这种类型的攻击目前并没有在现实世界中由恶意方实施，”麻省理工学院的研究员 Anish Athalye 说。“但鉴于这一领域的所有研究，似乎许多机器学习系统都非常脆弱，如果现实世界的系统容易受到这种攻击，我不会感到惊讶。”

Athalye 自己的研究旨在使对抗性攻击更加强大。一些被归类为“标准”的攻击只从特定的角度起作用，而其他攻击则不管神经网络从什么角度看对象或图像都起作用。“标准的对抗性例子是通过稍微调整图像中的像素来将分类转向某个目标类别而创建的——让一张猫的照片被归类为鳄梨色拉酱，”他说。“一次又一次地重复这一过程，做出微小的改变，结果证明，有可能制作出一幅图像，对一个人来说看起来是一回事，但却让机器误以为它完全是另一回事。”他说，研究表明，标准的对抗性攻击是“脆弱的”，在现实世界中不太可能成立。

因此，麻省理工学院和 LabSix 的 Athalye 及其同事等研究人员构建了更好的例子，优化了攻击图像，使其无论角度或距离都有效。“我们还将这种方法扩展到 3D 物体，因此你可以拥有一个看起来像乌龟的物理物体，例如，对人类来说，但对机器来说，无论如何感知，它看起来都完全不同，”他说。这包括他的 3D 打印玩具乌龟，在 ImageNet 分类器看来它像一支步枪。

如果攻击只在一个精确的角度起作用，或者干扰很容易被人类发现，那么攻击就没什么用。考虑一下自动驾驶汽车:它们通过依赖神经网络识别物体的计算机视觉来看世界。任何对抗性的技巧都必须在汽车接近的每个角度都起作用，无论是远距离还是近距离，也不能被人类司机注意到——没有人能读懂只是被油漆覆盖的标志。包括 Fernandes 和 Song 在内的研究人员利用不模糊标志的细微油漆标记和看起来像涂鸦的贴纸来解决这一问题，但导致神经网络将“停止”解释为限速。