# 规模要素:构建和扩展数据平台——高可伸缩性——

> 原文：<http://highscalability.com/blog/2015/5/4/elements-of-scale-composing-and-scaling-data-platforms.html?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website>

*这是 [Ben Stopford](https://twitter.com/benstopford) 关于[规模元素:构成和扩展数据平台](http://www.benstopford.com/2015/04/28/elements-of-scale-composing-and-scaling-data-platforms/)的史诗帖子的嘉宾转帖。通过进化力量塑造系统如何适应关键挑战的大师之旅。*

作为软件工程师，我们不可避免地会受到周围工具的影响。语言、框架、甚至过程都会塑造我们构建的软件。

同样，数据库已经走过了一条非常特殊的道路，不可避免地会影响我们处理可变性和共享应用程序状态的方式。

在过去的十年里，我们探索了如果我们选择了不同的道路，世界会是什么样子。小型开源项目尝试不同的想法。这些生长。他们和其他人一起组成。由此产生的平台利用工具套件，每个组件通常利用一些基本硬件或系统效率。结果，解决问题的平台过于笨拙或过于具体，无法在任何单一工具中工作。

因此，今天的数据平台在复杂性方面差异很大。从简单的缓存层或多语言持久化到完全集成的数据管道。有很多路径。他们去许多不同的地方。至少在这些地方的一些地方，美好的事物被发现。

所以这次演讲的目的是解释这些流行的方法是如何工作的以及为什么会工作。为此，我们将首先考虑组成它们的构件。这些是我们需要的直觉，以便在以后整合更大的东西。

从某种抽象的意义上来说，当我们处理数据时，我们实际上只是在安排位置。CPU 的局部性。我们需要的其他数据的位置。顺序访问数据是其中的一个重要组成部分。计算机只是擅长顺序操作。可以预测顺序操作。

如果你按顺序从磁盘获取数据，它将被预取到磁盘缓冲区、页面缓存和不同级别的 CPU 缓存中。这对性能有很大的影响。但它对随机寻址数据没有什么帮助，不管是在主存中，在磁盘上还是在网络上。事实上，预取实际上会阻碍随机工作负载，因为各种缓存和前端总线会填充不太可能使用的数据。

虽然磁盘以其缓慢的性能而闻名，但人们通常认为主存很快。这并不像人们通常认为的那样普遍正确。随机和顺序主存工作负载之间有一到两个数量级。使用一种为你管理记忆的语言，事情通常会变得更糟。

从磁盘顺序传输数据实际上比随机寻址的主内存要好。因此，磁盘可能并不总是像我们想象的那样像乌龟一样，至少如果我们能够安排顺序访问，就不会这样。SSD，尤其是那些利用 PCIe 的 SSD，进一步使情况复杂化，因为它们[展示了](http://www.benstopford.com/ssd-performance-2015/)不同的权衡，但无论如何，两种访问模式的缓存优势仍然存在。

让我们想象一下，作为一个简单的思维实验，我们想要创建一个非常简单的数据库。我们将从基础开始:一个文件。

我们希望保持写入和读取顺序，因为它与硬件配合得很好。我们可以有效地将写入内容添加到文件的末尾。我们可以通过扫描整个文件来阅读。当数据流通过 CPU 时，我们希望做的任何处理都可以发生。我们可能会过滤、聚合甚至做一些更复杂的事情。世界是我们的！

那么，发生变化、更新等的数据呢？

我们有两个选择。我们可以就地更新该值。为此，我们需要使用固定宽度的字段，但是对于我们的小思想实验来说，这是可以的。但是就地更新意味着随机 IO。我们知道这对性能不好。

或者，我们可以将更新附加到文件的末尾，并在回读时处理被取代的值。

所以我们有了第一个权衡。添加到“日志”或“日志”中，并获得顺序访问的好处。或者，如果我们使用就地更新，我们将回到每秒 300 次左右的写入，假设我们实际上刷新到底层介质。

当然，在实际操作中，完整地读取文件会非常慢。我们只需要获取数 GB 的数据，最快的磁盘只需几秒钟。这就是数据库在结束表扫描时所做的事情。

此外，我们经常需要一些更具体的东西，比如说名为“bob”的客户，所以扫描整个文件可能有些多余。我们需要一个索引。

现在我们可以使用许多不同类型的索引。最简单的是一个固定宽度值的有序数组，在本例中是客户名称，用堆文件中相应的偏移量保存。可以用二分搜索法搜索有序阵列。我们当然也可以使用某种形式的树、位图索引、散列索引、术语索引等。这里我们描绘了一棵树。

这类指数的特点是它们强加了一个总体结构。这些值被特意排序，以便我们在需要读取时可以快速访问它们。总体结构的问题在于，随着数据的流入，它需要随机写入。因此，我们出色的、写优化的、仅附加文件必须通过分散文件系统的写来扩充。这会让我们慢下来。

任何在数据库表上放了很多索引的人都会熟悉这个问题。如果我们使用常规的旋转硬盘驱动器，如果我们以这种方式维护索引的磁盘完整性，我们的运行速度可能会慢 1000 倍。

幸运的是，有一些方法可以解决这个问题。这里我们要讨论三个。这些代表了三个极端，它们实际上是真实世界的简化，但是当我们考虑更大的构图时，这些概念是有用的。

我们的第一个选择是简单地将索引放在主存中。这将区分随机写入 RAM 的问题。堆文件保留在磁盘上。

对于我们的随机写入问题，这是一个简单有效的解决方案。它也是许多真实数据库使用的方法。MongoDB、Cassandra、Riak 和许多其他公司都使用这种类型的优化。通常使用内存映射文件。

然而，如果我们的数据远远超过我们的主存，这个策略就失效了。这在有许多小物体的地方尤其明显。我们的索引会变得非常大。因此，我们的存储空间受到可用主存容量的限制。对于许多任务来说，这很好，但是如果我们有大量的数据，这可能是一个负担。

一个流行的解决方案是摆脱单一的“总体”指数。取而代之的是，我们使用一些更小的。

这是一个简单的想法。当写入数据进来时，我们在主存中进行批处理。一旦我们有了足够的数据——比如几 MB——我们就对它们进行排序，并将它们作为一个单独的迷你索引写入磁盘。我们最终得到的是一系列小的、不可变的索引文件。

那么这样做的目的是什么呢？我们的不可变文件集可以按顺序流式传输。这将我们带回到一个快速写入的世界，而不需要将整个索引保存在内存中。不错！

当然，这种方法也有不好的一面。当我们阅读时，我们必须单独查阅许多小索引。*所以我们真正做的是将随机问题从写入转移到读取上*。然而，在许多情况下，这被证明是一个很好的权衡。优化随机读取比优化随机写入更容易。

在内存中保留一个小的元索引或使用 Bloom Filter 提供了一种低内存的方法来评估在读操作期间是否需要查询单个索引文件。这为我们提供了与单个总体索引几乎相同的读取性能，同时保持了快速、连续的写入。

实际上，我们偶尔也需要清除孤立的更新，但这可以通过良好的顺序读写来完成。

我们所创建的被称为[日志结构的合并树](http://www.benstopford.com/2015/02/14/log-structured-merge-trees/)。一种在 HBase、Cassandra、Google 的 BigTable 和许多其他大数据工具中使用的存储方法。它以相对较小的内存开销平衡了读写性能。

因此，我们可以通过将索引存储在内存中，或者使用像 LSM 这样的写优化索引结构，来规避“随机写惩罚”。不过，还有第三种方法。纯蛮力。

回想一下我们最初的文件示例。我们可以完整地阅读它。在如何处理其中的数据方面，这给了我们许多选择。强力方法就是按列而不是按行保存数据，并通过 CPU 完整地只传输查询所需的列。这种方法称为列或列导向。

(应该注意的是，在真正的列存储和遵循大表模式的存储之间有一个不幸的命名冲突。虽然它们有一些相似之处，但实际上它们是完全不同的。明智的做法是将它们视为不同的事物。)

列方向是另一个简单的想法。我们不是将数据存储为附加到单个文件中的一组行，而是按列拆分每一行。然后，我们将每一列存储在一个单独的文件中。

我们保持文件的顺序不变，所以第 N 行在每个列文件中有相同的位置(偏移量)。这很重要，因为我们需要同时读取多个列来服务一个查询。这意味着动态地“连接”列。如果这些列的顺序相同，我们可以在一个紧密的循环中完成，这样缓存和 cpu 效率都非常高。许多实现大量使用矢量化来进一步优化简单连接和过滤操作的吞吐量。

写入可以利用仅附加的优势。不利的一面是，我们现在有许多文件需要更新，每次写入数据库时，每一列都要更新一个文件。对此最常见的解决方案是以类似于上述 LSM 方法中使用的方式批量写入。许多列式数据库还将表作为一个整体进行整体排序，以提高一个所选键的读取性能。

通过按列分割数据，我们显著减少了需要从磁盘中取出的数据量，只要我们的查询在所有列的子集上操作。

除此之外，单列中的数据通常压缩得很好。如果我们知道列的数据类型，我们可以利用它来做到这一点。这意味着我们可以经常使用高效、低成本的编码，如游程长度、增量、位打包等。对于某些编码，谓词也可以直接用于未压缩的流。

其结果是一种强力方法，对于需要大量扫描的操作特别有效。average、max、min、group by 等聚合函数就是典型的例子。

这与我们前面提到的使用“堆文件&索引”方法非常不同。理解这一点的一个好方法是问自己:像这样的列式方法与“堆和索引”之间有什么区别，在“堆和索引”中，索引被添加到每个字段中？

这个问题的答案在于索引文件的排序。BTrees 等将根据它们索引的字段进行排序。将两个索引中的数据连接起来，一方面涉及到流操作，另一方面索引查找必须读取第二个索引中的随机位置。这通常比联接两个保持相同顺序的索引(列)效率低。我们再次利用顺序存取。

因此，我们可能希望在数据平台中用作组件的许多最佳技术将利用这些核心效率中的一个来超越特定的工作负载集。

将索引存储在内存中，而不是堆文件中，这是许多 NoSQL 存储所青睐的，比如 Riak、Couchbase 或 MongoDB 以及一些关系数据库。这是一个简单的模型，效果很好。

设计用于处理大型数据集的工具倾向于采用 LSM 方法。使用基于磁盘的结构，这为它们提供了快速接收和良好的读取性能。HBase、Cassandra、RocksDB、LevelDB 甚至 Mongo 现在都支持这种方式。

每文件列引擎在 Redshift 或 Vertica 等 MPP 数据库以及使用 Parquet 的 Hadoop 堆栈中大量使用。这些是需要大量遍历的数据处理问题的引擎。聚合是这些工具的发源地。

Kafka 等其他产品将简单、硬件高效的契约应用于消息传递。最简单地说，消息传递只是附加到一个文件，或者从一个预定义的偏移量读取。您从偏移量读取消息。你走开。你回来了。您从先前完成的偏移量开始读取。所有好的顺序 IO。

这不同于大多数面向消息的中间件。像 JMS 和 AMQP 这样的规范需要添加如上所述的索引来管理选择器和会话信息。这意味着它们最终表现得更像数据库而不是文件。吉姆·格雷在他 1995 年的出版物《队列是数据库》中提出了这个著名的观点。

因此，所有这些方法都倾向于这种或那种折衷，通常保持事情简单，硬件支持，作为一种扩展的方式。

我们已经介绍了存储引擎的一些核心方法。事实上，我们做了一些简化。现实世界要复杂一点。但是这些概念仍然是有用的。

然而，扩展数据平台不仅仅是存储引擎。我们需要考虑并行性。

当在许多机器上分发数据时，我们有两个核心原语可以使用:分区和复制。分区，有时称为分片，适用于随机访问和强力工作负载。

如果使用基于散列的分区模型，数据将使用众所周知的散列函数分布在许多机器上。这类似于哈希表的工作方式，每个桶保存在不同的机器上。

结果是，任何值都可以通过哈希函数直接进入包含数据的机器来读取。这种模式具有极好的可伸缩性，并且是唯一一种随着客户端请求数量的增加而呈现线性可伸缩性的模式。请求被隔离到一台机器上。每一个都将由集群中的一台机器提供服务。

我们还可以使用分区来提供批量计算的并行性，例如聚合函数或更复杂的算法，如我们可能用于聚类或机器学习的算法。关键的区别在于，我们以广播的方式同时锻炼所有的机器。这允许我们使用分而治之的方法，在更短的时间内解决一个大的计算问题。

批处理系统适用于大型问题，但是提供的并发性很少，因为它们在执行时会耗尽集群上的资源。

所以这两个极端很简单:一端是定向接入，另一端是广播、分而治之。我们需要小心的是位于两者之间的中间地带。这方面的一个很好的例子是在 NoSQL 商店中跨多台机器使用二级索引。

辅助索引是不在主键上的索引。这意味着数据不会按照索引中的值进行分区。经由散列函数的定向路由不再是一种选择。我们必须向所有机器广播请求。这限制了并发性。每个查询都必须涉及到每个节点。

由于这个原因，许多关键价值商店抵制了添加二级索引的诱惑，尽管它们有明显的用途。HBase 和伏地魔就是这样的例子。但是许多其他人确实暴露了他们，MongoDB，Cassandra，Riak 等等。这很好，因为二级索引很有用。但是理解它们对系统整体并发性的影响是很重要的。

![](img/a970147b084477f6caa04640d7ac98ab.png)

解决这个并发瓶颈的方法是复制。您可能已经熟悉了使用异步从数据库或复制的 NoSQL 商店(如 Mongo 或 Cassandra)的复制。

实际上，副本可以是不可见的(仅用于恢复)、只读的(增加读并发性)或读写的(增加分区容差)。你选择哪一个将与系统的一致性相权衡。这简直就是 cap 定理的应用(虽然 CAP 定理也[可能没有](http://blog.thislongrun.com/2015/04/cap-availability-high-availability-and_16.html)你想的那么简单)。

这种与一致性*的权衡给我们带来了一个重要的问题。什么时候一致性很重要？

一致性是昂贵的。在数据库世界中，ACID 是由可串行化来保证的。这实质上是确保所有操作都按顺序进行。事实证明这是一件非常昂贵的事情。事实上，它非常禁止，许多数据库根本不提供它作为隔离级别。那些不这样做的永远不会将其设置为默认设置。

可以说，如果您对一个进行分布式写操作的系统应用强一致性，您很可能会陷入乌龟的领地。

(*注意一致性一词有两种常见用法。酸中的碳和帽中的碳。不幸的是，它们并不相同。我使用 CAP 定义:所有节点同时看到相同的数据)

一致性问题的解决方案很简单。避免它。如果您无法避免，请将它隔离到尽可能少的编写人员和机器上。

避免一致性问题通常很容易，尤其是当您的数据是不可变的事实流时。一组网络日志就是一个很好的例子。它们没有一致性问题，因为它们只是永不改变的事实。

然而，还有其他一些用例需要一致性。在账户之间转账是一个经常使用的例子。应用折扣代码等非交换行为是另一个例子。

但是，从传统意义上来说，那些看起来需要一致性的事情往往并不需要。例如，如果一个动作可以从一个突变变成一组新的相关事实，我们就可以避免易变状态。考虑将交易标记为潜在欺诈。我们可以用新的字段直接更新它。或者，我们可以简单地使用链接回原始事务的单独的事实流。

![](img/b0a4aab4347d02990b2c64f792348614.png)

因此，在数据平台中，要么完全消除一致性需求，要么至少将其隔离，这是非常有用的。隔离的一种方法是使用单编写器主体，这可以让您了解一些情况。Datomic 就是一个很好的例子。另一种方法是通过分割可变和不可变的世界来物理隔离一致性需求。

像 [Bloom/CALM](http://www.bloom-lang.net/calm/) 这样的方法进一步扩展了这一思想，默认采用无序的概念，只在必要时强加秩序。

这些是我们需要考虑的一些基本权衡。现在，我们如何将这些东西整合在一起，构建一个数据平台？

![](img/524216584e1099a7b821044bbe575c04.png)

典型的应用程序架构可能如下所示。我们有一套将数据写入数据库并再次读取的过程。这对于许多简单的工作负载来说很好。许多成功的应用程序都是用这种模式构建的。但我们知道，随着吞吐量的增长，它的效果会越来越差。在应用程序领域，这是一个我们可能用消息传递、参与者、负载平衡等来解决的问题。

另一个问题是这种方法将数据库视为一个黑盒。数据库是聪明的软件。它们提供了大量的特性。但它们几乎没有提供走出酸性世界的机制。这在很多方面都是好事。我们默认为安全。但是当扩展被一般的保证所抑制时，这就变得很烦人了，这对于我们的需求来说可能是多余的。

最简单的方法是 [CQRS](http://martinfowler.com/bliki/CQRS.html) (命令查询责任分离)。

另一个非常简单的想法。我们将读取和写入工作负载分开。写操作会进行写优化。更接近于一个简单的日志文件。读取来自于读取优化的东西。有许多方法可以做到这一点，可以是用于关系技术的 Goldengate 工具，也可以是内部集成复制的产品，如 MongoDB 中的副本集。

许多数据库在幕后做类似的事情。德鲁伊就是一个很好的例子。Druid 是一个开源的、分布式的、时间序列的、柱状的分析引擎。如果我们以大块的形式输入数据，列存储效果最好，因为数据必须分布在许多文件中。为了获得良好的写性能，Druid 将最近的数据存储在一个写优化的存储中。随着时间的推移，这逐渐被移植到读取优化存储中。

当查询 Druid 时，查询路由到写优化和读优化组件。结果被组合(“简化”)并返回给用户。德鲁伊使用标记在每个记录上的时间来决定排序。

像这样的复合方法在单一抽象背后提供了 CQRS 的好处。

另一个类似的方法是使用[操作/分析桥](http://www.benstopford.com/2015/04/07/upside-down-databases-bridging-the-operational-and-analytic-worlds-with-streams/)。使用事件流将读写优化视图分开。状态流被无限期保留，因此异步视图可以在以后通过重放进行重组和扩充。

所以前面的部分提供了同步读写。这可以简单到立即读取写入的数据，也可以复杂到支持 ACID 事务。

后端利用异步性和不可变状态的[优势，通过复制、反规范化甚至完全不同的存储引擎来扩展离线处理。消息传递桥将两者连接在一起，允许应用程序监听流经平台的数据。](http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf)

作为一种模式，这非常适合中等规模的部署，其中至少有一部分不可避免的可变视图需求。

如果我们正在为一个不变的世界而设计，那么更容易接受更大的数据集和更复杂的分析。Hadoop 堆栈中几乎无处不在的批处理管道就是典型的例子。

Hadoop 堆栈的美妙之处在于它有太多的工具。无论您想要快速读写访问、廉价存储、批处理、高吞吐量消息还是用于提取、处理和分析数据的工具，Hadoop 生态系统都能满足您的需求。

批处理管道架构从几乎任何来源提取数据，无论是推还是拉。将其吸收到 HDFS，然后进行处理，以提供原始数据的优化版本。数据可能会被丰富、清理、反规范化、聚合、移动到读取优化格式(如 Parquet)或加载到服务层或数据集市。在整个过程中可以查询和处理数据。

这种体系结构非常适合不可变的数据，这些数据被大量摄取和处理。想想 100 的 TBs plus。然而，这种架构的发展将是缓慢的。直通计时通常以小时为单位。

批处理管道的问题是，我们通常不想等待几个小时才能得到一个结果。常见的解决方案是在它旁边添加一个流层。这有时被称为[λ架构](http://lambda-architecture.net/)。

Lambda 架构保留了一个批处理管道，就像上面的一样，但是它用一个快速流层来绕过它。这有点像在一个繁忙的城镇周围建造一条旁路。流层通常使用流处理工具，如 Storm 或 Samza。

Lambda 架构的关键见解是，我们通常乐于快速获得一个近似的答案，但我们最终想要一个准确的答案。

因此，流层绕过批处理层，在流窗口内提供*最佳答案。这些被写入服务层。稍后，批处理流水线计算精确的数据并覆盖近似值。*

这是一种平衡准确性和响应性的聪明方法。如果这两个分支最终在流层和批处理层中被双重编码，这种模式的一些实现就会受到影响。但是通常可以简单地将这种逻辑抽象成可以重用的公共库，特别是因为这种处理通常是用外部库编写的，比如 Python 或 R。或者，像 Spark 这样的系统在一个系统中同时提供流和批处理功能(尽管 Spark 中的流实际上是微批处理)。

因此，这种模式同样适合大容量数据平台，比如 100TB 以上的平台，这些平台希望将流与现有的、丰富的、基于批处理的分析功能相结合。

有另一种方法可以解决数据流水线慢的问题。它有时被称为 [Kappa 架构](http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html)。实际上，我认为这个名字是“开玩笑”，但我现在不太确定。无论是哪一种，我都将使用术语流数据平台，这也是一个正在使用的术语。

流数据平台颠倒了批处理模式。数据不是存储在 HDFS 中，并通过增量批处理作业进行提炼，而是存储在一个横向扩展的消息系统或日志中，如 Kafka。这成为记录系统，数据流被实时处理以创建一组三级视图、索引、服务层或数据集市。

这大体上类似于 Lambda 架构的流层，但是去掉了批处理层。显然，这要求消息传递层能够存储和出售大量数据，并且有足够强大的流处理器来处理这些数据。

没有免费的午餐，因此，对于困难的问题，流数据平台的运行速度可能不会比同等的批处理系统快，但将默认方法从“存储和处理”切换到“流和处理”可以为更快的结果提供更大的机会。

最后，流数据平台方法可以应用于“应用集成”问题。这是一个棘手而困难的问题，多年来一直受到 Informatica、Tibco 和 Oracle 等大供应商的关注。大部分结果是有益的，但不是变革性的。应用程序集成仍然是一个寻找真正可行的解决方案的主题。

流数据平台为这个问题提供了一个有趣的潜在解决方案。它们利用了 O/A 桥的许多优势——各种异步存储格式和重新创建视图的能力——但是将一致性要求隔离在通常是现有的来源中:

由于记录系统是一个日志，所以很容易实现不变性。像卡夫卡这样的产品可以在内部保留足够的体积，用作历史记录。这意味着恢复可以是重放和重新生成状态的过程，而不是不断地检查点。

类似风格的方法以前已经在许多大型机构中采用，例如 Goldengate，将数据移植到企业数据仓库或最近的数据湖。复制层吞吐量的不足和管理不断变化的模式的复杂性常常阻碍了它们的发展。第一个问题似乎不太可能继续存在。至于后面的问题，还没有定论。

~

所以我们从定位开始。读取和写入都采用顺序寻址。这主导了我们使用的组件内部的权衡。我们着眼于横向扩展这些组件，利用原语进行分片和复制。最后，我们将一致性重新命名为我们应该在构建的平台中隔离的问题。

但是数据平台本身实际上是在一个单一的整体形式中平衡这些单独组件的最佳点。渐进式重组。将写优化迁移到读优化。从一致性的约束转移到开放的流式、异步、不可变状态。

要做到这一点，必须记住几件事。模式就是其中之一。时间，分布式、异步世界的危险，是另一个。但如果认真解决，这些问题是可以控制的。当然，未来可能会包括更多这些东西，特别是随着在大数据领域创新的工具渗透到解决更广泛问题的平台中，无论是旧的还是新的。