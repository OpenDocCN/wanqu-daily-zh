# Etsy 工程|无可指责的事后分析和公正的文化

> 原文:[https://codeascraft.com/2012/05/22/blameless-postmortems/?UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://codeascraft.com/2012/05/22/blameless-postmortems/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

上周，欧文·托马斯(Owen Thomas)在 Business Insider 上写了一篇关于我们如何处理 Etsy 错误的文章。我想我可能会给出一些细节，关于这实际上是如何发生的，为什么。

任何与任何规模的技术打交道的人都熟悉失败。失败不关心你努力完成的架构设计，你编写和审查的代码，或者你仔细研究的警告和指标。

所以:失败发生了。在处理复杂系统时，这是一个必然的结论。但是那些由于个人行为(或者在某些情况下缺乏行为)而导致的失败呢？你怎么处理那些粗心的人类，他们让每个人都过了糟糕的一天？

也许他们应该被解雇。或者可能需要防止他们再次接触危险的部分。或者他们需要更多的训练。

这是“人为错误”的传统观点，它侧重于所涉及的个人的特征。这就是西德尼·德克尔所说的“坏苹果理论”——除掉坏苹果，你就能除掉人为错误。看起来很简单，对吧？

我们在 Etsy 没有这种传统观点。相反，我们希望看到错误、失误、失误等。用*学*的视角。对断电和事故进行无可指责的事后检查是其中的一部分。

## 无可指责的验尸

“无可指责的”尸检意味着什么？这是不是意味着每个人犯了错误都可以脱身？号码

嗯，也许吧。这取决于“摆脱困境”是什么意思。让我解释一下。

拥有公正的文化意味着你在努力平衡安全和责任。这意味着，通过调查错误，重点关注故障机制的情景方面和故障附近个人的决策过程，组织可以比通常情况下更安全地出来，如果它只是惩罚参与者作为补救措施。

拥有一个“无可指责的”事后分析过程意味着，其行为导致事故的工程师可以详细说明:

*   他们在什么时间采取了什么行动，
*   他们观察到了什么影响，
*   他们的期望，
*   他们所做的假设，
*   以及他们对事件发生时间线的理解。

...而且他们可以给出这个详细的叙述 ***而不用害怕惩罚或报复。*T3】**

为什么不应该惩罚或斥责他们？因为一个认为自己会被斥责的工程师不愿给出必要的细节来理解故障的机理、病理和操作。对事故是如何发生的缺乏了解几乎保证了它会再次发生。如果不是和原来的工程师，将来还有一个。

我们相信这一细节对于提高 Etsy 的安全性至关重要。

如果我们把“责备”作为主要方法，那么我们就含蓄地接受了威慑是组织变得更安全的方式。这是基于这样一种信念，即导致错误的是个人，而不是环境。这也与这样一种想法相一致，那就是人们担心**不**正确地做好自己的工作可能会受到惩罚。因为对惩罚的恐惧会激励人们在未来正确行事。对吗？

这种点名/指责/羞辱的循环可以这样看:

1.  工程师采取行动，导致失败或事故。
2.  工程师受到惩罚、羞辱、指责或重新培训。
3.  现场工程师(“尖端”)和管理层(“钝端”)之间的信任减少，寻找替罪羊
4.  工程师对行动/情况/观察的细节保持沉默，导致“掩护你的屁股”工程(由于害怕惩罚)
5.  由于上述第 4 条中提到的沉默，管理层对日常工作的了解和信息变得越来越少，工程师对潜在的失败条件的了解也越来越少
6.  更有可能的错误是，由于上面的第 5 条，潜在条件无法识别
7.  从步骤 1 开始重复

我们需要避免这种循环。我们希望犯了错误的工程师详细说明为什么(明确地或含蓄地)他或她这样做；为什么这个行动在当时对他们有意义。这对于理解故障的病理至关重要。在他们采取行动的时候，这个行动对他们来说是有意义的，因为如果这个行动当时对他们来说没有意义，他们一开始就不会采取这个行动。T3】

这里的基本原则是埃里克·霍尔内格尔所说的

> ```
> We must strive to understand that accidents don't happen because people gamble and lose.
> Accidents happen because the person believes that:
> ...what is about to happen is not possible,
> ...or what is about to happen has no connection to what they are doing,
> ...or that the possibility of getting the intended outcome is well worth whatever risk there is.
> ```

## 第二个故事

这种深入挖掘工程师所处环境的想法被称为寻找“第二个故事”。在事后总结会上，我们希望找到第二个故事来帮助理解哪里出了问题。
来自[人为错误背后](http://www.amazon.com/Behind-Human-Error-David-Woods/dp/0754678342 "Behind Human Error")以下是关于人为错误的“第一个”和“第二个”故事的区别:

| 第一个故事 | 第二个故事 |
| --- | --- |
| 人为错误被视为失败的原因 | 人为错误被视为组织内部更深层次的系统漏洞的结果 |
| 说人们应该做什么是描述失败的一种令人满意的方式 | 说人们应该做什么并不能解释为什么他们做了什么是有意义的 |
| 告诉人们更加小心将会使问题消失 | 只有通过不断寻找漏洞，组织才能提高安全性 |

## 允许工程师拥有他们自己的故事

有趣的事情发生了，当工程师犯了错误，并在给出相关细节时感到安全:他们不仅愿意承担责任，还热衷于帮助公司其他人避免未来犯同样的错误。毕竟，他们对自己的错误最在行。他们应该积极参与提出补救项目。

所以从技术上来说，工程师们并没有摆脱无可指责的验尸过程。最终，他们将帮助 Etsy 变得更安全、更有弹性。你瞧:我认识的大多数工程师都觉得让别人的事情变得更好是一件值得做的事情。

那么，我们如何在 Etsy 营造一种“公正的文化”呢？

*   我们通过对断电和事故进行这些无可指责的事后分析来鼓励学习。
*   我们的目标是了解事故是如何发生的，以便更好地防范将来的事故
*   我们寻找第二个故事，从多角度收集失败的细节，我们不会因为人们犯错而惩罚他们。
*   我们没有惩罚工程师，而是给予他们必要的权力来改善安全，允许他们详细说明他们对故障的贡献。
*   我们鼓励那些犯了错误的人成为专家，教育组织中的其他人将来不要再犯同样的错误。
*   我们承认，总有一个自由裁量的空间，人们可以在那里决定采取行动或不采取行动，这些决定的判断在于事后。
*   我们承认[后见之明偏差](http://en.wikipedia.org/wiki/Hindsight "Hindsight Bias")将继续模糊我们对过去事件的评估，并努力消除它。
*   我们承认[基本归因错误](http://en.wikipedia.org/wiki/Fundamental_attribution_error "Fundamental Attribution Error")也是难以避免的，因此我们在调查事故时关注人们工作的环境和情况。
*   我们努力确保组织的钝端了解工作实际上是如何完成的(与他们通过甘特图和程序想象的完成方式相反)。
*   依靠尖锐的一端来告知组织适当和不适当行为之间的界限。这不是钝端能自己想出来的东西。

失败发生了。为了理解失败是如何发生的，我们首先要理解我们对失败的**反应。**

 **一种选择是假设单一原因是不称职，并对工程师大喊大叫，让他们“注意！”或者“多加小心！”另一个选择是认真调查事故究竟是如何发生的，尊重参与事故的工程师，并从事故中吸取教训。

这就是为什么我们在 Etsy 有无可指责的验尸报告，也是为什么我们希望在这里创造一种公正的文化。**