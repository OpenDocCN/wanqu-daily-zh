# LinkedIn | LinkedIn 工程的发展简史

> 原文:[https://engineering . LinkedIn . com/architecture/brief-history-scaling-LinkedIn？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

如今，LinkedIn 在全球运营，拥有超过 3.5 亿会员。我们每天每秒都在为成千上万的网页提供服务。我们已经到达了我们的[移动时刻](http://blog.linkedin.com/2014/04/18/the-next-three-billion/)，移动流量占全球总流量的 50%以上。所有这些请求都从我们的后端系统获取数据，反过来每秒处理数百万次查询。

LinkedIn 和今天的许多网站一样，都是作为一个单一的应用程序开始的。这个应用叫做 Leo。它托管各种页面的 web servlets，处理业务逻辑，并连接到一些 LinkedIn 数据库。

 *### 成员图表

作为一个社交网络，首先要做的事情之一就是管理成员之间的联系。我们需要一个使用图遍历来查询连接数据的系统，并驻留在内存中以获得最高的效率和性能。有了这种不同的使用模式，很明显它需要独立于 Leo 进行扩展，所以一个名为 Cloud 的独立系统诞生了——LinkedIn 的第一个服务。为了将这个图形服务与 Leo 分开，我们使用 Java RPC 进行通信。

大约在这个时候，我们需要搜索功能。我们的成员 graph 服务开始向一个运行着 Lucene 的新搜索服务提供数据。

### 副本读取数据库

随着站点的增长，Leo 也在增长，增加了它的角色和责任，自然也增加了它的复杂性。当多个 Leo 实例启动时，负载平衡很有帮助。但是，额外的负担加重了 LinkedIn 最关键的系统的负担——它的**会员档案数据库**。

我们做的一个简单修复是经典的垂直扩展——投入更多的 CPU 和内存！虽然这赢得了一些时间，但我们需要进一步扩大规模。配置文件数据库同时处理读取和写入流量，因此为了进行扩展，引入了复制副本从属 DBs。副本数据库是成员数据库的副本，使用最早版本的 [databus](http://data.linkedin.com/blog/2012/10/driving-the-databus) (现在的[开源](https://engineering.linkedin.com/data-replication/open-sourcing-databus-linkedins-low-latency-change-data-capture-system))保持同步。它们被设置为处理所有读取流量，逻辑被构建为知道何时从副本数据库读取与从主数据库读取安全(一致)。

![](../Images/9dce59177b0c5bd796cd33b0198f40fc.png)

**虽然主从模式是一种中期解决方案，但我们已经转向分区数据库*

随着网站的流量越来越大，我们的单一应用 Leo 在生产中经常出现故障，很难进行故障排除和恢复，也很难发布新代码。高可用性对 LinkedIn 至关重要。很明显，我们需要“杀死 Leo ”,把它分成许多小的功能性的无状态服务。

![](../Images/fa2bac4d4ce9bb86e4f2d3ff12f1b6ee.png)

“杀死狮子座”是多年来公司内部的口头禅...

### 面向服务的架构

工程开始提取微服务来保存 API 和业务逻辑，如我们的搜索、个人资料、通信和群组平台。后来，我们的表示层被提取出来用于招聘人员产品或公共档案等领域。对于新产品，在 Leo 之外创建了全新的服务。随着时间的推移，每个功能区域都出现了垂直堆栈。

我们构建了前端服务器来从不同的域获取数据模型，处理表示逻辑，并构建 HTML(通过 JSP)。我们构建中间层服务来提供对数据模型的 API 访问，构建后端数据服务来提供对数据库的一致访问。到 2010 年，我们已经有超过 150 个独立的服务。今天，我们有超过 750 项服务。

![](../Images/c9e57e3b8453088046793f349de5e270.png)

【LinkedIn 中面向服务的多层架构示例

由于是无状态的，可以通过增加任何服务的新实例并在它们之间使用硬件负载平衡器来实现扩展。我们积极地开始标记每项服务，以了解它可以承受多少负载，并建立早期供应和性能监控功能。

### 贮藏

LinkedIn 经历了高速增长，需要进一步扩大规模。我们知道可以通过增加更多的缓存层来降低负载。许多应用程序开始引入中间层缓存层，如 [memcache](https://en.wikipedia.org/wiki/Memcached) 或 [couchbase](https://en.wikipedia.org/wiki/Couchbase_Server) 。我们还在数据层中添加了缓存，并开始在适当的时候使用[伏地魔](http://engineering.linkedin.com/tags/voldemort)和预先计算的结果。

随着时间的推移，我们实际上移除了许多中间层缓存。中间层缓存存储来自多个域的派生数据。虽然一开始缓存似乎是减少负载的一种简单方法，但是无效和调用图的复杂性正在失去控制。让缓存尽可能靠近数据存储可以保持较低的延迟，允许我们横向扩展，并减少认知负荷。

### 卡夫卡

为了收集越来越多的数据，LinkedIn 开发了许多自定义数据管道，用于传输和排队数据。例如，我们需要我们的数据流入数据仓库，我们需要将批量数据发送到我们的 [Hadoop 工作流](http://data.linkedin.com/projects/hadoop)进行分析，我们收集并汇总来自每个服务的日志，我们收集页面浏览量等跟踪事件，我们需要为我们的 inMail 消息系统排队，我们需要在有人更新其个人资料时保持我们的人员搜索系统更新。

随着网站的发展，更多的定制管道出现了。随着站点需要扩展，每条单独的管道也需要扩展。有些东西必须放弃。结果是我们的分布式发布订阅消息平台 Kafka T1 的开发。Kafka 成为了一个通用的管道，围绕着[提交日志](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)的概念构建，并且在构建时考虑了速度和可伸缩性。它实现了对任何数据源的近乎实时的访问，增强了我们的 Hadoop 作业，允许我们构建[实时分析](http://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot)，极大地改进了我们的[站点监控](http://engineering.linkedin.com/samza/real-time-insights-linkedins-performance-using-apache-samza)和[警报能力](http://engineering.linkedin.com/52/autometrics-self-service-metrics-collection)，并使我们能够可视化和[跟踪我们的调用图](http://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency)。今天，卡夫卡每天处理超过 5000 亿个事件。

![](../Images/79a14623f701187562f21e7979fd5d05.png)

*Kafka 作为通用数据流代理*

### 倒置

伸缩性可以从多个维度来衡量，包括组织层面。2011 年末，LinkedIn 启动了一项名为[反转](http://www.bloomberg.com/bw/articles/2013-04-10/inside-operation-inversion-the-code-freeze-that-saved-linkedin)的内部计划。这一举措暂停了特性开发，并允许整个工程组织专注于改进工具和部署、基础设施以及开发人员的生产力。它成功地实现了我们构建可扩展新产品所需的工程灵活性。

## 现代岁月

### Rest.li

当我们从 Leo 转换到面向服务的架构时，我们提取的 API 假设基于 Java 的 RPC，在团队之间不一致，与表示层紧密耦合，而且情况越来越糟。为了解决这个问题，我们构建了一个名为 [Rest.li](http://engineering.linkedin.com/architecture/restli-restful-service-architecture-scale) 的新 API 模型。Rest.li 是我们迈向以数据模型为中心的架构的一步，这确保了整个公司的一致的无状态 Restful API 模型[。](http://engineering.linkedin.com/restli/linkedins-restli-moment)

通过在 HTTP 上使用 JSON，我们的新 API 最终使得拥有非基于 Java 的客户机变得容易了。LinkedIn 今天仍然主要是一个 Java 商店，但也有许多使用 Python、Ruby、Node.js 和 C++的客户，这些都是内部开发的，也来自我们收购的技术堆栈。远离 RPC 也将我们从表示层的高度耦合和许多向后兼容性问题中解放出来。另外，通过将[动态发现(D2)](https://github.com/linkedin/rest.li/wiki/Dynamic-Discovery) 与 Rest.li 一起使用，我们获得了每个服务 API 的基于客户端的自动化负载平衡、发现和可伸缩性。

今天，LinkedIn 拥有超过 975 个 Rest.li 资源，每天在我们所有的数据中心有超过 1000 亿个 Rest.li 呼叫。

![](../Images/84cc2b655a281dc5ae941f0f6409bb67.png)

*李与理工栈*

### 超级块

面向服务的架构可以很好地分离域和独立扩展服务。但是也有不利的一面。我们的许多应用程序获取多种不同类型的数据，进而进行数百次下游调用。当考虑所有许多下游调用时，这通常被称为“调用图”或“扇出”。例如，任何个人资料页面请求获取的不仅仅是个人资料数据，还包括照片、连接、群组、订阅信息、关注信息、长篇博客文章、我们图表的连接度、推荐等。这个调用图很难管理，而且变得越来越难以控制。

我们引入了超级块的概念——用单一的访问 API 对后端服务进行分组。这允许我们有一个特定的团队来优化块，同时保持我们的调用图在检查每个客户。

### 多数据中心

作为一家成员数量快速增长的全球性公司，我们需要超越从一个数据中心提供流量[的规模。我们几年前就开始努力解决这个问题，首先通过两个数据中心(洛杉矶和芝加哥)提供公共配置文件。一旦得到验证，我们就着手增强我们的所有服务，以处理数据复制、来自不同来源的回调、单向数据复制事件，以及将用户锁定在地理位置接近的数据中心。](https://www.linkedin.com/pulse/armen-hamstra-how-he-broke-linkedin-got-promoted-angel-au-yeung)

我们的许多数据库运行在 [Espresso](http://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store) (一种新的内部多租户数据存储)上。Espresso 在设计时就考虑了多数据中心。它提供主/主支持，并处理许多困难的复制。

多个数据中心对于维护“站点启动”和高可用性非常重要。您需要避免任何单点故障，不仅是针对每个单独的服务，而是针对整个站点。如今，LinkedIn 有三个主要的数据中心，另外还有遍布全球的[pop](http://engineering.linkedin.com/performance/how-linkedin-used-pops-and-rum-make-dynamic-content-download-25-faster)。

![](../Images/4a003e428c65e47a2be4852746699bd2.png)

*LinkedIn 截至 2015 年的运营设置(圆圈代表数据中心，菱形代表 pop)*

### 我们还做了什么？

当然，我们的扩展故事从来没有这么简单。这些年来，我们在所有工程和运营团队中做了无数的事情，包括一些更大的计划:

多年来，我们许多最关键的系统都有自己丰富的历史和发展来解决规模问题。这包括我们的[会员图服务](http://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed)(我们在 Leo 之外的第一个服务)[搜索](https://engineering.linkedin.com/search/did-you-mean-galene)(我们的第二个服务)、新闻推送、[交流平台](http://www.slideshare.net/manivannan57/LinkedIn-Communication-Architecture-Presentation-2?related=5)和会员档案后端。

我们构建了支持长期发展的数据基础设施。这一点在 Databus 和 Kafka 上表现得尤为明显，在数据流领域延续了 [Samza](https://engineering.linkedin.com/samza/real-time-insights-linkedins-performance-using-apache-samza) ，存储解决方案领域延续了 [Espresso](http://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store) 和 Voldemort， [Pinot](http://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot) 用于我们的分析系统，以及其他定制解决方案。另外，我们的工具已经得到了改进，开发人员可以[自动提供这些基础设施](https://www.linkedin.com/pulse/invisible-infrastructure-alex-vauthey)。

我们已经开发了一个大规模的离线工作流，使用 [Hadoop](http://data.linkedin.com/projects/hadoop) 和我们的[伏地魔数据存储库](http://engineering.linkedin.com/tags/voldemort)来预先计算数据洞察，比如你可能认识的人、相似的个人资料、著名的校友和个人资料浏览地图。

我们重新思考了我们的前端方法，添加了[客户端模板](http://www.slideshare.net/brikis98/dustjs)([个人资料页面](http://engineering.linkedin.com/profile/engineering-new-linkedin-profile)，[大学页面](http://engineering.linkedin.com/university/building-linkedin-university-pages))。这支持更多的交互式应用程序，要求我们的服务器只发送 JSON 或部分 JSON。另外，模板缓存在 CDNs 和浏览器中。我们也开始使用 [BigPipe](http://engineering.linkedin.com/frontend/new-technologies-new-linkedin-home-page) 和 [Play framework](https://engineering.linkedin.com/play/composable-and-streamable-play-apps) ，将我们的模型从一个线程化的 web 服务器改变为一个[非阻塞异步的](https://engineering.linkedin.com/play/play-framework-async-io-without-thread-pool-and-callback-hell)服务器。

除了应用程序代码，我们还引入了[多层代理](http://www.slideshare.net/thenickberry/reflecting-a-year-after-migrating-to-apache-traffic-server),使用 Apache Traffic Server 和 HAProxy 来处理负载平衡、数据中心锁定、安全性、智能路由、服务器端渲染等等。

最后，我们继续通过优化硬件、[高级内存](http://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications)和[系统](http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)调整以及利用更新的 Java 运行时来提高我们服务器的性能。

## 下一步是什么

LinkedIn 继续快速增长，我们仍有大量的工作要做。我们正在解决很少有人能解决的问题- [来加入我们吧！](https://www.linkedin.com/company/linkedin/careers?trk=eng-blog)

*感谢[史蒂夫](https://www.linkedin.com/in/stevenihde)、[斯威](https://www.linkedin.com/in/sweelim)、[文卡特](https://in.linkedin.com/in/ganesanvenkatasubramanian)、[埃兰](https://www.linkedin.com/in/eranl)、[拉姆](https://www.linkedin.com/in/ramvem)、[布兰登](https://www.linkedin.com/in/bchesla)、[马马德](https://www.linkedin.com/in/mammadzand)和[尼克](https://www.linkedin.com/in/nberry)帮助审核。**