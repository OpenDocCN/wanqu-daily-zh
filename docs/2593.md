# 神奇的口袋里——收纳箱

> 原文:[https://blogs . Dropbox . com/tech/2016/05/inside-the-magic-pocket/？UTM _ source = Wanqu . co&UTM _ campaign = Wanqu+Daily&UTM _ medium = website](https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)

自从[宣布 Magic Pocket](https://blogs.dropbox.com/tech/2016/03/magic-pocket-infrastructure/) 以来，我们收到了很多积极的反馈，这是我们内部的多 EB 存储系统。在发布之后，我们将发布一系列技术博客文章，介绍系统有趣的方面，包括我们的保护机制、操作工具以及硬件和软件之间的创新。但首先，我们需要一些背景:在这篇文章中，我们将给出 Magic Pocket 的高层次架构概述以及它的设计标准。

正如我们在[介绍文章](https://blogs.dropbox.com/tech/2016/03/magic-pocket-infrastructure/)中解释的，Dropbox 存储两种数据:文件内容和关于文件和用户的元数据。Magic Pocket 是我们用来存储文件内容的系统。这些文件被分割成块，为保持持久性而复制，并分布在我们位于多个地理区域的基础架构中。

Magic Pocket 是基于一套相当简单的核心协议，但它也是一个庞大而复杂的系统，所以我们必须掩盖一些细节。欢迎在下面的评论中添加反馈；我们将在以后的文章中尽最大努力深入研究。

注意:在内部我们称这个系统为“MP ”,这样我们就不会觉得一直说“魔法”这个词很傻。我们也会在这篇文章中这样做。

# 要求

Magic Pocket 是一个*不可变的块存储系统*。它存储最大 4 兆字节大小的加密文件块，一旦块被写入系统，它就永远不会改变。不变性让我们的生活*轻松了许多*。

当用户对 Dropbox 上的文件进行更改时，我们会在一个名为 *FileJournal* 的独立系统中记录所有的更改。这使我们能够简单地存储不可变块，同时将支持可变性的逻辑移到堆栈的更高层。有许多大规模的存储系统为可变块提供了本机支持，但是一旦进入较低层，它们通常都基于不可变的存储原语。

Dropbox 拥有大量数据和高度的时间局部性。许多数据在上传后的一小时内被频繁访问，之后访问频率越来越低。这种模式是有意义的:我们的用户在 Dropbox 中进行大量的协作，因此一个文件可能在上传后很快就会被同步到其他设备上。但是我们仍然需要可靠的快速访问:你可能不会太频繁地查看 1997 年以来的税务记录，但是当你查看时，你会立即想要它们。我们有一个相当“冷”的存储系统，但要求对所有数据块进行低延迟读取。

为了应对这一工作负载，我们构建了一个基于旋转介质(一种“硬盘驱动器”的奇特说法)的系统，该系统具有耐用、廉价、存储密集和相当低的延迟的优势，我们将固态驱动器(SSD)用于数据库和缓存。我们对最近的上传使用高度的初始复制和缓存，同时对其余数据使用更高效的存储编码。

耐用性在魔术口袋里是不可协商的。我们理论上的耐久性必须是有效无限的，到了末日小行星撞击造成的损失比随机磁盘故障更有可能的程度——在那个阶段，我们可能会有更大的问题要担心。这些数据经过擦除编码以提高效率，并跨多个地理区域存储，具有广泛的复制性，以确保针对灾难和自然灾害的保护。

作为一名工程师，这是有趣的部分。Magic Pocket 必须在大约 6 个月的时间内从最初的两位数 Pb 的原型发展到数 EB 的庞然大物，这是一个相当前所未有的转变。这要求我们花费大量时间思考、设计和构建原型，以消除我们可以预见的瓶颈。这个过程还帮助我们确保架构具有足够的可扩展性，因此我们可以在不可预见的需求出现时对其进行更改。

在这个过程中，有很多不可预见的需求的例子。有一次，流量突然增加，我们的网络集群之间的路由器开始饱和。这要求我们改变数据放置算法和请求路由，以更好地反映集群关联性(以及可用存储容量、集群增长计划等)，并最终彻底改变我们的集群间网络架构。

作为工程师，我们知道复杂性通常是可靠性的对立面。我们中的许多人已经花了足够多的时间来编写复杂的共识协议，知道整天重新实现 Pax OS T1 通常是个坏主意。当以容错和可伸缩的方式执行时，MP 尽可能地避免仲裁式的一致或分布式协调，并大量利用集中协调点。有时候，我们可以选择分布式哈希表或 trie 作为块索引，而不是选择一个巨大的分片 MySQL 集群；从简化开发和最小化未知的角度来看，这被证明是一个非常好的决定。

在我们讨论架构本身之前，首先让我们弄清楚我们在存储什么。

MP 存储**块**，这些块是不透明的文件块，最大 4MB:

这些数据块经过压缩和加密，然后传递给 MP 进行存储。每个块都需要一个键或名称，对于我们的大多数用例来说，这是块的阿沙-256 **散列**。

但是，4MB 在多 EB 的存储系统中是一个非常小的数据量，太小了，不足以在我们需要更换磁盘或擦除某些数据时移动。为了使这个问题易于处理，我们将这些块聚集到 1GB 的逻辑存储容器中，称为**桶**。给定桶中的块不一定有任何共同点；它们只是碰巧在同一时间上传的区块。

为了可靠性，需要在多个物理机上复制存储桶。最近上传的数据块被直接复制到多台机器上，然后最终包含这些数据块的存储桶被聚合在一起，并进行擦除编码以提高存储效率。我们使用术语**卷**来指代复制到一组物理存储节点上的一个或多个存储桶。

总结一下:由**散列**标识的**块**，被写入**桶**。每个存储桶以复制或擦除编码的形式存储在跨多台机器的**卷**中。

那么现在我们知道了我们的需求和数据模型，Magic Pocket 实际上是什么样子的呢？嗯，大概是这样的:

这可能看起来不多，但很重要。MP 是一个*多区域*架构，在美国西部、中部和东部都有服务器集群。MP 中的每个数据块都独立存储在至少两个独立的区域中，然后在这些区域中可靠地复制*。这种冗余对于避免自然灾害和大规模停机非常重要，而且还允许我们建立非常清晰的管理域和抽象边界，以避免跨区域级联造成的配置错误或拥塞崩溃。*

*【我们正在为访问频率较低(“较冷”)的数据进行一些扩展，这些扩展采用了与此不同的多区域架构。]*

然而，大多数神奇的事情都发生在一个区域内，所以让我们深入了解一下:

我们将逐一介绍这些组件。

**These nodes accept storage requests from outside the system , and are the gateway to Magic Pocket . They determine where a block should be stored and issue commands inside MP to read or write the block.

### **街区指数**

This is the service that maps each block to the bucket where it’s stored. You can think of this as a giant database with the following schema:Copy

```
hash → cell, bucket, checksum
```

( 我们的 real schema 比这个稍微复杂一点来支持 之类的东西 删除，跨区域复制等等 。 )

块索引是一个巨大的分片 MySQL 集群前端是 一个 RPC 服务层 ，再加上很多数据库操作和可靠性的工具。我们最初计划为此目的建立一个专用的键值存储，但是 MySQL 表现得更好。我们已经在 Dropbox 堆栈 、 中部署了数千个数据库节点，因此这使我们能够利用我们围绕大规模管理 MySQL 而建立的运营能力。

我们最终可能会建立一个更复杂的系统 ， ，但是我们现在对此很满意。键值存储很流行，并且提供高性能 ， ，但是数据库是高度可靠的，并且提供了一个表达性的数据模型，这使得我们可以随着时间的推移轻松地扩展我们的模式和功能。

### **跨区复制**

跨区域复制守护程序负责将所有数据块从一个区域异步复制到另一个区域。我们在本地上传数据块后的一秒钟内将每个数据块写入远程区域。我们将这种复制延迟纳入我们的耐久性模型，并确保数据在本地区域中得到足够广泛的复制。

### **细胞**

单元是独立的逻辑存储集群，可存储约 50PB 的原始数据 。每当我们想要向 MP 添加更多空间时，我们通常会调出一个新单元。虽然这些单元在逻辑上完全独立 、 ，但我们在机架上对每个单元进行分条，以确保单元内最大的物理多样性。

让我们深入细胞内部，看看它是如何工作的:

### **【对象存储设备】**

一个单元中最重要的角色是 OSD、 装满磁盘的存储盒 单台机器可以存储超过 1pb 的数据，或每个机架超过 8 PB。这些设备上有一些非常复杂的逻辑来管理缓存 、 、磁盘调度 、 和数据验证 、 ，但从系统的其余部分来看，这些都是“哑”节点 : 它们存储块但不了解单元拓扑或参与 分布式协议 。

### **复制表**

复制表是单元的索引，它将每个逻辑数据桶映射到存储该桶的卷和 OSD。像块索引一样，复制表存储为 MySQL 数据库，但是要小得多，更新频率也低得多。复制表的工作集完全放在这些数据库的内存中，这为我们在少量物理机上提供了非常高的读取吞吐量。

复制表上的模式如下:

Copy

```
bucket → volume
volume → OSDs, open, type, generation 
```

这里的一个重要概念是 `open` 标志，其中 指示卷是“打开”还是“关闭”。打开的卷只能写入新数据，不能写入其他数据。封闭的体积是不可变的，可以在单元内安全地移动。在任一时间点，只有少量卷处于打开状态。

`type`指定了卷类型:复制卷或使用我们的擦除编码方案之一编码的卷。 `generation` 号用于在移动卷以从磁盘故障中恢复或优化存储布局时确保一致性。

### **大师**

主人 最好被认为是牢房的看门人或协调员。它 包含了系统中大多数复杂的协议逻辑and Its 的主要工作是监视 OSD，并在其中一个出现故障时触发数据修复操作。它还协调后台操作，如当存储桶变满时创建新的存储桶，当数据被删除时触发垃圾收集，或在 垃圾收集后 桶变得太小时将桶合并在一起。

复制表存储权威卷状态，因此主卷本身完全处于软状态。注意，主设备不在数据平面 : 上，没有实时流量流过它 ， ，并且如果主设备停机，单元可以继续提供读取服务。单元甚至可以在没有主设备的情况下接收写入，尽管它最终会用完可用的存储桶，而主设备不会在它们填满时创建新的存储桶。如果主单元不在创建这些新的存储桶时，总会有大量的其他单元需要写入。

我们为每个单元运行一个主节点， ，这为我们提供了一个集中的协调点，用于复杂的数据放置决策，而没有分布式协议的显著复杂性。这种集中式模型对每个单元的大小有限制 : 在内存和 CPU 开销成为瓶颈之前，我们可以支持大约 100 Pb 的 。幸运的是 、 拥有多个单元从部署角度来看也非常方便，并且提供了更好的隔离以避免级联故障。

### **卷管理器**

卷管理人员是单元的举重运动员。它们响应主机的请求，在 、 周围移动卷或擦除代码卷。 这通常是指 从一堆 OSD 中读取，向其他 OSD、 和 写入，然后将控制权交还给主机以完成操作。

卷管理器进程运行在与 OSD 相同的物理硬件上，因为 这允许 美国 在单元中的空闲 存储 硬件上分摊其 繁重的网络容量需求。

唷！您已经做到了这一步，希望对高级 Magic Pocket 架构有一个合理的理解。我们将非常粗略地概述一些核心 MP 协议，我们可以在以后的帖子中详细阐述。幸运的是，这些协议已经很简单了。

在收到 Put 请求之前，前端配备了一些信息:它们定期联系每个单元以确定它有多少可用空间，以及可以接收新写入的*打开卷*的列表。

当 Put 请求到达时，前端首先检查该块是否已经存在(通过块索引)，然后选择一个目标卷来存储该块。从单元中选择卷的方式可以均匀分布单元负载，并最大限度地减少存储集群之间的网络流量。然后，前端查询复制表以确定当前存储卷的 OSD。

前端向这些 OSD 发出存储命令，所有 OSD 在响应之前将数据块同步到磁盘(或板载 SSD)。如果这是成功的，则前端向块索引添加新的条目，并且可以成功地返回到客户端。如果任何 OSD 在此过程中出现故障，前端只会在另一个卷(可能在另一个单元中)中重试。如果块索引失败，则前端会将请求转发到另一个区域。主服务器定期运行后台任务，以清除失败操作的任何部分写入。

![Put Protocol](../Images/6b04f70a2f3f9ea4d4ce081d8b5c37a5.png)

Put Protocol



幕后有一些微妙的细节，但最终它是相当简单的。如果我们采用基于仲裁的协议，其中前端仅需要写入卷中 OSD 的子集，那么我们将避免其中一些重试，并可能实现更低的尾部延迟，但代价是更大的复杂性。基于重试的方案中明智的超时管理已经导致了较低的尾部延迟，并为我们提供了非常满意的性能。

一旦我们知道了 Put 协议，服务 Get 的过程应该是不言自明的。前端从块索引中查找单元和桶，然后从复制表中查找卷和 OSD，然后从其中一个 OSD 中获取块，如果一个 OSD 不可用，则重试。

如上所述，我们将复制数据和擦除编码数据都存储在 MP 中。从复制的卷中读取很容易，因为卷中的每个 OSD 都存储所有的块。

从擦除编码卷中读取数据可能会更棘手一些。我们以这样一种方式编码，每个块可以从一个给定的 OSD 中完整地读取，所以大多数读取只命中一个磁盘轴；这对于减轻我们硬件的负荷是很重要的。如果该 OSD 不可用，则前端需要通过从其他 OSD 读取编码数据来重建该块。它在卷管理器的帮助下执行这种重建。

![Erasure Coded Volume](../Images/0496077c33046a675008d8567b816393.png)

Erasure Coded Volume



在上面的编码方案中，前端可以从 OSD 1 读取块 A，以绿色突出显示。如果读取失败，它可以通过从其他 OSD 上读取足够数量的块来重建块 A，以红色突出显示。我们实际的编码比这稍微复杂一点，并且经过优化，允许在大多数故障情况下从较小的 OSD 子集进行重建。

主服务器运行许多不同的协议来管理单元中的卷，并在操作失败后进行清理。但是主设备执行的最重要的操作是修复。

修复是每当磁盘出现故障时用于重新复制卷的操作。主设备通过我们的服务发现系统持续监控 OSD 的健康状况，并在 OSD 离线 15 分钟后触发修复操作——这一时间足以重启节点而不会触发不必要的修复，但也足够短以提供快速恢复并最大限度地减少任何漏洞窗口。

卷在某种程度上随机分布在整个单元中，每个 OSD 容纳几千个卷。这意味着，如果我们丢失了一个 OSD，我们可以同时从数百个其他 OSD 重建完整的卷集:

在上图中，我们丢失了 OSD 3，但是可以从 OSD 1、2、4 和 5 中恢复卷 A、B 和 C。实际上，每个 OSD 都有成千上万的卷，数百个其他 OSD 共享这些数据。这使我们能够在数百个网卡和数千个磁盘轴之间分摊重建流量，从而最大限度地缩短恢复时间。

当一个 OSD 出现故障时，主设备做的第一件事是关闭该 OSD 上的所有卷，并指示其他 OSD 在本地反映这一变化。现在卷已经关闭，我们知道它们不会接受任何未来的写入，因此可以安全地移动。

然后，主机构建一个*重建计划*，其中它选择一组要复制的 OSD 和一组要复制到的 OSD，以这种方式在尽可能多的 OSD 上均匀分布负载。这一步使我们能够避免特定磁盘或机器上的流量高峰。重建计划允许我们为每个 OSD 提供更少的硬件资源，如果没有主设备作为协调中心，将很难生产。

我们将忽略数据传输过程，但它涉及卷管理器将数据从源复制到目的地，必要时进行擦除编码，然后将控制权交还给主设备。

最后一步相当简单，但也很关键:此时，卷在源和目标 OSD 上都存在，但移动还没有提交。如果主节点此时出现故障，卷将停留在旧位置，并由新的主节点再次修复。为了提交修复操作，主机首先增加新 OSD 上的卷上的*世代号*，然后更新复制表以存储具有新世代的新卷到 OSD 映射(提交点)。现在，我们已经增加了代号，我们知道不会混淆哪个 OSD 保存卷，即使失败的 OSD 复活。

该协议确保任何节点都可以在任何时候发生故障，而不会使系统处于不一致的状态。我们在生产中见过各种疯狂的东西。在一个实例中，一个数据库前端冻结了整整一个小时，然后才恢复正常，并向复制表转发一个请求，在此期间，主服务器也出现了故障并重新启动，发出了一组完全不同的修复操作。面对这样的任意故障，我们的一致性协议需要完全可靠。主服务器还运行许多其他后台进程，如 Reconcile，它验证 OSD 状态并回滚失败的修复或未完成的操作。

开放/封闭卷模型是确保实时流量不干扰后台操作的关键，并且允许我们使用比不实施这种二分法简单得多的一致性协议。

谢谢你能走到这一步！希望这篇文章能给我们一些关于 Magic Pocket 如何工作以及我们的一些动机的背景。

这里的主要设计原则是**保持简单！**设计一个分布式存储系统是一个巨大的挑战，但是*更难的是*建立一个能够大规模可靠运行，并支持所有监控和验证系统以及确保其正确运行的工具。做出针对正确问题的正确解决方案的技术决策也非常重要，不仅仅是因为它们很酷很新颖。MP 的大部分是由不到六个人的团队构建的，这要求我们专注于重要的事情，并在项目的成功中发挥了重要作用。

显然我们遗漏了很多细节。(以防你要回应，*“等等！这个在 X，Y，Z 发生的时候就不管用了！”—* 我们已经考虑过了，我保证。)请继续关注未来的博客帖子，在那里我们将更详细地讨论构建和操作这种规模的系统的具体方面。**